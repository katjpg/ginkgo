{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5242fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path.cwd().parent) not in sys.path:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "\n",
    "from models.entities import EntityType\n",
    "from nlp.candidates.filter import PairFilter, TypeConfig\n",
    "from nlp.candidates.npmi import NPMI\n",
    "from nlp.candidates.collocation import Collocation\n",
    "from nlp.relations.explicit import Explicit\n",
    "from nlp.relations.implicit import ImplicitCluster\n",
    "from nlp.relations.embeddings import EntityEmbeddings\n",
    "from nlp.entity_filter import filter_pipeline, FilterConfig\n",
    "from nlp.syntactic import nlp\n",
    "from config.llm import GeminiConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7f6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1810.04805\"\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "gemini_config = GeminiConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b284af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 26\n",
      "Average sentence length: 1446 chars\n"
     ]
    }
   ],
   "source": [
    "full_text_path = output_dir / f\"{arxiv_id}_full_text.txt\"\n",
    "full_text = full_text_path.read_text()\n",
    "\n",
    "sentences = full_text.split(\"\\n\\n\")\n",
    "sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Average sentence length: {np.mean([len(s) for s in sentences]):.0f} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e13081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 459\n",
      "Entity type distribution:\n",
      "  dataset: 88\n",
      "  method: 76\n",
      "  metric: 15\n",
      "  other: 200\n",
      "  task: 80\n"
     ]
    }
   ],
   "source": [
    "with open(output_dir / f\"{arxiv_id}_all_entities.json\") as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "print(f\"Total entities: {len(all_entities)}\")\n",
    "\n",
    "type_counts = {}\n",
    "for e in all_entities:\n",
    "    t = e['type']\n",
    "    type_counts[t] = type_counts.get(t, 0) + 1\n",
    "\n",
    "print(\"Entity type distribution:\")\n",
    "for t in sorted(type_counts.keys()):\n",
    "    print(f\"  {t}: {type_counts[t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21a3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered entities: 79\n",
      "Filtered entity type distribution:\n",
      "  dataset: 50\n",
      "  method: 11\n",
      "  metric: 2\n",
      "  task: 16\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(full_text)\n",
    "\n",
    "filter_config = FilterConfig()\n",
    "filtered_entities = filter_pipeline(all_entities, doc, filter_config)\n",
    "\n",
    "print(f\"Filtered entities: {len(filtered_entities)}\")\n",
    "\n",
    "type_counts_filtered = {}\n",
    "for e in filtered_entities:\n",
    "    t = e['type']\n",
    "    type_counts_filtered[t] = type_counts_filtered.get(t, 0) + 1\n",
    "\n",
    "print(\"Filtered entity type distribution:\")\n",
    "for t in sorted(type_counts_filtered.keys()):\n",
    "    print(f\"  {t}: {type_counts_filtered[t]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4a9118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-filtered candidates: 1603\n"
     ]
    }
   ],
   "source": [
    "type_config = TypeConfig()\n",
    "pair_filter = PairFilter(type_config)\n",
    "\n",
    "candidates = pair_filter.generate(filtered_entities)\n",
    "\n",
    "print(f\"Type-filtered candidates: {len(candidates)}\")\n",
    "\n",
    "candidates_output = [{\"e1\": e1, \"e2\": e2} for e1, e2 in candidates]\n",
    "with open(output_dir / f\"{arxiv_id}_candidates_typed.json\", \"w\") as f:\n",
    "    json.dump(candidates_output, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5042c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI candidates: 590\n",
      "After collocation filter: 129\n",
      "NPMI range: 0.019 - 1.000\n"
     ]
    }
   ],
   "source": [
    "npmi_calc = NPMI(tau=0.0, min_cooc=1)\n",
    "npmi_candidates = npmi_calc.select(filtered_entities, sentences)\n",
    "\n",
    "entity_dict = {e['text']: e for e in filtered_entities}\n",
    "colloc_filter = Collocation(min_cooc=1, max_sent_dist=5)\n",
    "colloc_candidates = colloc_filter.filter(npmi_candidates, entity_dict)\n",
    "\n",
    "print(f\"NPMI candidates: {len(npmi_candidates)}\")\n",
    "print(f\"After collocation filter: {len(colloc_candidates)}\")\n",
    "\n",
    "if colloc_candidates:\n",
    "    npmi_vals = [c.npmi for c in colloc_candidates]\n",
    "    print(f\"NPMI range: {min(npmi_vals):.3f} - {max(npmi_vals):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb41bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High confidence (≥0.40): 50\n",
      "Medium confidence (0.20-0.40): 37\n",
      "Low confidence (<0.20): 42\n",
      "Total top candidates: 60\n"
     ]
    }
   ],
   "source": [
    "colloc_candidates.sort(key=lambda x: -x.npmi)\n",
    "\n",
    "high_conf = [c for c in colloc_candidates if c.npmi >= 0.40]\n",
    "medium_conf = [c for c in colloc_candidates if 0.20 <= c.npmi < 0.40]\n",
    "low_conf = [c for c in colloc_candidates if c.npmi < 0.20]\n",
    "\n",
    "print(f\"High confidence (≥0.40): {len(high_conf)}\")\n",
    "print(f\"Medium confidence (0.20-0.40): {len(medium_conf)}\")\n",
    "print(f\"Low confidence (<0.20): {len(low_conf)}\")\n",
    "\n",
    "top_candidates = [(c.e1, c.e2) for c in high_conf + medium_conf[:10]]\n",
    "\n",
    "print(f\"Total top candidates: {len(top_candidates)}\")\n",
    "\n",
    "top_ranked = [\n",
    "    {\"e1\": c.e1, \"e2\": c.e2, \"npmi\": c.npmi, \"cooc\": c.n_cooc}\n",
    "    for c in high_conf + medium_conf[:10]\n",
    "]\n",
    "\n",
    "with open(output_dir / f\"{arxiv_id}_ranked_candidates.json\", \"w\") as f:\n",
    "    json.dump(top_ranked, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c996cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences prepared: 26\n",
      "Candidates prepared: 60\n"
     ]
    }
   ],
   "source": [
    "sentence_strings = [s.strip() for s in full_text.split(\"\\n\\n\") if s.strip()]\n",
    "\n",
    "candidates_dicts = [{\"e1\": e1, \"e2\": e2} for e1, e2 in top_candidates]\n",
    "\n",
    "print(f\"Sentences prepared: {len(sentence_strings)}\")\n",
    "print(f\"Candidates prepared: {len(candidates_dicts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2b1169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicit relations discovered: 22\n",
      "  ELMo --[evaluates]--> Open AI GPT\n",
      "    Confidence: 0.40\n",
      "  classification --[uses]--> sentence pairs\n",
      "    Confidence: 0.30\n",
      "  NER --[evaluates]--> named entity recognition\n",
      "    Confidence: 0.30\n",
      "  ELMo --[proposes]--> named entity recognition\n",
      "    Confidence: 0.20\n",
      "  NER --[uses]--> paraphrasing\n",
      "    Confidence: 0.20\n"
     ]
    }
   ],
   "source": [
    "explicit_extractor = Explicit(gemini_config)\n",
    "\n",
    "explicit_relations = explicit_extractor.discover(\n",
    "    candidates_dicts,\n",
    "    sentence_strings\n",
    ")\n",
    "\n",
    "print(f\"Explicit relations discovered: {len(explicit_relations)}\")\n",
    "\n",
    "for rel in explicit_relations[:5]:\n",
    "    print(f\"  {rel.e1} --[{rel.rel_type}]--> {rel.e2}\")\n",
    "    print(f\"    Confidence: {rel.confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd4a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated: (79, 384)\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "embeddings_encoder = EntityEmbeddings()\n",
    "entity_texts = [e['text'] for e in filtered_entities]\n",
    "embeddings = embeddings_encoder.compute(entity_texts)\n",
    "\n",
    "print(f\"Embeddings generated: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings_encoder.dim()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a8e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit relations discovered: 215\n",
      "Confidence distribution:\n",
      "  0.5: 6\n",
      "  0.38: 20\n",
      "  0.25: 39\n",
      "  0.12: 150\n"
     ]
    }
   ],
   "source": [
    "explicit_pairs = set()\n",
    "\n",
    "implicit_cluster = ImplicitCluster(k=8, tau_sim=0.45, tau_b=1)\n",
    "implicit_relations_raw = implicit_cluster.infer(\n",
    "    filtered_entities,\n",
    "    embeddings,\n",
    "    explicit_pairs\n",
    ")\n",
    "\n",
    "print(f\"Implicit relations discovered: {len(implicit_relations_raw)}\")\n",
    "\n",
    "confidence_dist = {}\n",
    "for rel in implicit_relations_raw:\n",
    "    conf_bin = round(rel.confidence, 2)\n",
    "    confidence_dist[conf_bin] = confidence_dist.get(conf_bin, 0) + 1\n",
    "\n",
    "print(\"Confidence distribution:\")\n",
    "for conf in sorted(confidence_dist.keys(), reverse=True)[:10]:\n",
    "    print(f\"  {conf}: {confidence_dist[conf]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4752d44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity type mapping: 79 entries\n"
     ]
    }
   ],
   "source": [
    "entity_types_map = {}\n",
    "for e in filtered_entities:\n",
    "    entity_types_map[e['text']] = EntityType(e['type'])\n",
    "\n",
    "print(f\"Entity type mapping: {len(entity_types_map)} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a39d4c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit relations: 215 → 99\n",
      "High-quality implicit (≥0.15): 29\n",
      "  task       → dataset   : machine translation → CoLA The Corpus of Linguistic Acceptability (0.500)\n",
      "  task       → dataset   : machine translation → natural language inference dataset (0.500)\n",
      "  task       → dataset   : classification → pretraining data (0.375)\n",
      "  task       → dataset   : next sentence prediction → Books Corpus (0.375)\n",
      "  task       → dataset   : next sentence prediction → large text corpus (0.375)\n",
      "  task       → dataset   : named entity recognition → shuffled sentence-level corpus (0.375)\n",
      "  task       → dataset   : named entity recognition → monolingual corpus (0.375)\n",
      "  task       → dataset   : machine translation → Stanford Sentiment Treebank (0.375)\n",
      "  task       → dataset   : text generation → document-level corpus (0.375)\n",
      "  task       → dataset   : text generation → shuffled sentence-level corpus (0.375)\n"
     ]
    }
   ],
   "source": [
    "implicit_typed = pair_filter.filter_relations(implicit_relations_raw, entity_types_map)\n",
    "\n",
    "print(f\"Implicit relations: {len(implicit_relations_raw)} → {len(implicit_typed)}\")\n",
    "\n",
    "high_quality_implicit = [r for r in implicit_typed if r.confidence >= 0.15]\n",
    "print(f\"High-quality implicit (≥0.15): {len(high_quality_implicit)}\")\n",
    "\n",
    "for rel in high_quality_implicit[:10]:\n",
    "    e1_type = entity_types_map[rel.e_i].value\n",
    "    e2_type = entity_types_map[rel.e_j].value\n",
    "    print(f\"  {e1_type:10s} → {e2_type:10s}: {rel.e_i} → {rel.e_j} ({rel.confidence:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45d1f1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph: 79 nodes, 121 edges\n",
      "Connected component: 56 nodes, 121 edges\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for e in filtered_entities:\n",
    "    G.add_node(e['text'], type=e['type'])\n",
    "\n",
    "for rel in explicit_relations:\n",
    "    G.add_edge(rel.e1, rel.e2, relation=rel.rel_type, \n",
    "               confidence=rel.confidence, edge_type='explicit')\n",
    "\n",
    "for rel in implicit_typed:\n",
    "    if rel.e_i in {e['text'] for e in filtered_entities}:\n",
    "        if rel.e_j in {e['text'] for e in filtered_entities}:\n",
    "            G.add_edge(rel.e_i, rel.e_j, confidence=rel.confidence, \n",
    "                       edge_type='implicit')\n",
    "\n",
    "print(f\"Knowledge graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "\n",
    "weakly_connected = list(nx.weakly_connected_components(G))\n",
    "largest_cc = max(weakly_connected, key=len)\n",
    "G_connected = G.subgraph(largest_cc).copy()\n",
    "\n",
    "print(f\"Connected component: {G_connected.number_of_nodes()} nodes, {G_connected.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bd0ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations saved to: output/1810.04805_relations_final.json\n",
      "Explicit: 22, Implicit: 99\n"
     ]
    }
   ],
   "source": [
    "relations_output = {\n",
    "    \"explicit\": [\n",
    "        {\n",
    "            \"source\": r.e1,\n",
    "            \"target\": r.e2,\n",
    "            \"type\": r.rel_type,\n",
    "            \"confidence\": r.confidence,\n",
    "            \"support\": r.n_supporting\n",
    "        }\n",
    "        for r in explicit_relations\n",
    "    ],\n",
    "    \"implicit\": [\n",
    "        {\n",
    "            \"source\": r.e_i,\n",
    "            \"target\": r.e_j,\n",
    "            \"confidence\": r.confidence,\n",
    "            \"n_bridges\": len(r.bridges),\n",
    "            \"bridges\": r.bridges\n",
    "        }\n",
    "        for r in implicit_typed\n",
    "    ]\n",
    "}\n",
    "\n",
    "relations_path = output_dir / f\"{arxiv_id}_relations_final.json\"\n",
    "with open(relations_path, \"w\") as f:\n",
    "    json.dump(relations_output, f, indent=2)\n",
    "\n",
    "print(f\"Relations saved to: {relations_path}\")\n",
    "print(f\"Explicit: {len(explicit_relations)}, Implicit: {len(implicit_typed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41194f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout & colors prepared\n",
      "Explicit edges: 22, Implicit edges: 99\n"
     ]
    }
   ],
   "source": [
    "pos = nx.spring_layout(G_connected, k=2.5, iterations=50, seed=42)\n",
    "\n",
    "explicit_edges = [\n",
    "    (u, v) for u, v, d in G_connected.edges(data=True)\n",
    "    if d.get('edge_type') == 'explicit'\n",
    "]\n",
    "\n",
    "implicit_edges = [\n",
    "    (u, v) for u, v, d in G_connected.edges(data=True)\n",
    "    if d.get('edge_type') == 'implicit'\n",
    "]\n",
    "\n",
    "type_to_color = {\n",
    "    EntityType.TASK.value: '#FF6B6B',\n",
    "    EntityType.METHOD.value: '#4ECDC4',\n",
    "    EntityType.DATASET.value: '#45B7D1',\n",
    "    EntityType.OBJECT.value: '#FFA07A',\n",
    "    EntityType.METRIC.value: '#98D8C8',\n",
    "}\n",
    "\n",
    "node_colors = []\n",
    "for node in G_connected.nodes():\n",
    "    entity = next((e for e in filtered_entities if e['text'] == node), None)\n",
    "    if entity:\n",
    "        color = type_to_color.get(entity['type'], '#CCCCCC')\n",
    "        node_colors.append(color)\n",
    "    else:\n",
    "        node_colors.append('#CCCCCC')\n",
    "\n",
    "print(f\"Layout & colors prepared\")\n",
    "print(f\"Explicit edges: {len(explicit_edges)}, Implicit edges: {len(implicit_edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gravis as gv\n",
    "\n",
    "\n",
    "g = G_connected.copy()\n",
    "\n",
    "\n",
    "type_to_color = {\n",
    "    EntityType.TASK.value: '#FF6B6B',\n",
    "    EntityType.METHOD.value: '#4ECDC4',\n",
    "    EntityType.DATASET.value: '#45B7D1',\n",
    "    EntityType.OBJECT.value: '#FFA07A',\n",
    "    EntityType.METRIC.value: '#98D8C8',\n",
    "}\n",
    "\n",
    "\n",
    "for node in g.nodes():\n",
    "    entity = next((e for e in filtered_entities if e['text'] == node), None)\n",
    "    entity_type = entity['type'] if entity else 'unknown'\n",
    "    color = type_to_color.get(entity_type, '#CCCCCC')\n",
    "    \n",
    "    g.nodes[node]['color'] = color\n",
    "    g.nodes[node]['title'] = f\"ID: {node}\\nTYPE: {entity_type}\"\n",
    "    g.nodes[node]['size'] = 20\n",
    "    g.nodes[node]['group'] = entity_type\n",
    "\n",
    "\n",
    "for u, v, data in g.edges(data=True):\n",
    "    edge_type = data.get('edge_type', 'implicit')\n",
    "    \n",
    "    if edge_type == 'explicit':\n",
    "        g[u][v]['color'] = '#2E86AB'\n",
    "        g[u][v]['width'] = 2.5\n",
    "    else:\n",
    "        g[u][v]['color'] = 'rgba(169, 169, 169, 0.4)'\n",
    "        g[u][v]['width'] = 2\n",
    "    \n",
    "    g[u][v]['title'] = f\"Type: {edge_type}\"\n",
    "\n",
    "fig = gv.vis(g)\n",
    "fig\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginkgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
