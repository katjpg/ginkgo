{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d925d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path.cwd().parent) not in sys.path:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import json\n",
    "from client.arxiv import ArXivClient\n",
    "from client.grobid import GROBIDClient\n",
    "from models.grobid import Form, File\n",
    "from parsers.tei import Parser\n",
    "from utils.clean_text import preprocess_section\n",
    "from nlp.semantic import EntityExtractor\n",
    "from config.llm import LangExtractConfig\n",
    "from config.nlp import NLPConfig, normalize_section\n",
    "from nlp.syntactic import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7470df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1810.04805\"\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "langextract_config = LangExtractConfig()\n",
    "nlp_config = NLPConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b453981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded to: output/1810.04805.pdf\n",
      "File size: 0.74 MB\n"
     ]
    }
   ],
   "source": [
    "arxiv_client = ArXivClient()\n",
    "metadata = arxiv_client.get_metadata(arxiv_id)\n",
    "pdf_path = output_dir / f\"{arxiv_id}.pdf\"\n",
    "arxiv_client.download_pdf(arxiv_id, str(pdf_path))\n",
    "\n",
    "print(f\"PDF downloaded to: {pdf_path}\")\n",
    "print(f\"File size: {pdf_path.stat().st_size / 1024 / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049a0df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID processing complete\n",
      "TEI XML saved to: output/1810.04805.tei.xml\n"
     ]
    }
   ],
   "source": [
    "grobid_client = GROBIDClient()\n",
    "pdf_bytes = pdf_path.read_bytes()\n",
    "\n",
    "form = Form(\n",
    "    file=File(payload=pdf_bytes, file_name=f\"{arxiv_id}.pdf\"),\n",
    "    consolidate_citations=1,\n",
    "    consolidate_header=1,\n",
    "    segment_sentences=True\n",
    ")\n",
    "\n",
    "response = grobid_client.process_pdf(form)\n",
    "tei_path = output_dir / f\"{arxiv_id}.tei.xml\"\n",
    "tei_path.write_bytes(response.content)\n",
    "\n",
    "print(f\"GROBID processing complete\")\n",
    "print(f\"TEI XML saved to: {tei_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151c605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections parsed: 27\n",
      "  - Introduction\n",
      "  - Related Work\n",
      "  - Unsupervised Feature-based Approaches\n",
      "  - Unsupervised Fine-tuning Approaches\n",
      "  - Transfer Learning from Supervised Data\n"
     ]
    }
   ],
   "source": [
    "parser = Parser(response.content)\n",
    "article = parser.parse()\n",
    "\n",
    "print(f\"Sections parsed: {len(article.sections)}\")\n",
    "for section in article.sections[:5]:\n",
    "    print(f\"  - {section.title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b7975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections extracted: 27\n"
     ]
    }
   ],
   "source": [
    "sections_data = []\n",
    "\n",
    "for section in article.sections:\n",
    "    section_text = \" \".join(p.plain_text for p in section.paragraphs)\n",
    "    clean_text = preprocess_section(section_text.strip())\n",
    "    \n",
    "    sections_data.append({\n",
    "        \"title\": section.title,\n",
    "        \"raw_text\": section_text.strip(),\n",
    "        \"clean_text\": clean_text\n",
    "    })\n",
    "\n",
    "print(f\"Sections extracted: {len(sections_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2b963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities extracted: 459\n"
     ]
    }
   ],
   "source": [
    "extractor = EntityExtractor(langextract_config)\n",
    "all_entities = []\n",
    "\n",
    "for section_data in sections_data:\n",
    "    normalized_title = normalize_section(section_data[\"title\"], nlp_config.patterns)\n",
    "    section_config = nlp_config.sections.get(normalized_title, nlp_config.sections[\"default\"])\n",
    "    \n",
    "    section_doc = nlp(section_data[\"clean_text\"])\n",
    "    section_entities = extractor.extract(section_data[\"clean_text\"], section_config)\n",
    "    entities_with_context = extractor.convert_to_spans(section_entities, section_doc, context_size=1)\n",
    "    \n",
    "    serializable_entities = []\n",
    "    for entity in entities_with_context:\n",
    "        entity_copy = dict(entity)\n",
    "        del entity_copy[\"span\"]\n",
    "        entity_copy[\"section\"] = section_data[\"title\"]\n",
    "        serializable_entities.append(entity_copy)\n",
    "    \n",
    "    all_entities.extend(serializable_entities)\n",
    "\n",
    "print(f\"Total entities extracted: {len(all_entities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3287da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section entity files saved\n"
     ]
    }
   ],
   "source": [
    "for i, section_data in enumerate(sections_data):\n",
    "    section_title = section_data[\"title\"].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    section_path = output_dir / f\"{arxiv_id}_{section_title}_entities.json\"\n",
    "    \n",
    "    section_entities = [e for e in all_entities if e.get(\"section\") == section_data[\"title\"]]\n",
    "    section_path.write_text(json.dumps(section_entities, indent=2))\n",
    "\n",
    "print(f\"Section entity files saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175ff47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text saved to: output/1810.04805_full_text.txt\n",
      "Character count: 37,651\n"
     ]
    }
   ],
   "source": [
    "full_clean_text = \"\\n\\n\".join(s[\"clean_text\"] for s in sections_data)\n",
    "full_text_path = output_dir / f\"{arxiv_id}_full_text.txt\"\n",
    "full_text_path.write_text(full_clean_text)\n",
    "\n",
    "print(f\"Full text saved to: {full_text_path}\")\n",
    "print(f\"Character count: {len(full_clean_text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad26014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entities saved to: output/1810.04805_all_entities.json\n",
      "Total entities: 459\n",
      "Entity type distribution:\n",
      "  other: 200\n",
      "  dataset: 88\n",
      "  task: 80\n",
      "  method: 76\n",
      "  metric: 15\n"
     ]
    }
   ],
   "source": [
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "all_entities_path.write_text(json.dumps(all_entities, indent=2))\n",
    "\n",
    "print(f\"All entities saved to: {all_entities_path}\")\n",
    "print(f\"Total entities: {len(all_entities)}\")\n",
    "\n",
    "type_counts = {}\n",
    "for entity in all_entities:\n",
    "    entity_type = entity.get(\"type\", \"unknown\")\n",
    "    type_counts[entity_type] = type_counts.get(entity_type, 0) + 1\n",
    "\n",
    "print(\"Entity type distribution:\")\n",
    "for entity_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {entity_type}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baabf297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities loaded: 459\n",
      "Full text characters: 37,651\n",
      "spaCy Doc tokens: 7300\n"
     ]
    }
   ],
   "source": [
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "full_text_path = output_dir / f\"{arxiv_id}_full_text.txt\"\n",
    "\n",
    "with open(all_entities_path) as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "full_text = full_text_path.read_text()\n",
    "doc = nlp(full_text)\n",
    "\n",
    "print(f\"Entities loaded: {len(all_entities)}\")\n",
    "print(f\"Full text characters: {len(full_text):,}\")\n",
    "print(f\"spaCy Doc tokens: {len(doc)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginkgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
