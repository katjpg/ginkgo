[
  {
    "text": "Extended Neural GPU",
    "type": "method",
    "char_interval": {
      "start_pos": 77,
      "end_pos": 96
    },
    "section": "Background"
  },
  {
    "text": "ByteNet",
    "type": "method",
    "char_interval": {
      "start_pos": 102,
      "end_pos": 109
    },
    "section": "Background"
  },
  {
    "text": "ConvS2S",
    "type": "method",
    "char_interval": {
      "start_pos": 118,
      "end_pos": 125
    },
    "section": "Background"
  },
  {
    "text": "convolutional neural networks",
    "type": "method",
    "char_interval": {
      "start_pos": 147,
      "end_pos": 176
    },
    "section": "Background"
  },
  {
    "text": "these models",
    "type": "generic",
    "char_interval": {
      "start_pos": 286,
      "end_pos": 298
    },
    "section": "Background"
  },
  {
    "text": "learning dependencies",
    "type": "task",
    "char_interval": null,
    "section": "Background"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 581,
      "end_pos": 592
    },
    "section": "Background"
  },
  {
    "text": "attention",
    "type": "other",
    "char_interval": {
      "start_pos": 713,
      "end_pos": 722
    },
    "section": "Background"
  },
  {
    "text": "Multi-Head Attention",
    "type": "other",
    "char_interval": {
      "start_pos": 772,
      "end_pos": 792
    },
    "section": "Background"
  },
  {
    "text": "Self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 822,
      "end_pos": 836
    },
    "section": "Background"
  },
  {
    "text": "intra-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 855,
      "end_pos": 870
    },
    "section": "Background"
  },
  {
    "text": "reading comprehension",
    "type": "task",
    "char_interval": {
      "start_pos": 1075,
      "end_pos": 1096
    },
    "section": "Background"
  },
  {
    "text": "abstractive summarization",
    "type": "task",
    "char_interval": {
      "start_pos": 1098,
      "end_pos": 1123
    },
    "section": "Background"
  },
  {
    "text": "textual entailment",
    "type": "task",
    "char_interval": {
      "start_pos": 1125,
      "end_pos": 1143
    },
    "section": "Background"
  },
  {
    "text": "learning task-independent sentence representations",
    "type": "task",
    "char_interval": {
      "start_pos": 1148,
      "end_pos": 1198
    },
    "section": "Background"
  },
  {
    "text": "End-to-end memory networks",
    "type": "method",
    "char_interval": {
      "start_pos": 1212,
      "end_pos": 1238
    },
    "section": "Background"
  },
  {
    "text": "recurrent attention mechanism",
    "type": "other",
    "char_interval": {
      "start_pos": 1254,
      "end_pos": 1283
    },
    "section": "Background"
  },
  {
    "text": "language question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 1368,
      "end_pos": 1395
    },
    "section": "Background"
  },
  {
    "text": "language modeling",
    "type": "task",
    "char_interval": {
      "start_pos": 1400,
      "end_pos": 1417
    },
    "section": "Background"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 1472,
      "end_pos": 1483
    },
    "section": "Background"
  },
  {
    "text": "self-attention",
    "type": "method",
    "char_interval": {
      "start_pos": 1536,
      "end_pos": 1550
    },
    "section": "Background"
  },
  {
    "text": "RNs",
    "type": "method",
    "char_interval": {
      "start_pos": 1632,
      "end_pos": 1635
    },
    "section": "Background"
  },
  {
    "text": "convolution",
    "type": "method",
    "char_interval": {
      "start_pos": 1639,
      "end_pos": 1650
    },
    "section": "Background"
  }
]