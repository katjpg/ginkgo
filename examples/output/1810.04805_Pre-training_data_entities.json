[
  {
    "text": "pre-training procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 26
    },
    "section": "Pre-training data"
  },
  {
    "text": "language model pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 70,
      "end_pos": 97
    },
    "section": "Pre-training data"
  },
  {
    "text": "Books Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 138,
      "end_pos": 150
    },
    "section": "Pre-training data"
  },
  {
    "text": "English Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 168,
      "end_pos": 185
    },
    "section": "Pre-training data"
  },
  {
    "text": "Wikipedia",
    "type": "generic",
    "char_interval": {
      "start_pos": 206,
      "end_pos": 215
    },
    "section": "Pre-training data"
  },
  {
    "text": "the text passages",
    "type": "generic",
    "char_interval": {
      "start_pos": 232,
      "end_pos": 249
    },
    "section": "Pre-training data"
  },
  {
    "text": "Billion Word Benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 392,
      "end_pos": 414
    },
    "section": "Pre-training data"
  },
  {
    "text": "long contiguous sequences",
    "type": "object",
    "char_interval": {
      "start_pos": 435,
      "end_pos": 460
    },
    "section": "Pre-training data"
  }
]