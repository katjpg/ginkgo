[
  {
    "text": "language model pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 70,
      "end_pos": 97
    },
    "attributes": {},
    "sentence_context": "The pre-training procedure largely follows the existing literature on language model pre-training.",
    "sentence_index": 0,
    "section": "Pre-training data"
  },
  {
    "text": "Books Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 138,
      "end_pos": 150
    },
    "attributes": {},
    "sentence_context": "For the pre-training corpus we use the Books Corpus (800M words) and English Wikipedia (2,500M words).",
    "sentence_index": 1,
    "section": "Pre-training data"
  },
  {
    "text": "English Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 168,
      "end_pos": 185
    },
    "attributes": {},
    "sentence_context": "For the pre-training corpus we use the Books Corpus (800M words) and English Wikipedia (2,500M words).",
    "sentence_index": 1,
    "section": "Pre-training data"
  },
  {
    "text": "text passages",
    "type": "other",
    "char_interval": {
      "start_pos": 236,
      "end_pos": 249
    },
    "attributes": {},
    "sentence_context": "For Wikipedia we extract only the text passages and ignore lists, tables, and headers.",
    "sentence_index": 2,
    "section": "Pre-training data"
  },
  {
    "text": "document-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 313,
      "end_pos": 334
    },
    "attributes": {},
    "sentence_context": "It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.",
    "sentence_index": 3,
    "section": "Pre-training data"
  },
  {
    "text": "shuffled sentence-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 349,
      "end_pos": 379
    },
    "attributes": {},
    "sentence_context": "It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.",
    "sentence_index": 3,
    "section": "Pre-training data"
  },
  {
    "text": "Billion Word Benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 392,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.",
    "sentence_index": 3,
    "section": "Pre-training data"
  }
]