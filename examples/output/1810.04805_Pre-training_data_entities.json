[
  {
    "text": "language model pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 70,
      "end_pos": 97
    },
    "attributes": {},
    "sentence_context": "The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the Books Corpus (800M words) and English Wikipedia (2,500M words).",
    "sentence_index": 0,
    "span_text": "language model pre-training",
    "section": "Pre-training data"
  },
  {
    "text": "Books Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 138,
      "end_pos": 150
    },
    "attributes": {},
    "sentence_context": "The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the Books Corpus (800M words) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers.",
    "sentence_index": 1,
    "span_text": "Books Corpus",
    "section": "Pre-training data"
  },
  {
    "text": "English Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 168,
      "end_pos": 185
    },
    "attributes": {},
    "sentence_context": "The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the Books Corpus (800M words) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers.",
    "sentence_index": 1,
    "span_text": "English Wikipedia",
    "section": "Pre-training data"
  },
  {
    "text": "text passages",
    "type": "other",
    "char_interval": {
      "start_pos": 236,
      "end_pos": 249
    },
    "attributes": {},
    "sentence_context": "For the pre-training corpus we use the Books Corpus (800M words) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.",
    "sentence_index": 2,
    "span_text": "text passages",
    "section": "Pre-training data"
  },
  {
    "text": "document-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 313,
      "end_pos": 334
    },
    "attributes": {},
    "sentence_context": "For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.",
    "sentence_index": 3,
    "span_text": "document-level corpus",
    "section": "Pre-training data"
  },
  {
    "text": "shuffled sentence-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 349,
      "end_pos": 379
    },
    "attributes": {},
    "sentence_context": "For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.",
    "sentence_index": 3,
    "span_text": "shuffled sentence-level corpus",
    "section": "Pre-training data"
  },
  {
    "text": "Billion Word Benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 392,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences.",
    "sentence_index": 3,
    "span_text": "Billion Word Benchmark",
    "section": "Pre-training data"
  }
]