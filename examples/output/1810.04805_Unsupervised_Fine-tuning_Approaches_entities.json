[
  {
    "text": "feature-based approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 12,
      "end_pos": 36
    },
    "attributes": {},
    "sentence_context": "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text. More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task.",
    "sentence_index": 0,
    "span_text": "feature-based approaches",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "word embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 89,
      "end_pos": 103
    },
    "attributes": {},
    "sentence_context": "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text. More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task.",
    "sentence_index": 0,
    "span_text": "word embedding",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "sentence encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 151,
      "end_pos": 159
    },
    "attributes": {},
    "sentence_context": "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text. More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task. The advantage of these approaches is that few parameters need to be learned from scratch.",
    "sentence_index": 1,
    "span_text": "sentence",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "document encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 163,
      "end_pos": 180
    },
    "attributes": {},
    "sentence_context": "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text. More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task. The advantage of these approaches is that few parameters need to be learned from scratch.",
    "sentence_index": 1,
    "span_text": "document encoders",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "contextual token representations",
    "type": "other",
    "char_interval": {
      "start_pos": 195,
      "end_pos": 227
    },
    "attributes": {},
    "sentence_context": "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text. More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task. The advantage of these approaches is that few parameters need to be learned from scratch.",
    "sentence_index": 1,
    "span_text": "contextual token representations",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 572,
      "end_pos": 586
    },
    "attributes": {},
    "sentence_context": "At least partly due to this advantage, Open AI GPT achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark. Left-to-right language model-BERT BERT. ing and auto-encoder objectives have been used for pre-training such models.",
    "sentence_index": 4,
    "span_text": "language model",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 448,
      "end_pos": 459
    },
    "attributes": {},
    "sentence_context": "The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, Open AI GPT achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark. Left-to-right language model-BERT BERT.",
    "sentence_index": 3,
    "span_text": "Open AI GPT",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 151,
      "end_pos": 317
    },
    "attributes": {},
    "sentence_context": "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text. More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task. The advantage of these approaches is that few parameters need to be learned from scratch.",
    "sentence_index": 1,
    "span_text": "sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "GLUE benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 542,
      "end_pos": 556
    },
    "attributes": {},
    "sentence_context": "The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, Open AI GPT achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark. Left-to-right language model-BERT BERT.",
    "sentence_index": 3,
    "span_text": "GLUE benchmark",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 587,
      "end_pos": 591
    },
    "attributes": {},
    "sentence_context": "At least partly due to this advantage, Open AI GPT achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark. Left-to-right language model-BERT BERT. ing and auto-encoder objectives have been used for pre-training such models.",
    "sentence_index": 4,
    "span_text": "BERT",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 572,
      "end_pos": 586
    },
    "attributes": {},
    "sentence_context": "At least partly due to this advantage, Open AI GPT achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark. Left-to-right language model-BERT BERT. ing and auto-encoder objectives have been used for pre-training such models.",
    "sentence_index": 4,
    "span_text": "language model",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "auto-encoder objectives",
    "type": "other",
    "char_interval": {
      "start_pos": 606,
      "end_pos": 629
    },
    "attributes": {},
    "sentence_context": "Left-to-right language model-BERT BERT. ing and auto-encoder objectives have been used for pre-training such models.",
    "sentence_index": 5,
    "span_text": "auto-encoder objectives",
    "section": "Unsupervised Fine-tuning Approaches"
  }
]