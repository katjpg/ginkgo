[
  {
    "text": "self-attention layers",
    "type": "other",
    "char_interval": {
      "start_pos": 46,
      "end_pos": 67
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "recurrent and convolutional layers",
    "type": "other",
    "char_interval": {
      "start_pos": 75,
      "end_pos": 109
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "sequence of symbol representations",
    "type": "object",
    "char_interval": {
      "start_pos": 156,
      "end_pos": 190
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "sequence",
    "type": "object",
    "char_interval": {
      "start_pos": 218,
      "end_pos": 226
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "hidden layer",
    "type": "object",
    "char_interval": {
      "start_pos": 291,
      "end_pos": 303
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "sequence transduction",
    "type": "task",
    "char_interval": {
      "start_pos": 317,
      "end_pos": 338
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 381,
      "end_pos": 395
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "computational complexity",
    "type": "other",
    "char_interval": {
      "start_pos": 443,
      "end_pos": 467
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "computation",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 515
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "sequential operations",
    "type": "other",
    "char_interval": {
      "start_pos": 579,
      "end_pos": 600
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "path length",
    "type": "other",
    "char_interval": {
      "start_pos": 628,
      "end_pos": 639
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "long-range dependencies",
    "type": "other",
    "char_interval": {
      "start_pos": 648,
      "end_pos": 671
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "Learning long-range dependencies",
    "type": "task",
    "char_interval": {
      "start_pos": 688,
      "end_pos": 720
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "paths",
    "type": "other",
    "char_interval": {
      "start_pos": 862,
      "end_pos": 867
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "signals",
    "type": "other",
    "char_interval": {
      "start_pos": 889,
      "end_pos": 896
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "network",
    "type": "other",
    "char_interval": {
      "start_pos": 921,
      "end_pos": 928
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "path length",
    "type": "other",
    "char_interval": {
      "start_pos": 1115,
      "end_pos": 1126
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "self-attention layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1243,
      "end_pos": 1263
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "recurrent layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1357,
      "end_pos": 1372
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "computational complexity",
    "type": "other",
    "char_interval": {
      "start_pos": 1422,
      "end_pos": 1446
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "self-attention layers",
    "type": "other",
    "char_interval": {
      "start_pos": 1448,
      "end_pos": 1469
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "recurrent layers",
    "type": "other",
    "char_interval": {
      "start_pos": 1486,
      "end_pos": 1502
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "sentence representations",
    "type": "object",
    "char_interval": {
      "start_pos": 1617,
      "end_pos": 1641
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "machine translations",
    "type": "task",
    "char_interval": {
      "start_pos": 1677,
      "end_pos": 1697
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "word-piece",
    "type": "other",
    "char_interval": {
      "start_pos": 1707,
      "end_pos": 1717
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "byte-pair",
    "type": "other",
    "char_interval": {
      "start_pos": 1726,
      "end_pos": 1735
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "computational performance",
    "type": "other",
    "char_interval": {
      "start_pos": 1768,
      "end_pos": 1793
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "long sequences",
    "type": "other",
    "char_interval": {
      "start_pos": 1819,
      "end_pos": 1833
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1835,
      "end_pos": 1849
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "neighborhood",
    "type": "other",
    "char_interval": {
      "start_pos": 1892,
      "end_pos": 1904
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "input sequence",
    "type": "object",
    "char_interval": {
      "start_pos": 1922,
      "end_pos": 1936
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "output position",
    "type": "object",
    "char_interval": {
      "start_pos": 1968,
      "end_pos": 1983
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "the maximum path length",
    "type": "generic",
    "char_interval": {
      "start_pos": 2005,
      "end_pos": 2028
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "this approach",
    "type": "generic",
    "char_interval": {
      "start_pos": 2063,
      "end_pos": 2076
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "convolutional layer",
    "type": "other",
    "char_interval": {
      "start_pos": 2110,
      "end_pos": 2129
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "contiguous kernels",
    "type": "other",
    "char_interval": {
      "start_pos": 2284,
      "end_pos": 2302
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "dilated convolutions",
    "type": "other",
    "char_interval": {
      "start_pos": 2335,
      "end_pos": 2355
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "Convolutional layers",
    "type": "other",
    "char_interval": {
      "start_pos": 2248,
      "end_pos": 2268
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "recurrent layers",
    "type": "other",
    "char_interval": {
      "start_pos": 2501,
      "end_pos": 2517
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "Separable convolutions",
    "type": "other",
    "char_interval": {
      "start_pos": 2537,
      "end_pos": 2559
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "separable convolution",
    "type": "other",
    "char_interval": {
      "start_pos": 2664,
      "end_pos": 2685
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "self-attention",
    "type": "method",
    "char_interval": {
      "start_pos": 2719,
      "end_pos": 2733
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "point-wise feed-forward layer",
    "type": "other",
    "char_interval": {
      "start_pos": 2746,
      "end_pos": 2775
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "our model",
    "type": "generic",
    "char_interval": {
      "start_pos": 2801,
      "end_pos": 2810
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "self-attention",
    "type": "method",
    "char_interval": {
      "start_pos": 2829,
      "end_pos": 2843
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "our models",
    "type": "generic",
    "char_interval": {
      "start_pos": 2801,
      "end_pos": 2810
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "attention heads",
    "type": "other",
    "char_interval": {
      "start_pos": 3007,
      "end_pos": 3022
    },
    "section": "Why Self-Attention"
  },
  {
    "text": "attention distributions",
    "type": "other",
    "char_interval": {
      "start_pos": 2894,
      "end_pos": 2917
    },
    "section": "Why Self-Attention"
  }
]