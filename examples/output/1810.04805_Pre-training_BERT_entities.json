[
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 108,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": ". and Radford et al., we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section.",
    "sentence_index": 1,
    "span_text": "BERT",
    "section": "Pre-training BERT"
  },
  {
    "text": "Masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 260,
      "end_pos": 269
    },
    "attributes": {},
    "sentence_context": "This step is presented in the left part of Figure 1. Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model.",
    "sentence_index": 4,
    "span_text": "Masked LM",
    "section": "Pre-training BERT"
  },
  {
    "text": "Transformer encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 780,
      "end_pos": 799
    },
    "attributes": {},
    "sentence_context": "Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \"see itself\", and the model could trivially predict the target word in a multi-layered context. former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"Transformer decoder\" since it can be used for text generation. In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.",
    "sentence_index": 7,
    "span_text": "Transformer encoder",
    "section": "Pre-training BERT"
  },
  {
    "text": "Transformer decoder",
    "type": "other",
    "char_interval": {
      "start_pos": 858,
      "end_pos": 877
    },
    "attributes": {},
    "sentence_context": "Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \"see itself\", and the model could trivially predict the target word in a multi-layered context. former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"Transformer decoder\" since it can be used for text generation. In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.",
    "sentence_index": 7,
    "span_text": "Transformer decoder",
    "section": "Pre-training BERT"
  },
  {
    "text": "text generation",
    "type": "task",
    "char_interval": {
      "start_pos": 904,
      "end_pos": 919
    },
    "attributes": {},
    "sentence_context": "Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \"see itself\", and the model could trivially predict the target word in a multi-layered context. former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"Transformer decoder\" since it can be used for text generation. In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.",
    "sentence_index": 7,
    "span_text": "text generation",
    "section": "Pre-training BERT"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 1109,
      "end_pos": 1118
    },
    "attributes": {},
    "sentence_context": "In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \"masked LM\" (MLM), although it is often referred to as a Cloze task in the literature. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.",
    "sentence_index": 9,
    "span_text": "masked LM",
    "section": "Pre-training BERT"
  },
  {
    "text": "Cloze task",
    "type": "task",
    "char_interval": {
      "start_pos": 1165,
      "end_pos": 1175
    },
    "attributes": {},
    "sentence_context": "In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \"masked LM\" (MLM), although it is often referred to as a Cloze task in the literature. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.",
    "sentence_index": 9,
    "span_text": "Cloze task",
    "section": "Pre-training BERT"
  },
  {
    "text": "denoising auto-encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 1447,
      "end_pos": 1470
    },
    "attributes": {},
    "sentence_context": "In all of our experiments, we mask 15% of all Word Piece tokens in each sequence at random. In contrast to denoising auto-encoders, we only predict the masked words rather than reconstructing the entire input. Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning.",
    "sentence_index": 12,
    "span_text": "denoising auto-encoders",
    "section": "Pre-training BERT"
  },
  {
    "text": "pre-trained model",
    "type": "other",
    "char_interval": {
      "start_pos": 1600,
      "end_pos": 1617
    },
    "attributes": {},
    "sentence_context": "In contrast to denoising auto-encoders, we only predict the masked words rather than reconstructing the entire input. Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace \"masked\" words with the actual [MASK] token.",
    "sentence_index": 13,
    "span_text": "pre-trained model",
    "section": "Pre-training BERT"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1854,
      "end_pos": 1867
    },
    "attributes": {},
    "sentence_context": "To mitigate this, we do not always replace \"masked\" words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.",
    "sentence_index": 15,
    "span_text": "training data",
    "section": "Pre-training BERT"
  },
  {
    "text": "Next Sentence Prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2261,
      "end_pos": 2285
    },
    "attributes": {},
    "sentence_context": "We compare variations of this procedure in Appendix C.2. Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling.",
    "sentence_index": 19,
    "span_text": "Next Sentence Prediction",
    "section": "Pre-training BERT"
  },
  {
    "text": "Question Answering",
    "type": "task",
    "char_interval": {
      "start_pos": 2332,
      "end_pos": 2350
    },
    "attributes": {},
    "sentence_context": "Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.",
    "sentence_index": 20,
    "span_text": "Question Answering",
    "section": "Pre-training BERT"
  },
  {
    "text": "Natural Language Inference",
    "type": "task",
    "char_interval": {
      "start_pos": 2360,
      "end_pos": 2386
    },
    "attributes": {},
    "sentence_context": "Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.",
    "sentence_index": 20,
    "span_text": "Natural Language Inference",
    "section": "Pre-training BERT"
  },
  {
    "text": "monolingual corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2680,
      "end_pos": 2698
    },
    "attributes": {},
    "sentence_context": "Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as Not Next).",
    "sentence_index": 21,
    "span_text": "monolingual corpus",
    "section": "Pre-training BERT"
  },
  {
    "text": "pretraining example",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2759,
      "end_pos": 2778
    },
    "attributes": {},
    "sentence_context": "In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as Not Next). As we show in Figure 1, C is used for next sentence prediction (NSP).",
    "sentence_index": 22,
    "span_text": "pretraining example",
    "section": "Pre-training BERT"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2983,
      "end_pos": 3007
    },
    "attributes": {},
    "sentence_context": "Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as Not Next). As we show in Figure 1, C is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.",
    "sentence_index": 23,
    "span_text": "next sentence prediction",
    "section": "Pre-training BERT"
  },
  {
    "text": "corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2915,
      "end_pos": 2921
    },
    "attributes": {},
    "sentence_context": "In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as Not Next). As we show in Figure 1, C is used for next sentence prediction (NSP).",
    "sentence_index": 22,
    "span_text": "corpus",
    "section": "Pre-training BERT"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2983,
      "end_pos": 3007
    },
    "attributes": {},
    "sentence_context": "Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as Not Next). As we show in Figure 1, C is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.",
    "sentence_index": 23,
    "span_text": "next sentence prediction",
    "section": "Pre-training BERT"
  },
  {
    "text": "QA",
    "type": "task",
    "char_interval": {
      "start_pos": 3132,
      "end_pos": 3134
    },
    "attributes": {},
    "sentence_context": "As we show in Figure 1, C is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. Position Embeddings",
    "sentence_index": 24,
    "span_text": "QA",
    "section": "Pre-training BERT"
  },
  {
    "text": "NLI",
    "type": "task",
    "char_interval": {
      "start_pos": 3139,
      "end_pos": 3142
    },
    "attributes": {},
    "sentence_context": "As we show in Figure 1, C is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. Position Embeddings",
    "sentence_index": 24,
    "span_text": "NLI",
    "section": "Pre-training BERT"
  },
  {
    "text": "Position Embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 3144,
      "end_pos": 3163
    },
    "attributes": {},
    "sentence_context": "Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. Position Embeddings The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee.",
    "sentence_index": 25,
    "span_text": "Position Embeddings",
    "section": "Pre-training BERT"
  },
  {
    "text": "NSP task",
    "type": "task",
    "char_interval": {
      "start_pos": 3168,
      "end_pos": 3176
    },
    "attributes": {},
    "sentence_context": "Position Embeddings The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee. However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.",
    "sentence_index": 26,
    "span_text": "NSP task",
    "section": "Pre-training BERT"
  }
]