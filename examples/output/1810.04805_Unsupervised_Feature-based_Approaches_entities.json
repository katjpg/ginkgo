[
  {
    "text": "representations",
    "type": "other",
    "char_interval": {
      "start_pos": 27,
      "end_pos": 42
    },
    "attributes": {},
    "sentence_context": "Learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch.",
    "sentence_index": 0,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 154,
      "end_pos": 169
    },
    "attributes": {},
    "sentence_context": "Learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context.",
    "sentence_index": 1,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 253,
      "end_pos": 263
    },
    "attributes": {},
    "sentence_context": "Learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context.",
    "sentence_index": 1,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embedding vectors",
    "type": "other",
    "char_interval": {
      "start_pos": 298,
      "end_pos": 320
    },
    "attributes": {},
    "sentence_context": "Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context. These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings.",
    "sentence_index": 2,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "language modeling",
    "type": "other",
    "char_interval": {
      "start_pos": 336,
      "end_pos": 353
    },
    "attributes": {},
    "sentence_context": "Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context. These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings.",
    "sentence_index": 2,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "sentence embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 548,
      "end_pos": 567
    },
    "attributes": {},
    "sentence_context": "To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context. These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives.",
    "sentence_index": 3,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "paragraph embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 571,
      "end_pos": 591
    },
    "attributes": {},
    "sentence_context": "To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context. These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives.",
    "sentence_index": 3,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "sentence representations",
    "type": "other",
    "char_interval": {
      "start_pos": 602,
      "end_pos": 626
    },
    "attributes": {},
    "sentence_context": "These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives. ELMo and its predecessor ) generalize traditional word embedding research along a different dimension.",
    "sentence_index": 4,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "denoising autoencoder",
    "type": "other",
    "char_interval": {
      "start_pos": 793,
      "end_pos": 814
    },
    "attributes": {},
    "sentence_context": "These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives. ELMo and its predecessor ) generalize traditional word embedding research along a different dimension.",
    "sentence_index": 4,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 835,
      "end_pos": 839
    },
    "attributes": {},
    "sentence_context": "To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives. ELMo and its predecessor ) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model.",
    "sentence_index": 5,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embedding research",
    "type": "other",
    "char_interval": {
      "start_pos": 885,
      "end_pos": 908
    },
    "attributes": {},
    "sentence_context": "To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives. ELMo and its predecessor ) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model.",
    "sentence_index": 5,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "context-sensitive features",
    "type": "other",
    "char_interval": {
      "start_pos": 951,
      "end_pos": 977
    },
    "attributes": {},
    "sentence_context": "ELMo and its predecessor ) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations.",
    "sentence_index": 6,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 1019,
      "end_pos": 1033
    },
    "attributes": {},
    "sentence_context": "ELMo and its predecessor ) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations.",
    "sentence_index": 6,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "contextual representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1039,
      "end_pos": 1064
    },
    "attributes": {},
    "sentence_context": "They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs.",
    "sentence_index": 7,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1184,
      "end_pos": 1199
    },
    "attributes": {},
    "sentence_context": "The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 1321,
      "end_pos": 1339
    },
    "attributes": {},
    "sentence_context": "The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "sentiment analysis",
    "type": "task",
    "char_interval": {
      "start_pos": 1341,
      "end_pos": 1359
    },
    "attributes": {},
    "sentence_context": "The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "named entity recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 1365,
      "end_pos": 1389
    },
    "attributes": {},
    "sentence_context": "The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "predict a single word",
    "type": "task",
    "char_interval": {
      "start_pos": 1455,
      "end_pos": 1476
    },
    "attributes": {},
    "sentence_context": "The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "LSTMs",
    "type": "method",
    "char_interval": {
      "start_pos": 1516,
      "end_pos": 1521
    },
    "attributes": {},
    "sentence_context": "The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 1534,
      "end_pos": 1538
    },
    "attributes": {},
    "sentence_context": "When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 9,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "feature-based",
    "type": "other",
    "char_interval": {
      "start_pos": 1555,
      "end_pos": 1568
    },
    "attributes": {},
    "sentence_context": "When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 9,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "deeply bidirectional",
    "type": "other",
    "char_interval": {
      "start_pos": 1577,
      "end_pos": 1597
    },
    "attributes": {},
    "sentence_context": "When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 9,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "cloze task",
    "type": "other",
    "char_interval": {
      "start_pos": 1615,
      "end_pos": 1625
    },
    "attributes": {},
    "sentence_context": "When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 9,
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "text generation",
    "type": "task",
    "char_interval": {
      "start_pos": 1667,
      "end_pos": 1682
    },
    "attributes": {},
    "sentence_context": "When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition.. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.. shows that the cloze task can be used to improve the robustness of text generation models.",
    "sentence_index": 9,
    "section": "Unsupervised Feature-based Approaches"
  }
]