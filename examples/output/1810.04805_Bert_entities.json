[
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 13,
      "end_pos": 17
    },
    "attributes": {},
    "sentence_context": "We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning.",
    "sentence_index": 0,
    "span_text": "BERT",
    "section": "Bert"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 105,
      "end_pos": 117
    },
    "attributes": {},
    "sentence_context": "We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks.",
    "sentence_index": 1,
    "span_text": "pre-training",
    "section": "Bert"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 122,
      "end_pos": 133
    },
    "attributes": {},
    "sentence_context": "We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks.",
    "sentence_index": 1,
    "span_text": "fine-tuning",
    "section": "Bert"
  },
  {
    "text": "unlabeled data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 180,
      "end_pos": 194
    },
    "attributes": {},
    "sentence_context": "There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.",
    "sentence_index": 2,
    "span_text": "unlabeled data",
    "section": "Bert"
  },
  {
    "text": "pre-training tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 210,
      "end_pos": 228
    },
    "attributes": {},
    "sentence_context": "There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.",
    "sentence_index": 2,
    "span_text": "pre-training tasks",
    "section": "Bert"
  },
  {
    "text": "labeled data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 362,
      "end_pos": 374
    },
    "attributes": {},
    "sentence_context": "During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.",
    "sentence_index": 3,
    "span_text": "labeled data",
    "section": "Bert"
  },
  {
    "text": "downstream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 384,
      "end_pos": 400
    },
    "attributes": {},
    "sentence_context": "During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.",
    "sentence_index": 3,
    "span_text": "downstream tasks",
    "section": "Bert"
  },
  {
    "text": "question-answering",
    "type": "task",
    "char_interval": {
      "start_pos": 530,
      "end_pos": 548
    },
    "attributes": {},
    "sentence_context": "Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its unified architecture across different tasks.",
    "sentence_index": 5,
    "span_text": "question-answering",
    "section": "Bert"
  },
  {
    "text": "BERT model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 644,
      "end_pos": 648
    },
    "attributes": {},
    "sentence_context": "The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its unified architecture across different tasks. There is mini-mal difference between the pre-trained architecture and the final downstream architecture.",
    "sentence_index": 6,
    "span_text": "BERT",
    "section": "Bert"
  },
  {
    "text": "Transformer encoder",
    "type": "method",
    "char_interval": {
      "start_pos": 882,
      "end_pos": 901
    },
    "attributes": {},
    "sentence_context": "There is mini-mal difference between the pre-trained architecture and the final downstream architecture. Model Architecture BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. and released in the tensor 2tensor library. Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. as well as excellent guides such as \"The Annotated Transformer.",
    "sentence_index": 8,
    "span_text": "Transformer encoder",
    "section": "Bert"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1415,
      "end_pos": 1429
    },
    "attributes": {},
    "sentence_context": "\"2n this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3We primarily report results on two model sizes: BERT BASE (L=12, H=768, A=12, Total Param-eters=110M) and BERT LARGE. BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes.",
    "sentence_index": 11,
    "span_text": "self-attention",
    "section": "Bert"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1490,
      "end_pos": 1499
    },
    "attributes": {},
    "sentence_context": "\"2n this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3We primarily report results on two model sizes: BERT BASE (L=12, H=768, A=12, Total Param-eters=110M) and BERT LARGE. BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes.",
    "sentence_index": 11,
    "span_text": "BERT BASE",
    "section": "Bert"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1548,
      "end_pos": 1558
    },
    "attributes": {},
    "sentence_context": "\"2n this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3We primarily report results on two model sizes: BERT BASE (L=12, H=768, A=12, Total Param-eters=110M) and BERT LARGE. BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes.",
    "sentence_index": 11,
    "span_text": "BERT LARGE",
    "section": "Bert"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1612,
      "end_pos": 1623
    },
    "attributes": {},
    "sentence_context": "this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3We primarily report results on two model sizes: BERT BASE (L=12, H=768, A=12, Total Param-eters=110M) and BERT LARGE. BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.",
    "sentence_index": 12,
    "span_text": "Open AI GPT",
    "section": "Bert"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1674,
      "end_pos": 1678
    },
    "attributes": {},
    "sentence_context": "BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence.",
    "sentence_index": 13,
    "span_text": "BERT",
    "section": "Bert"
  },
  {
    "text": "bidirectional self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1696,
      "end_pos": 1724
    },
    "attributes": {},
    "sentence_context": "BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence.",
    "sentence_index": 13,
    "span_text": "bidirectional self-attention",
    "section": "Bert"
  },
  {
    "text": "GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1736,
      "end_pos": 1739
    },
    "attributes": {},
    "sentence_context": "BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence.",
    "sentence_index": 13,
    "span_text": "GPT",
    "section": "Bert"
  },
  {
    "text": "constrained self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1757,
      "end_pos": 1783
    },
    "attributes": {},
    "sentence_context": "BERT BASE was chosen to have the same model size as Open AI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence.",
    "sentence_index": 13,
    "span_text": "constrained self-attention",
    "section": "Bert"
  },
  {
    "text": "down-stream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1906,
      "end_pos": 1923
    },
    "attributes": {},
    "sentence_context": "Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence.",
    "sentence_index": 14,
    "span_text": "down-stream tasks",
    "section": "Bert"
  },
  {
    "text": "sentence pair",
    "type": "other",
    "char_interval": {
      "start_pos": 1999,
      "end_pos": 2007
    },
    "attributes": {},
    "sentence_context": "Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence.",
    "sentence_index": 14,
    "span_text": "sentence",
    "section": "Bert"
  },
  {
    "text": "Question",
    "type": "other",
    "char_interval": {
      "start_pos": 2039,
      "end_pos": 2047
    },
    "attributes": {},
    "sentence_context": "Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence.",
    "sentence_index": 14,
    "span_text": "Question",
    "section": "Bert"
  },
  {
    "text": "Answer",
    "type": "other",
    "char_interval": {
      "start_pos": 2049,
      "end_pos": 2055
    },
    "attributes": {},
    "sentence_context": "Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence.",
    "sentence_index": 14,
    "span_text": "Answer",
    "section": "Bert"
  },
  {
    "text": "span of contiguous text",
    "type": "other",
    "char_interval": {
      "start_pos": 2136,
      "end_pos": 2159
    },
    "attributes": {},
    "sentence_context": "4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \"sequence\" refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.",
    "sentence_index": 15,
    "span_text": "span of contiguous text",
    "section": "Bert"
  },
  {
    "text": "sequence",
    "type": "other",
    "char_interval": {
      "start_pos": 2207,
      "end_pos": 2215
    },
    "attributes": {},
    "sentence_context": "Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \"sequence\" refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. We use Word Piece embeddings with a 30,000 token vocabulary.",
    "sentence_index": 16,
    "span_text": "sequence",
    "section": "Bert"
  },
  {
    "text": "Word Piece embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 2333,
      "end_pos": 2354
    },
    "attributes": {},
    "sentence_context": "A \"sequence\" refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. We use Word Piece embeddings with a 30,000 token vocabulary. The first token of every sequence is always a special classification token([CLS]).",
    "sentence_index": 17,
    "span_text": "Word Piece embeddings",
    "section": "Bert"
  },
  {
    "text": "classification token",
    "type": "other",
    "char_interval": {
      "start_pos": 2441,
      "end_pos": 2461
    },
    "attributes": {},
    "sentence_context": "We use Word Piece embeddings with a 30,000 token vocabulary. The first token of every sequence is always a special classification token([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.",
    "sentence_index": 18,
    "span_text": "classification token([CLS",
    "section": "Bert"
  },
  {
    "text": "classification tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 2574,
      "end_pos": 2594
    },
    "attributes": {},
    "sentence_context": "The first token of every sequence is always a special classification token([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence.",
    "sentence_index": 19,
    "span_text": "classification tasks",
    "section": "Bert"
  },
  {
    "text": "Sentence pairs",
    "type": "other",
    "char_interval": {
      "start_pos": 2596,
      "end_pos": 2610
    },
    "attributes": {},
    "sentence_context": "The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways.",
    "sentence_index": 20,
    "span_text": "Sentence pairs",
    "section": "Bert"
  },
  {
    "text": "special token",
    "type": "other",
    "char_interval": {
      "start_pos": 2730,
      "end_pos": 2743
    },
    "attributes": {},
    "sentence_context": "We differentiate the sentences in two ways. First, we separate them with a special token([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B.",
    "sentence_index": 22,
    "span_text": "special token([SEP",
    "section": "Bert"
  },
  {
    "text": "learned embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 2769,
      "end_pos": 2786
    },
    "attributes": {},
    "sentence_context": "First, we separate them with a special token([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.",
    "sentence_index": 23,
    "span_text": "learned embedding",
    "section": "Bert"
  },
  {
    "text": "sentence A",
    "type": "other",
    "char_interval": {
      "start_pos": 2835,
      "end_pos": 2845
    },
    "attributes": {},
    "sentence_context": "First, we separate them with a special token([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.",
    "sentence_index": 23,
    "span_text": "sentence A",
    "section": "Bert"
  },
  {
    "text": "sentence B",
    "type": "other",
    "char_interval": {
      "start_pos": 2849,
      "end_pos": 2859
    },
    "attributes": {},
    "sentence_context": "First, we separate them with a special token([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.",
    "sentence_index": 23,
    "span_text": "sentence B.",
    "section": "Bert"
  },
  {
    "text": "input embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 2893,
      "end_pos": 2908
    },
    "attributes": {},
    "sentence_context": "Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
    "sentence_index": 24,
    "span_text": "input embedding",
    "section": "Bert"
  },
  {
    "text": "hidden vector",
    "type": "other",
    "char_interval": {
      "start_pos": 2925,
      "end_pos": 2938
    },
    "attributes": {},
    "sentence_context": "Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
    "sentence_index": 24,
    "span_text": "hidden vector",
    "section": "Bert"
  },
  {
    "text": "hidden vector",
    "type": "other",
    "char_interval": {
      "start_pos": 2992,
      "end_pos": 3005
    },
    "attributes": {},
    "sentence_context": "Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
    "sentence_index": 24,
    "span_text": "hidden vector",
    "section": "Bert"
  },
  {
    "text": "input representation",
    "type": "other",
    "char_interval": {
      "start_pos": 3057,
      "end_pos": 3077
    },
    "attributes": {},
    "sentence_context": "Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
    "sentence_index": 24,
    "span_text": "input representation",
    "section": "Bert"
  },
  {
    "text": "token",
    "type": "other",
    "char_interval": {
      "start_pos": 3122,
      "end_pos": 3127
    },
    "attributes": {},
    "sentence_context": "Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
    "sentence_index": 24,
    "span_text": "token",
    "section": "Bert"
  },
  {
    "text": "segment",
    "type": "other",
    "char_interval": {
      "start_pos": 3129,
      "end_pos": 3136
    },
    "attributes": {},
    "sentence_context": "Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
    "sentence_index": 24,
    "span_text": "segment",
    "section": "Bert"
  },
  {
    "text": "position embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 3142,
      "end_pos": 3161
    },
    "attributes": {},
    "sentence_context": "Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C \u2208 R H, and the final hidden vector for the i th input token as For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.",
    "sentence_index": 24,
    "span_text": "position embeddings",
    "section": "Bert"
  }
]