[
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 11,
      "end_pos": 15
    },
    "attributes": {},
    "sentence_context": "All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages.",
    "sentence_index": 0,
    "span_text": "BERT",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "fine-tuning approach",
    "type": "other",
    "char_interval": {
      "start_pos": 55,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages.",
    "sentence_index": 0,
    "span_text": "fine-tuning approach",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 92,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": "All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages.",
    "sentence_index": 0,
    "span_text": "classification layer",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "pre-trained model",
    "type": "other",
    "char_interval": {
      "start_pos": 129,
      "end_pos": 146
    },
    "attributes": {},
    "sentence_context": "All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages.",
    "sentence_index": 0,
    "span_text": "pre-trained model",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 225,
      "end_pos": 247
    },
    "attributes": {},
    "sentence_context": "All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added.",
    "sentence_index": 1,
    "span_text": "feature-based approach",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Transformer encoder architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 387,
      "end_pos": 419
    },
    "attributes": {},
    "sentence_context": "However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.",
    "sentence_index": 2,
    "span_text": "Transformer encoder architecture",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "task-specific model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 445,
      "end_pos": 477
    },
    "attributes": {},
    "sentence_context": "However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.",
    "sentence_index": 2,
    "span_text": "task-specific model architecture",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 588,
      "end_pos": 601
    },
    "attributes": {},
    "sentence_context": "First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNL-2003 Named Entity Recognition (NER) task.",
    "sentence_index": 3,
    "span_text": "training data",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Named Entity Recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 769,
      "end_pos": 793
    },
    "attributes": {},
    "sentence_context": "Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNL-2003 Named Entity Recognition (NER) task. In the input to BERT, we use a case-preserving Word Piece model, and we include the maximal document context provided by the data.",
    "sentence_index": 4,
    "span_text": "Named Entity Recognition",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "CoNL-2003",
    "type": "dataset",
    "char_interval": {
      "start_pos": 759,
      "end_pos": 768
    },
    "attributes": {},
    "sentence_context": "Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNL-2003 Named Entity Recognition (NER) task. In the input to BERT, we use a case-preserving Word Piece model, and we include the maximal document context provided by the data.",
    "sentence_index": 4,
    "span_text": "CoNL-2003",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "NER",
    "type": "method",
    "char_interval": {
      "start_pos": 795,
      "end_pos": 798
    },
    "attributes": {},
    "sentence_context": "Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNL-2003 Named Entity Recognition (NER) task. In the input to BERT, we use a case-preserving Word Piece model, and we include the maximal document context provided by the data.",
    "sentence_index": 4,
    "span_text": "NER",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Word Piece model",
    "type": "other",
    "char_interval": {
      "start_pos": 853,
      "end_pos": 869
    },
    "attributes": {},
    "sentence_context": "In this section, we compare the two approaches by applying BERT to the CoNL-2003 Named Entity Recognition (NER) task. In the input to BERT, we use a case-preserving Word Piece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output.",
    "sentence_index": 5,
    "span_text": "Word Piece model",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 931,
      "end_pos": 935
    },
    "attributes": {},
    "sentence_context": "In this section, we compare the two approaches by applying BERT to the CoNL-2003 Named Entity Recognition (NER) task. In the input to BERT, we use a case-preserving Word Piece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output.",
    "sentence_index": 5,
    "span_text": "data",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "tagging task",
    "type": "task",
    "char_interval": {
      "start_pos": 989,
      "end_pos": 1001
    },
    "attributes": {},
    "sentence_context": "In the input to BERT, we use a case-preserving Word Piece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.",
    "sentence_index": 6,
    "span_text": "tagging task",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "CRF layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1019,
      "end_pos": 1028
    },
    "attributes": {},
    "sentence_context": "In the input to BERT, we use a case-preserving Word Piece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.",
    "sentence_index": 6,
    "span_text": "CRF layer",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "token-level classifier",
    "type": "other",
    "char_interval": {
      "start_pos": 1113,
      "end_pos": 1135
    },
    "attributes": {},
    "sentence_context": "Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set. To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT.",
    "sentence_index": 7,
    "span_text": "token-level classifier",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "NER label set",
    "type": "other",
    "char_interval": {
      "start_pos": 1145,
      "end_pos": 1158
    },
    "attributes": {},
    "sentence_context": "Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set. To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT.",
    "sentence_index": 7,
    "span_text": "NER label set",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1209,
      "end_pos": 1231
    },
    "attributes": {},
    "sentence_context": "We use the representation of the first sub-token as the input to the token-level classifier over the NER label set. To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.",
    "sentence_index": 8,
    "span_text": "feature-based approach",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1324,
      "end_pos": 1328
    },
    "attributes": {},
    "sentence_context": "We use the representation of the first sub-token as the input to the token-level classifier over the NER label set. To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.",
    "sentence_index": 8,
    "span_text": "BERT",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "contextual embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1336,
      "end_pos": 1357
    },
    "attributes": {},
    "sentence_context": "To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7.",
    "sentence_index": 9,
    "span_text": "contextual embeddings",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BiLSTM",
    "type": "method",
    "char_interval": {
      "start_pos": 1428,
      "end_pos": 1434
    },
    "attributes": {},
    "sentence_context": "To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7.",
    "sentence_index": 9,
    "span_text": "BiLSTM",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1446,
      "end_pos": 1466
    },
    "attributes": {},
    "sentence_context": "To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7.",
    "sentence_index": 9,
    "span_text": "classification layer",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1502,
      "end_pos": 1512
    },
    "attributes": {},
    "sentence_context": "Results are presented in Table 7. BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F 1 behind fine-tuning the entire model.",
    "sentence_index": 11,
    "span_text": "BERT LARGE",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "state-of-the-art methods",
    "type": "other",
    "char_interval": {
      "start_pos": 1541,
      "end_pos": 1565
    },
    "attributes": {},
    "sentence_context": "Results are presented in Table 7. BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F 1 behind fine-tuning the entire model.",
    "sentence_index": 11,
    "span_text": "state-of-the-art methods",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "token representations",
    "type": "other",
    "char_interval": {
      "start_pos": 1611,
      "end_pos": 1632
    },
    "attributes": {},
    "sentence_context": "BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F 1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.",
    "sentence_index": 12,
    "span_text": "token representations",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 1684,
      "end_pos": 1695
    },
    "attributes": {},
    "sentence_context": "BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F 1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.",
    "sentence_index": 12,
    "span_text": "Transformer",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1715,
      "end_pos": 1718
    },
    "attributes": {},
    "sentence_context": "BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F 1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.",
    "sentence_index": 12,
    "span_text": "F 1",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 1726,
      "end_pos": 1737
    },
    "attributes": {},
    "sentence_context": "BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F 1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.",
    "sentence_index": 12,
    "span_text": "fine-tuning",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1779,
      "end_pos": 1783
    },
    "attributes": {},
    "sentence_context": "The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F 1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.",
    "sentence_index": 13,
    "span_text": "BERT",
    "section": "Feature-based Approach with BERT"
  }
]