[
  {
    "text": "natural language processing tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 78,
      "end_pos": 111
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "natural language processing tasks",
    "section": "Introduction"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 127,
      "end_pos": 147
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "sentence-level tasks",
    "section": "Introduction"
  },
  {
    "text": "natural language inference",
    "type": "task",
    "char_interval": {
      "start_pos": 156,
      "end_pos": 182
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "natural language inference",
    "section": "Introduction"
  },
  {
    "text": "paraphrasing",
    "type": "task",
    "char_interval": {
      "start_pos": 187,
      "end_pos": 199
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "paraphrasing",
    "section": "Introduction"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 301,
      "end_pos": 318
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "token-level tasks",
    "section": "Introduction"
  },
  {
    "text": "named entity recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 327,
      "end_pos": 351
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "named entity recognition",
    "section": "Introduction"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 356,
      "end_pos": 374
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "question answering",
    "section": "Introduction"
  },
  {
    "text": "pre-trained language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 500,
      "end_pos": 536
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "pre-trained language representations",
    "section": "Introduction"
  },
  {
    "text": "feature-based",
    "type": "other",
    "char_interval": {
      "start_pos": 558,
      "end_pos": 571
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "feature-based",
    "section": "Introduction"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 576,
      "end_pos": 587
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "fine-tuning",
    "section": "Introduction"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 625,
      "end_pos": 629
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "ELMo",
    "section": "Introduction"
  },
  {
    "text": "Generative Pre-trained Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 771,
      "end_pos": 805
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Generative Pre-trained Transformer",
    "section": "Introduction"
  },
  {
    "text": "language models",
    "type": "other",
    "char_interval": {
      "start_pos": 1058,
      "end_pos": 1073
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "language models",
    "section": "Introduction"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1420,
      "end_pos": 1431
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "Open AI GPT",
    "section": "Introduction"
  },
  {
    "text": "left-toright architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 1451,
      "end_pos": 1476
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "left-toright architecture",
    "section": "Introduction"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1538,
      "end_pos": 1552
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "self-attention",
    "section": "Introduction"
  },
  {
    "text": "Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 1567,
      "end_pos": 1578
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "Transformer",
    "section": "Introduction"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1618,
      "end_pos": 1638
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "sentence-level tasks",
    "section": "Introduction"
  },
  {
    "text": "finetuning based approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 1680,
      "end_pos": 1707
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "finetuning based approaches",
    "section": "Introduction"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1711,
      "end_pos": 1728
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "token-level tasks",
    "section": "Introduction"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 1737,
      "end_pos": 1755
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "question answering",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1894,
      "end_pos": 1898
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "Bidirectional Encoder Representations from Transformers",
    "type": "other",
    "char_interval": {
      "start_pos": 1900,
      "end_pos": 1955
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "Bidirectional Encoder Representations from Transformers",
    "section": "Introduction"
  },
  {
    "text": "masked language model",
    "type": "other",
    "char_interval": {
      "start_pos": 2039,
      "end_pos": 2060
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "masked language model",
    "section": "Introduction"
  },
  {
    "text": "Cloze task",
    "type": "task",
    "char_interval": {
      "start_pos": 2108,
      "end_pos": 2118
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "Cloze task",
    "section": "Introduction"
  },
  {
    "text": "MLM objective",
    "type": "other",
    "char_interval": {
      "start_pos": 2354,
      "end_pos": 2367
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "MLM objective",
    "section": "Introduction"
  },
  {
    "text": "left and the right context",
    "type": "other",
    "char_interval": {
      "start_pos": 2407,
      "end_pos": 2433
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "left and the right context",
    "section": "Introduction"
  },
  {
    "text": "deep bidirectional Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 2465,
      "end_pos": 2495
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "deep bidirectional Transformer",
    "section": "Introduction"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2554,
      "end_pos": 2578
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 14,
    "span_text": "next sentence prediction",
    "section": "Introduction"
  },
  {
    "text": "bidirectional pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 2715,
      "end_pos": 2741
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "bidirectional pre-training",
    "section": "Introduction"
  },
  {
    "text": "language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2746,
      "end_pos": 2770
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "language representations",
    "section": "Introduction"
  },
  {
    "text": "language models",
    "type": "method",
    "char_interval": {
      "start_pos": 2800,
      "end_pos": 2815
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "language models",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 2834,
      "end_pos": 2838
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "masked language models",
    "type": "other",
    "char_interval": {
      "start_pos": 2844,
      "end_pos": 2866
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "masked language models",
    "section": "Introduction"
  },
  {
    "text": "deep bidirectional representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2888,
      "end_pos": 2922
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "deep bidirectional representations",
    "section": "Introduction"
  },
  {
    "text": "LMs",
    "type": "other",
    "char_interval": {
      "start_pos": 3059,
      "end_pos": 3062
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "LMs",
    "section": "Introduction"
  },
  {
    "text": "pre-trained representations",
    "type": "other",
    "char_interval": {
      "start_pos": 3077,
      "end_pos": 3104
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "pre-trained representations",
    "section": "Introduction"
  },
  {
    "text": "taskspecific architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 3149,
      "end_pos": 3175
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "taskspecific architectures",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 3177,
      "end_pos": 3181
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "representation model",
    "type": "other",
    "char_interval": {
      "start_pos": 3212,
      "end_pos": 3232
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "representation model",
    "section": "Introduction"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 3296,
      "end_pos": 3310
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "sentence-level",
    "section": "Introduction"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 3315,
      "end_pos": 3332
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "token-level tasks",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 2834,
      "end_pos": 2838
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "task-specific architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 3353,
      "end_pos": 3380
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "task-specific architectures",
    "section": "Introduction"
  }
]