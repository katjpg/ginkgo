[
  {
    "text": "natural language processing tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 78,
      "end_pos": 111
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "natural language processing tasks",
    "section": "Introduction"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 127,
      "end_pos": 147
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "sentence-level tasks",
    "section": "Introduction"
  },
  {
    "text": "natural language inference",
    "type": "task",
    "char_interval": {
      "start_pos": 156,
      "end_pos": 182
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "natural language inference",
    "section": "Introduction"
  },
  {
    "text": "paraphrasing",
    "type": "task",
    "char_interval": {
      "start_pos": 187,
      "end_pos": 199
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "paraphrasing",
    "section": "Introduction"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 301,
      "end_pos": 318
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "token-level tasks",
    "section": "Introduction"
  },
  {
    "text": "named entity recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 327,
      "end_pos": 351
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "named entity recognition",
    "section": "Introduction"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 356,
      "end_pos": 374
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "question answering",
    "section": "Introduction"
  },
  {
    "text": "pre-trained language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 500,
      "end_pos": 536
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "pre-trained language representations",
    "section": "Introduction"
  },
  {
    "text": "feature-based",
    "type": "other",
    "char_interval": {
      "start_pos": 558,
      "end_pos": 571
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "feature-based",
    "section": "Introduction"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 576,
      "end_pos": 587
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "fine-tuning",
    "section": "Introduction"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 625,
      "end_pos": 629
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "ELMo",
    "section": "Introduction"
  },
  {
    "text": "Generative Pre-trained Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 771,
      "end_pos": 805
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Generative Pre-trained Transformer",
    "section": "Introduction"
  },
  {
    "text": "language models",
    "type": "other",
    "char_interval": {
      "start_pos": 1058,
      "end_pos": 1073
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "language models",
    "section": "Introduction"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1420,
      "end_pos": 1431
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "Open AI GPT",
    "section": "Introduction"
  },
  {
    "text": "left-toright architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 1451,
      "end_pos": 1476
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "left-toright architecture",
    "section": "Introduction"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1538,
      "end_pos": 1552
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "self-attention",
    "section": "Introduction"
  },
  {
    "text": "Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 1567,
      "end_pos": 1578
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "Transformer",
    "section": "Introduction"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1618,
      "end_pos": 1638
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "sentence-level tasks",
    "section": "Introduction"
  },
  {
    "text": "finetuning based approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 1680,
      "end_pos": 1707
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "finetuning based approaches",
    "section": "Introduction"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1711,
      "end_pos": 1728
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "token-level tasks",
    "section": "Introduction"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 1737,
      "end_pos": 1755
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "question answering",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1894,
      "end_pos": 1898
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "Bidirectional Encoder Representations from Transformers",
    "type": "other",
    "char_interval": {
      "start_pos": 1900,
      "end_pos": 1955
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "Bidirectional Encoder Representations from Transformers",
    "section": "Introduction"
  },
  {
    "text": "masked language model",
    "type": "other",
    "char_interval": {
      "start_pos": 2039,
      "end_pos": 2060
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "masked language model",
    "section": "Introduction"
  },
  {
    "text": "Cloze task",
    "type": "task",
    "char_interval": {
      "start_pos": 2108,
      "end_pos": 2118
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "Cloze task",
    "section": "Introduction"
  },
  {
    "text": "MLM objective",
    "type": "other",
    "char_interval": {
      "start_pos": 2354,
      "end_pos": 2367
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "MLM objective",
    "section": "Introduction"
  },
  {
    "text": "left and the right context",
    "type": "other",
    "char_interval": {
      "start_pos": 2407,
      "end_pos": 2433
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "left and the right context",
    "section": "Introduction"
  },
  {
    "text": "deep bidirectional Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 2465,
      "end_pos": 2495
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "deep bidirectional Transformer",
    "section": "Introduction"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2554,
      "end_pos": 2578
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 14,
    "span_text": "next sentence prediction",
    "section": "Introduction"
  },
  {
    "text": "bidirectional pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 2715,
      "end_pos": 2741
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "bidirectional pre-training",
    "section": "Introduction"
  },
  {
    "text": "language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2746,
      "end_pos": 2770
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "language representations",
    "section": "Introduction"
  },
  {
    "text": "language models",
    "type": "method",
    "char_interval": {
      "start_pos": 2800,
      "end_pos": 2815
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "language models",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 2834,
      "end_pos": 2838
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "masked language models",
    "type": "other",
    "char_interval": {
      "start_pos": 2844,
      "end_pos": 2866
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "masked language models",
    "section": "Introduction"
  },
  {
    "text": "deep bidirectional representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2888,
      "end_pos": 2922
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "deep bidirectional representations",
    "section": "Introduction"
  },
  {
    "text": "LMs",
    "type": "other",
    "char_interval": {
      "start_pos": 3059,
      "end_pos": 3062
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "LMs",
    "section": "Introduction"
  },
  {
    "text": "pre-trained representations",
    "type": "other",
    "char_interval": {
      "start_pos": 3077,
      "end_pos": 3104
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "pre-trained representations",
    "section": "Introduction"
  },
  {
    "text": "taskspecific architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 3149,
      "end_pos": 3175
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "taskspecific architectures",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 3177,
      "end_pos": 3181
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "representation model",
    "type": "other",
    "char_interval": {
      "start_pos": 3212,
      "end_pos": 3232
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "representation model",
    "section": "Introduction"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 3296,
      "end_pos": 3310
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "sentence-level",
    "section": "Introduction"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 3315,
      "end_pos": 3332
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "token-level tasks",
    "section": "Introduction"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 2834,
      "end_pos": 2838
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "BERT",
    "section": "Introduction"
  },
  {
    "text": "task-specific architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 3353,
      "end_pos": 3380
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for eleven NLP tasks.",
    "sentence_index": 18,
    "span_text": "task-specific architectures",
    "section": "Introduction"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 27,
      "end_pos": 39
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "has been shown",
    "section": "Related Work"
  },
  {
    "text": "language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 48,
      "end_pos": 72
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "effective for improving",
    "section": "Related Work"
  },
  {
    "text": "representations",
    "type": "other",
    "char_interval": {
      "start_pos": 27,
      "end_pos": 42
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "has been shown",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 154,
      "end_pos": 169
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "as natural language",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 253,
      "end_pos": 263
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "sentences by analyzing",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embedding vectors",
    "type": "other",
    "char_interval": {
      "start_pos": 298,
      "end_pos": 320
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "as token-level tasks such",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "language modeling",
    "type": "other",
    "char_interval": {
      "start_pos": 336,
      "end_pos": 353
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "entity recognition and",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "sentence embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 548,
      "end_pos": 567
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "downstream tasks: feature-based",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "paragraph embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 571,
      "end_pos": 591
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 0,
    "span_text": "and fine-tuning. The",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "sentence representations",
    "type": "other",
    "char_interval": {
      "start_pos": 602,
      "end_pos": 626
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "based approach, such as ELMo",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "denoising autoencoder",
    "type": "other",
    "char_interval": {
      "start_pos": 793,
      "end_pos": 814
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Transformer (Open AI",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 835,
      "end_pos": 839
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "minimal",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embedding research",
    "type": "other",
    "char_interval": {
      "start_pos": 885,
      "end_pos": 908
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "the downstream tasks by",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "context-sensitive features",
    "type": "other",
    "char_interval": {
      "start_pos": 951,
      "end_pos": 977
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 0,
    "span_text": "parameters. The two approaches share",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 1019,
      "end_pos": 1033
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "training, where",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "contextual representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1039,
      "end_pos": 1064
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "use unidirectional language",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "word embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1184,
      "end_pos": 1199
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "trained representations",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 1321,
      "end_pos": 1339
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "and this limits the",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "sentiment analysis",
    "type": "task",
    "char_interval": {
      "start_pos": 1341,
      "end_pos": 1359
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "choice of architectures",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "named entity recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 1365,
      "end_pos": 1389
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "that can be used during",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "predict a single word",
    "type": "task",
    "char_interval": {
      "start_pos": 1455,
      "end_pos": 1476
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "-toright architecture",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "LSTMs",
    "type": "method",
    "char_interval": {
      "start_pos": 1516,
      "end_pos": 1521
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "previous",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 1534,
      "end_pos": 1538
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "the",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "feature-based",
    "type": "other",
    "char_interval": {
      "start_pos": 1555,
      "end_pos": 1568
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "layers of the Transformer",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "deeply bidirectional",
    "type": "other",
    "char_interval": {
      "start_pos": 1577,
      "end_pos": 1597
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 0,
    "span_text": "Transformer. Such restrictions",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "cloze task",
    "type": "other",
    "char_interval": {
      "start_pos": 1615,
      "end_pos": 1625
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "for sentence",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "text generation",
    "type": "task",
    "char_interval": {
      "start_pos": 1667,
      "end_pos": 1682
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "when applying finetuning",
    "section": "Unsupervised Feature-based Approaches"
  },
  {
    "text": "feature-based approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 12,
      "end_pos": 36
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "model pre-training has been",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "word embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 89,
      "end_pos": 103
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "language processing",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "sentence encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 151,
      "end_pos": 159
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "such as natural",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "document encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 163,
      "end_pos": 180
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "language inference",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "contextual token representations",
    "type": "other",
    "char_interval": {
      "start_pos": 195,
      "end_pos": 227
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "paraphrasing, which aim to predict the relationships",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 572,
      "end_pos": 586
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "and fine-tuning",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 448,
      "end_pos": 459
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "level. There",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 151,
      "end_pos": 317
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "GLUE benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 542,
      "end_pos": 556
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "downstream tasks",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 587,
      "end_pos": 591
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 0,
    "span_text": ". The",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 572,
      "end_pos": 586
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "and fine-tuning",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "auto-encoder objectives",
    "type": "other",
    "char_interval": {
      "start_pos": 606,
      "end_pos": 629
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "approach, such as ELMo",
    "section": "Unsupervised Fine-tuning Approaches"
  },
  {
    "text": "natural language inference",
    "type": "task",
    "char_interval": {
      "start_pos": 103,
      "end_pos": 129
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "span_text": "processing tasks. These include sentence",
    "section": "Transfer Learning from Supervised Data"
  },
  {
    "text": "machine translation",
    "type": "task",
    "char_interval": {
      "start_pos": 134,
      "end_pos": 153
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "sentence-level tasks such",
    "section": "Transfer Learning from Supervised Data"
  },
  {
    "text": "transfer learning",
    "type": "other",
    "char_interval": {
      "start_pos": 220,
      "end_pos": 237
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "predict the relationships",
    "section": "Transfer Learning from Supervised Data"
  },
  {
    "text": "Ima-ge Net",
    "type": "dataset",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 345
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "entity recognition",
    "section": "Transfer Learning from Supervised Data"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 13,
      "end_pos": 17
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "model pre",
    "section": "Bert"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 105,
      "end_pos": 117
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "span_text": "tasks. These",
    "section": "Bert"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 122,
      "end_pos": 133
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "include sentence",
    "section": "Bert"
  },
  {
    "text": "unlabeled data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 180,
      "end_pos": 194
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "inference and paraphrasing",
    "section": "Bert"
  },
  {
    "text": "pre-training tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 210,
      "end_pos": 228
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "to predict the relationships",
    "section": "Bert"
  },
  {
    "text": "labeled data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 362,
      "end_pos": 374
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "question answering",
    "section": "Bert"
  },
  {
    "text": "downstream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 384,
      "end_pos": 400
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "models are required",
    "section": "Bert"
  },
  {
    "text": "question-answering",
    "type": "task",
    "char_interval": {
      "start_pos": 530,
      "end_pos": 548
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "representations to downstream",
    "section": "Bert"
  },
  {
    "text": "BERT model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 644,
      "end_pos": 648
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "specific",
    "section": "Bert"
  },
  {
    "text": "Transformer encoder",
    "type": "method",
    "char_interval": {
      "start_pos": 882,
      "end_pos": 901
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "on the downstream tasks",
    "section": "Bert"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1415,
      "end_pos": 1429
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": ", in Open AI GPT",
    "section": "Bert"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1490,
      "end_pos": 1499
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "token can",
    "section": "Bert"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1548,
      "end_pos": 1558
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "attention layers",
    "section": "Bert"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1612,
      "end_pos": 1623
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "optimal for sentence",
    "section": "Bert"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1674,
      "end_pos": 1678
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "applying",
    "section": "Bert"
  },
  {
    "text": "bidirectional self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1696,
      "end_pos": 1724
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "approaches to token-level tasks",
    "section": "Bert"
  },
  {
    "text": "GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1736,
      "end_pos": 1739
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "question",
    "section": "Bert"
  },
  {
    "text": "constrained self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1757,
      "end_pos": 1783
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "where it is crucial to incorporate",
    "section": "Bert"
  },
  {
    "text": "down-stream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1906,
      "end_pos": 1923
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "Bidirectional Encoder Representations",
    "section": "Bert"
  },
  {
    "text": "sentence pair",
    "type": "other",
    "char_interval": {
      "start_pos": 1999,
      "end_pos": 2007
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "unidirectionality",
    "section": "Bert"
  },
  {
    "text": "Question",
    "type": "other",
    "char_interval": {
      "start_pos": 2039,
      "end_pos": 2047
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "masked language",
    "section": "Bert"
  },
  {
    "text": "Answer",
    "type": "other",
    "char_interval": {
      "start_pos": 2049,
      "end_pos": 2055
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "language",
    "section": "Bert"
  },
  {
    "text": "span of contiguous text",
    "type": "other",
    "char_interval": {
      "start_pos": 2136,
      "end_pos": 2159
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 12,
    "span_text": "language model randomly masks",
    "section": "Bert"
  },
  {
    "text": "sequence",
    "type": "other",
    "char_interval": {
      "start_pos": 2207,
      "end_pos": 2215
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 12,
    "span_text": "objective is",
    "section": "Bert"
  },
  {
    "text": "Word Piece embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 2333,
      "end_pos": 2354
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "model pre-training, the",
    "section": "Bert"
  },
  {
    "text": "classification token",
    "type": "other",
    "char_interval": {
      "start_pos": 2441,
      "end_pos": 2461
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "allows us to pretrain",
    "section": "Bert"
  },
  {
    "text": "classification tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 2574,
      "end_pos": 2594
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 14,
    "span_text": "prediction\" task that jointly",
    "section": "Bert"
  },
  {
    "text": "Sentence pairs",
    "type": "other",
    "char_interval": {
      "start_pos": 2596,
      "end_pos": 2610
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 14,
    "span_text": "jointly pretrains text",
    "section": "Bert"
  },
  {
    "text": "special token",
    "type": "other",
    "char_interval": {
      "start_pos": 2730,
      "end_pos": 2743
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "pre-training for",
    "section": "Bert"
  },
  {
    "text": "learned embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 2769,
      "end_pos": 2786
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "representations.., which uses unidirectional",
    "section": "Bert"
  },
  {
    "text": "sentence A",
    "type": "other",
    "char_interval": {
      "start_pos": 2835,
      "end_pos": 2845
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "BERT uses masked",
    "section": "Bert"
  },
  {
    "text": "sentence B",
    "type": "other",
    "char_interval": {
      "start_pos": 2849,
      "end_pos": 2859
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "masked language",
    "section": "Bert"
  },
  {
    "text": "input embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 2893,
      "end_pos": 2908
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "bidirectional representations",
    "section": "Bert"
  },
  {
    "text": "hidden vector",
    "type": "other",
    "char_interval": {
      "start_pos": 2925,
      "end_pos": 2938
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "This is also in",
    "section": "Bert"
  },
  {
    "text": "hidden vector",
    "type": "other",
    "char_interval": {
      "start_pos": 2992,
      "end_pos": 3005
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "concatenation of",
    "section": "Bert"
  },
  {
    "text": "input representation",
    "type": "other",
    "char_interval": {
      "start_pos": 3057,
      "end_pos": 3077
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 0,
    "span_text": "left LMs. We show that",
    "section": "Bert"
  },
  {
    "text": "token",
    "type": "other",
    "char_interval": {
      "start_pos": 3122,
      "end_pos": 3127
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "for many",
    "section": "Bert"
  },
  {
    "text": "segment",
    "type": "other",
    "char_interval": {
      "start_pos": 3129,
      "end_pos": 3136
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "heavily",
    "section": "Bert"
  },
  {
    "text": "position embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 3142,
      "end_pos": 3161
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "engineered taskspecific",
    "section": "Bert"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 108,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "tasks.",
    "section": "Pre-training BERT"
  },
  {
    "text": "Masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 260,
      "end_pos": 269
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "analyzing",
    "section": "Pre-training BERT"
  },
  {
    "text": "Transformer encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 780,
      "end_pos": 799
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Generative Pre-trained Transformer",
    "section": "Pre-training BERT"
  },
  {
    "text": "Transformer decoder",
    "type": "other",
    "char_interval": {
      "start_pos": 858,
      "end_pos": 877
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "parameters, and is trained",
    "section": "Pre-training BERT"
  },
  {
    "text": "text generation",
    "type": "task",
    "char_interval": {
      "start_pos": 904,
      "end_pos": 919
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "by simply fine",
    "section": "Pre-training BERT"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 1109,
      "end_pos": 1118
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "span_text": "representations. We",
    "section": "Pre-training BERT"
  },
  {
    "text": "Cloze task",
    "type": "task",
    "char_interval": {
      "start_pos": 1165,
      "end_pos": 1175
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "power of the",
    "section": "Pre-training BERT"
  },
  {
    "text": "denoising auto-encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 1447,
      "end_pos": 1470
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "use a left-toright architecture",
    "section": "Pre-training BERT"
  },
  {
    "text": "pre-trained model",
    "type": "other",
    "char_interval": {
      "start_pos": 1600,
      "end_pos": 1617
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "are sub-optimal for",
    "section": "Pre-training BERT"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1854,
      "end_pos": 1867
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "fine-tuning based",
    "section": "Pre-training BERT"
  },
  {
    "text": "Next Sentence Prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2261,
      "end_pos": 2285
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 12,
    "span_text": "masked word based only",
    "section": "Pre-training BERT"
  },
  {
    "text": "Question Answering",
    "type": "task",
    "char_interval": {
      "start_pos": 2332,
      "end_pos": 2350
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "model pre-training,",
    "section": "Pre-training BERT"
  },
  {
    "text": "Natural Language Inference",
    "type": "task",
    "char_interval": {
      "start_pos": 2360,
      "end_pos": 2386
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "objective enables the representation",
    "section": "Pre-training BERT"
  },
  {
    "text": "monolingual corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2680,
      "end_pos": 2698
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": ": We demonstrate the",
    "section": "Pre-training BERT"
  },
  {
    "text": "pretraining example",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2759,
      "end_pos": 2778
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "representations.., which",
    "section": "Pre-training BERT"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2983,
      "end_pos": 3007
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "shallow concatenation of independently",
    "section": "Pre-training BERT"
  },
  {
    "text": "corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2915,
      "end_pos": 2921
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "representations",
    "section": "Pre-training BERT"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2983,
      "end_pos": 3007
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "shallow concatenation of independently",
    "section": "Pre-training BERT"
  },
  {
    "text": "QA",
    "type": "task",
    "char_interval": {
      "start_pos": 3132,
      "end_pos": 3134
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "heavily",
    "section": "Pre-training BERT"
  },
  {
    "text": "NLI",
    "type": "task",
    "char_interval": {
      "start_pos": 3139,
      "end_pos": 3142
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "engineered",
    "section": "Pre-training BERT"
  },
  {
    "text": "Position Embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 3144,
      "end_pos": 3163
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "engineered taskspecific architectures",
    "section": "Pre-training BERT"
  },
  {
    "text": "NSP task",
    "type": "task",
    "char_interval": {
      "start_pos": 3168,
      "end_pos": 3176
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "architectures.",
    "section": "Pre-training BERT"
  },
  {
    "text": "language model pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 70,
      "end_pos": 97
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "improving many natural language processing",
    "section": "Pre-training data"
  },
  {
    "text": "Books Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 138,
      "end_pos": 150
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "level tasks such",
    "section": "Pre-training data"
  },
  {
    "text": "English Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 168,
      "end_pos": 185
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "language inference and",
    "section": "Pre-training data"
  },
  {
    "text": "text passages",
    "type": "other",
    "char_interval": {
      "start_pos": 236,
      "end_pos": 249
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "relationships between sentences",
    "section": "Pre-training data"
  },
  {
    "text": "document-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 313,
      "end_pos": 334
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "tasks such as named entity",
    "section": "Pre-training data"
  },
  {
    "text": "shuffled sentence-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 349,
      "end_pos": 379
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "recognition and question answering, where",
    "section": "Pre-training data"
  },
  {
    "text": "Billion Word Benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 392,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "required to produce fine",
    "section": "Pre-training data"
  },
  {
    "text": "Fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 0,
      "end_pos": 11
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "Language model",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "selfattention mechanism",
    "type": "other",
    "char_interval": {
      "start_pos": 41,
      "end_pos": 64
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "shown to be effective for improving",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 72,
      "end_pos": 83
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "many natural",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 91,
      "end_pos": 95
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "language",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "downstream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 110,
      "end_pos": 120
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "span_text": "tasks. These include",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "natural language",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "entity recognition and question",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 400,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "required to produce fine",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "entity recognition and question",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 494,
      "end_pos": 508
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "applying pre-trained",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "natural language",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 530,
      "end_pos": 559
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "representations to downstream tasks: feature",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "taskspecific inputs and outputs",
    "type": "task",
    "char_interval": {
      "start_pos": 620,
      "end_pos": 651
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "such as ELMo, uses task-specific architectures",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 657,
      "end_pos": 661
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "architectures",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "sequence tagging",
    "type": "task",
    "char_interval": {
      "start_pos": 970,
      "end_pos": 986
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "approaches share the same",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 891,
      "end_pos": 909
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "downstream tasks by simply",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "classification",
    "type": "task",
    "char_interval": {
      "start_pos": 952,
      "end_pos": 966
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 0,
    "span_text": ". The two approaches",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "entailment",
    "type": "task",
    "char_interval": {
      "start_pos": 849,
      "end_pos": 859
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "specific parameters",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "sentiment analysis",
    "type": "task",
    "char_interval": {
      "start_pos": 1227,
      "end_pos": 1245
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "fine-tuning approaches",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 751,
      "end_pos": 763
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "approach, such",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 1273,
      "end_pos": 1284
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "that standard",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "pre-trained model",
    "type": "method",
    "char_interval": {
      "start_pos": 1457,
      "end_pos": 1474
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "toright architecture",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "task-specific details",
    "type": "task",
    "char_interval": {
      "start_pos": 1492,
      "end_pos": 1513
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "token can only attend to",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Section 4",
    "type": "other",
    "char_interval": {
      "start_pos": 1550,
      "end_pos": 1559
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "attention layers",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Appendix A.5",
    "type": "other",
    "char_interval": {
      "start_pos": 1590,
      "end_pos": 1602
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "restrictions are",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 28,
      "end_pos": 32
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "has",
    "section": "Experiments"
  },
  {
    "text": "NLP tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 59,
      "end_pos": 68
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "for improving",
    "section": "Experiments"
  },
  {
    "text": "GLUE benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 47,
      "end_pos": 51
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "be effective",
    "section": "Glue"
  },
  {
    "text": "natural language understanding tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 90,
      "end_pos": 126
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "span_text": "language processing tasks. These include",
    "section": "Glue"
  },
  {
    "text": "GLUE datasets",
    "type": "dataset",
    "char_interval": {
      "start_pos": 153,
      "end_pos": 166
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "as natural language",
    "section": "Glue"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 870,
      "end_pos": 874
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "is trained",
    "section": "Glue"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 496,
      "end_pos": 516
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "applying pre-trained language",
    "section": "Glue"
  },
  {
    "text": "classification loss",
    "type": "other",
    "char_interval": {
      "start_pos": 591,
      "end_pos": 610
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "The feature-based approach",
    "section": "Glue"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 727,
      "end_pos": 737
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "span_text": "features. The",
    "section": "Glue"
  },
  {
    "text": "Dev set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 843,
      "end_pos": 850
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "task-specific",
    "section": "Glue"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 870,
      "end_pos": 880
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "is trained",
    "section": "Glue"
  },
  {
    "text": "fine-tuning data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1109,
      "end_pos": 1125
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "span_text": "representations. We argue",
    "section": "Glue"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1214,
      "end_pos": 1223
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "especially for the",
    "section": "Glue"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 1343,
      "end_pos": 1351
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "choice of",
    "section": "Glue"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1407,
      "end_pos": 1416
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "example,",
    "section": "Glue"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1421,
      "end_pos": 1432
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "Open AI GPT,",
    "section": "Glue"
  },
  {
    "text": "model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 1466,
      "end_pos": 1484
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "architecture, where",
    "section": "Glue"
  },
  {
    "text": "attention masking",
    "type": "other",
    "char_interval": {
      "start_pos": 1500,
      "end_pos": 1517
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "only attend to previous",
    "section": "Glue"
  },
  {
    "text": "GLUE task",
    "type": "task",
    "char_interval": {
      "start_pos": 1560,
      "end_pos": 1569
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "of the Transformer",
    "section": "Glue"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1571,
      "end_pos": 1575
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "Transformer",
    "section": "Glue"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1577,
      "end_pos": 1581
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 0,
    "span_text": "Transformer. Such",
    "section": "Glue"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 1606,
      "end_pos": 1614
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "optimal",
    "section": "Glue"
  },
  {
    "text": "GLUE leaderboard",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1644,
      "end_pos": 1660
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "could be very harmful",
    "section": "Glue"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1665,
      "end_pos": 1675
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "when applying",
    "section": "Glue"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1713,
      "end_pos": 1724
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "token-level tasks",
    "section": "Glue"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1884,
      "end_pos": 1897
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "proposing BERT",
    "section": "Glue"
  },
  {
    "text": "model size",
    "type": "other",
    "char_interval": {
      "start_pos": 1913,
      "end_pos": 1923
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "Encoder Representations",
    "section": "Glue"
  },
  {
    "text": "Stanford Question Answering Dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 39
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "Language model pre-training has been shown",
    "section": "SQuAD v1.1"
  },
  {
    "text": "SQuAD v 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 41,
      "end_pos": 52
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "shown to be effective",
    "section": "SQuAD v1.1"
  },
  {
    "text": "question/answer pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 91,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "language processing tasks.",
    "section": "SQuAD v1.1"
  },
  {
    "text": "GLUE data set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 156,
      "end_pos": 169
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "natural language",
    "section": "SQuAD v1.1"
  },
  {
    "text": "Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 316,
      "end_pos": 325
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "tasks such as",
    "section": "SQuAD v1.1"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 438,
      "end_pos": 456
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "the token level. There",
    "section": "SQuAD v1.1"
  },
  {
    "text": "A embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 564,
      "end_pos": 575
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "feature-based and",
    "section": "SQuAD v1.1"
  },
  {
    "text": "B embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 602,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "based approach",
    "section": "SQuAD v1.1"
  },
  {
    "text": "start vector",
    "type": "other",
    "char_interval": {
      "start_pos": 635,
      "end_pos": 647
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "task-specific",
    "section": "SQuAD v1.1"
  },
  {
    "text": "end vector",
    "type": "other",
    "char_interval": {
      "start_pos": 663,
      "end_pos": 673
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "that include",
    "section": "SQuAD v1.1"
  },
  {
    "text": "softmax",
    "type": "other",
    "char_interval": {
      "start_pos": 824,
      "end_pos": 831
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "introduces",
    "section": "SQuAD v1.1"
  },
  {
    "text": "start of the answer span",
    "type": "other",
    "char_interval": {
      "start_pos": 738,
      "end_pos": 762
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "fine-tuning approach, such",
    "section": "SQuAD v1.1"
  },
  {
    "text": "end of the answer span",
    "type": "other",
    "char_interval": {
      "start_pos": 933,
      "end_pos": 955
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 0,
    "span_text": "pretrained parameters. The",
    "section": "SQuAD v1.1"
  },
  {
    "text": "candidate span",
    "type": "other",
    "char_interval": {
      "start_pos": 972,
      "end_pos": 986
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "share the same",
    "section": "SQuAD v1.1"
  },
  {
    "text": "training objective",
    "type": "other",
    "char_interval": {
      "start_pos": 1114,
      "end_pos": 1132
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "span_text": "representations. We argue that current",
    "section": "SQuAD v1.1"
  },
  {
    "text": "log-likelihoods",
    "type": "other",
    "char_interval": {
      "start_pos": 1151,
      "end_pos": 1166
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "restrict the power",
    "section": "SQuAD v1.1"
  },
  {
    "text": "correct start and end positions",
    "type": "other",
    "char_interval": {
      "start_pos": 1174,
      "end_pos": 1205
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "the pre-trained representations,",
    "section": "SQuAD v1.1"
  },
  {
    "text": "SQuAD",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1396,
      "end_pos": 1401
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "training",
    "section": "SQuAD v1.1"
  },
  {
    "text": "public data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1502,
      "end_pos": 1513
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "only attend to",
    "section": "SQuAD v1.1"
  },
  {
    "text": "Trivia QA",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1623,
      "end_pos": 1632
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "sentence-level",
    "section": "SQuAD v1.1"
  },
  {
    "text": "SQuAD",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1654,
      "end_pos": 1659
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "very harmful",
    "section": "SQuAD v1.1"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1806,
      "end_pos": 1810
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "both",
    "section": "SQuAD v1.1"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1735,
      "end_pos": 1738
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "as question",
    "section": "SQuAD v1.1"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1762,
      "end_pos": 1765
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "it",
    "section": "SQuAD v1.1"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1865,
      "end_pos": 1868
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "based",
    "section": "SQuAD v1.1"
  },
  {
    "text": "tuning data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1875,
      "end_pos": 1886
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "approaches by proposing",
    "section": "SQuAD v1.1"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1908,
      "end_pos": 1911
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "Bidirectional",
    "section": "SQuAD v1.1"
  },
  {
    "text": "SQuAD v 2.0",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1978,
      "end_pos": 1989
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "previously mentioned",
    "section": "SQuAD v1.1"
  },
  {
    "text": "SQuAD 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2021,
      "end_pos": 2030
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "constraint by",
    "section": "SQuAD v1.1"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1806,
      "end_pos": 1810
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "both",
    "section": "SQuAD v1.1"
  },
  {
    "text": "SQuAD v 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2211,
      "end_pos": 2222
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 12,
    "span_text": "objective is to predict",
    "section": "SQuAD v1.1"
  },
  {
    "text": "Trivia QA",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2604,
      "end_pos": 2613
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 14,
    "span_text": "pretrains text-",
    "section": "SQuAD v1.1"
  },
  {
    "text": "Trivia QA-Wiki",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2655,
      "end_pos": 2669
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "our paper are",
    "section": "SQuAD v1.1"
  },
  {
    "text": "F1",
    "type": "metric",
    "char_interval": {
      "start_pos": 2918,
      "end_pos": 2921
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "representations",
    "section": "SQuAD v1.1"
  },
  {
    "text": "dev set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2898,
      "end_pos": 2905
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "bidirectional",
    "section": "SQuAD v1.1"
  },
  {
    "text": "Trivia QA data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2938,
      "end_pos": 2952
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "in contrast to",
    "section": "SQuAD v1.1"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 173,
      "end_pos": 176
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "inference",
    "section": "System"
  },
  {
    "text": "SWAG dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 45,
      "end_pos": 49
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "be",
    "section": "Swag"
  },
  {
    "text": "grounded commonsense inference",
    "type": "task",
    "char_interval": {
      "start_pos": 121,
      "end_pos": 151
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "include sentence-level tasks such",
    "section": "Swag"
  },
  {
    "text": "sentence-pair completion",
    "type": "task",
    "char_interval": {
      "start_pos": 73,
      "end_pos": 97
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "many natural language processing",
    "section": "Swag"
  },
  {
    "text": "SWAG dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 269,
      "end_pos": 281
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "analyzing them holistically",
    "section": "Swag"
  },
  {
    "text": "task-specific parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 438,
      "end_pos": 462
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "the token level. There are",
    "section": "Swag"
  },
  {
    "text": "softmax layer",
    "type": "other",
    "char_interval": {
      "start_pos": 601,
      "end_pos": 614
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "based approach",
    "section": "Swag"
  },
  {
    "text": "ESIM+ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 784,
      "end_pos": 793
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Pre-trained",
    "section": "Swag"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 739,
      "end_pos": 749
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "fine-tuning",
    "section": "Swag"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 815,
      "end_pos": 826
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "GPT), introduces",
    "section": "Swag"
  },
  {
    "text": "ablation experiments",
    "type": "other",
    "char_interval": {
      "start_pos": 28,
      "end_pos": 48
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "has been shown to be",
    "section": "Ablation Studies"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 76,
      "end_pos": 80
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "many natural",
    "section": "Ablation Studies"
  },
  {
    "text": "ablation studies",
    "type": "other",
    "char_interval": {
      "start_pos": 149,
      "end_pos": 165
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "such as natural language",
    "section": "Ablation Studies"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 62,
      "end_pos": 66
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "improving",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "pretraining objectives",
    "type": "other",
    "char_interval": {
      "start_pos": 85,
      "end_pos": 107
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "language processing tasks",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "pretraining data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 131,
      "end_pos": 147
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "sentence-level tasks",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "fine-tuning scheme",
    "type": "other",
    "char_interval": {
      "start_pos": 149,
      "end_pos": 167
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "such as natural language",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 173,
      "end_pos": 188
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "inference and paraphrasing",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 192,
      "end_pos": 201
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "paraphrasing,",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 261,
      "end_pos": 270
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "analyzing",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 295,
      "end_pos": 319
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "well as token-level tasks",
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "Left-to-Right (LTR) LM",
    "type": "method",
    "char_interval": {
      "start_pos": 60,
      "end_pos": 82
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "for improving many natural",
    "section": "LTR & No NSP:"
  },
  {
    "text": "MLM",
    "type": "other",
    "char_interval": {
      "start_pos": 99,
      "end_pos": 102
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "processing",
    "section": "LTR & No NSP:"
  },
  {
    "text": "left-only constraint",
    "type": "other",
    "char_interval": {
      "start_pos": 108,
      "end_pos": 128
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "span_text": "tasks. These include sentence",
    "section": "LTR & No NSP:"
  },
  {
    "text": "pre-train/fine-tune mismatch",
    "type": "other",
    "char_interval": {
      "start_pos": 195,
      "end_pos": 223
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "paraphrasing, which aim to predict the",
    "section": "LTR & No NSP:"
  },
  {
    "text": "NSP task",
    "type": "task",
    "char_interval": {
      "start_pos": 315,
      "end_pos": 323
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "tasks such",
    "section": "LTR & No NSP:"
  },
  {
    "text": "QNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 588,
      "end_pos": 592
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "The",
    "section": "LTR & No NSP:"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 594,
      "end_pos": 598
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "feature",
    "section": "LTR & No NSP:"
  },
  {
    "text": "SQuAD 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 604,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "based approach",
    "section": "LTR & No NSP:"
  },
  {
    "text": "bidirectional representations",
    "type": "other",
    "char_interval": {
      "start_pos": 656,
      "end_pos": 685
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "architectures that include the pre-",
    "section": "LTR & No NSP:"
  },
  {
    "text": "LTR model",
    "type": "method",
    "char_interval": {
      "start_pos": 731,
      "end_pos": 740
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "span_text": ". The fine",
    "section": "LTR & No NSP:"
  },
  {
    "text": "MLM model",
    "type": "method",
    "char_interval": {
      "start_pos": 765,
      "end_pos": 774
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "as the Generative",
    "section": "LTR & No NSP:"
  },
  {
    "text": "MRPC",
    "type": "dataset",
    "char_interval": {
      "start_pos": 809,
      "end_pos": 813
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Open AI",
    "section": "LTR & No NSP:"
  },
  {
    "text": "SQuAD",
    "type": "dataset",
    "char_interval": {
      "start_pos": 818,
      "end_pos": 823
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "), introduces",
    "section": "LTR & No NSP:"
  },
  {
    "text": "token predictions",
    "type": "task",
    "char_interval": {
      "start_pos": 899,
      "end_pos": 916
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "tasks by simply fine",
    "section": "LTR & No NSP:"
  },
  {
    "text": "rightside context",
    "type": "other",
    "char_interval": {
      "start_pos": 962,
      "end_pos": 979
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "approaches share",
    "section": "LTR & No NSP:"
  },
  {
    "text": "BiLSTM",
    "type": "method",
    "char_interval": {
      "start_pos": 1084,
      "end_pos": 1090
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "general",
    "section": "LTR & No NSP:"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1272,
      "end_pos": 1282
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "that standard",
    "section": "LTR & No NSP:"
  },
  {
    "text": "LTR",
    "type": "method",
    "char_interval": {
      "start_pos": 1346,
      "end_pos": 1349
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "choice",
    "section": "LTR & No NSP:"
  },
  {
    "text": "RTL models",
    "type": "method",
    "char_interval": {
      "start_pos": 1354,
      "end_pos": 1364
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "architectures",
    "section": "LTR & No NSP:"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 1433,
      "end_pos": 1437
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "the",
    "section": "LTR & No NSP:"
  },
  {
    "text": "bidirectional model",
    "type": "method",
    "char_interval": {
      "start_pos": 1496,
      "end_pos": 1515
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "can only attend to",
    "section": "LTR & No NSP:"
  },
  {
    "text": "QA",
    "type": "task",
    "char_interval": {
      "start_pos": 1558,
      "end_pos": 1560
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "layers",
    "section": "LTR & No NSP:"
  },
  {
    "text": "RTL model",
    "type": "method",
    "char_interval": {
      "start_pos": 1572,
      "end_pos": 1581
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 0,
    "span_text": "Transformer. Such",
    "section": "LTR & No NSP:"
  },
  {
    "text": "deep bidirectional model",
    "type": "method",
    "char_interval": {
      "start_pos": 1686,
      "end_pos": 1710
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "finetuning based approaches to",
    "section": "LTR & No NSP:"
  },
  {
    "text": "fine-tuning task",
    "type": "task",
    "char_interval": {
      "start_pos": 56,
      "end_pos": 72
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "effective for improving",
    "section": "Effect of Model Size"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 73,
      "end_pos": 81
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "many natural",
    "section": "Effect of Model Size"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 106,
      "end_pos": 110
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "tasks",
    "section": "Effect of Model Size"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 219,
      "end_pos": 234
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "predict the relationships",
    "section": "Effect of Model Size"
  },
  {
    "text": "training procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 239,
      "end_pos": 257
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "between sentences",
    "section": "Effect of Model Size"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 303,
      "end_pos": 313
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "token-level",
    "section": "Effect of Model Size"
  },
  {
    "text": "Dev Set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 373,
      "end_pos": 380
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "answering, where",
    "section": "Effect of Model Size"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 381,
      "end_pos": 389
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "models",
    "section": "Effect of Model Size"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 476,
      "end_pos": 484
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "strategies",
    "section": "Effect of Model Size"
  },
  {
    "text": "MRPC",
    "type": "dataset",
    "char_interval": {
      "start_pos": 532,
      "end_pos": 536
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "representations",
    "section": "Effect of Model Size"
  },
  {
    "text": "labeled training examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 558,
      "end_pos": 583
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "feature-based and fine-tuning",
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-training tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 625,
      "end_pos": 643
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "ELMo, uses task-specific",
    "section": "Effect of Model Size"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 842,
      "end_pos": 853
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "task-specific",
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 894,
      "end_pos": 904
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "downstream tasks",
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1014,
      "end_pos": 1024
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "pre-training",
    "section": "Effect of Model Size"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1039,
      "end_pos": 1048
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "use unidirectional",
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1063,
      "end_pos": 1073
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "language models",
    "section": "Effect of Model Size"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1078,
      "end_pos": 1088
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "learn general",
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1103,
      "end_pos": 1113
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "representations",
    "section": "Effect of Model Size"
  },
  {
    "text": "machine translation",
    "type": "task",
    "char_interval": {
      "start_pos": 1234,
      "end_pos": 1253
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 0,
    "span_text": "tuning approaches. The major",
    "section": "Effect of Model Size"
  },
  {
    "text": "language modeling",
    "type": "task",
    "char_interval": {
      "start_pos": 1258,
      "end_pos": 1275
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "limitation is that",
    "section": "Effect of Model Size"
  },
  {
    "text": "LM perplexity",
    "type": "metric",
    "char_interval": {
      "start_pos": 1306,
      "end_pos": 1319
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "unidirectional",
    "section": "Effect of Model Size"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1332,
      "end_pos": 1345
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "limits the choice",
    "section": "Effect of Model Size"
  },
  {
    "text": "model sizes",
    "type": "other",
    "char_interval": {
      "start_pos": 1464,
      "end_pos": 1475
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "architecture",
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-trained",
    "type": "other",
    "char_interval": {
      "start_pos": 1582,
      "end_pos": 1593
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "Such restrictions",
    "section": "Effect of Model Size"
  },
  {
    "text": "downstream task",
    "type": "other",
    "char_interval": {
      "start_pos": 1627,
      "end_pos": 1642
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "level tasks, and",
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-trained bi-LM",
    "type": "other",
    "char_interval": {
      "start_pos": 1668,
      "end_pos": 1685
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "when applying finetuning",
    "section": "Effect of Model Size"
  },
  {
    "text": "featurebased approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1918,
      "end_pos": 1939
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "Encoder Representations from",
    "section": "Effect of Model Size"
  },
  {
    "text": "downstream tasks",
    "type": "other",
    "char_interval": {
      "start_pos": 2006,
      "end_pos": 2022
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 11,
    "span_text": "unidirectionality constraint",
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-trained representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2173,
      "end_pos": 2200
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 12,
    "span_text": "tokens from the input, and",
    "section": "Effect of Model Size"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 11,
      "end_pos": 15
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "model",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "fine-tuning approach",
    "type": "other",
    "char_interval": {
      "start_pos": 55,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "effective for improving many",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 92,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "language processing tasks.",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "pre-trained model",
    "type": "other",
    "char_interval": {
      "start_pos": 129,
      "end_pos": 146
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "sentence-level tasks",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 225,
      "end_pos": 247
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "relationships between",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Transformer encoder architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 387,
      "end_pos": 419
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "models are required to produce fine-grained",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "task-specific model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 445,
      "end_pos": 477
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "level. There are two existing strategies",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 588,
      "end_pos": 601
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "The feature-",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Named Entity Recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 769,
      "end_pos": 793
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "the Generative Pre-trained",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "CoNL-2003",
    "type": "dataset",
    "char_interval": {
      "start_pos": 759,
      "end_pos": 768
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "such as the",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "NER",
    "type": "method",
    "char_interval": {
      "start_pos": 795,
      "end_pos": 798
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Transformer",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Word Piece model",
    "type": "other",
    "char_interval": {
      "start_pos": 853,
      "end_pos": 869
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "parameters, and",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 931,
      "end_pos": 935
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "pretrained",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "tagging task",
    "type": "task",
    "char_interval": {
      "start_pos": 989,
      "end_pos": 1001
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "objective function",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "CRF layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1019,
      "end_pos": 1028
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "training,",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "token-level classifier",
    "type": "other",
    "char_interval": {
      "start_pos": 1113,
      "end_pos": 1135
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "span_text": "representations. We argue that current",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "NER label set",
    "type": "other",
    "char_interval": {
      "start_pos": 1145,
      "end_pos": 1158
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "techniques restrict",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1209,
      "end_pos": 1231
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "especially for the fine-tuning",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1324,
      "end_pos": 1328
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "and this",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "contextual embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1336,
      "end_pos": 1357
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "limits the choice of architectures",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BiLSTM",
    "type": "method",
    "char_interval": {
      "start_pos": 1428,
      "end_pos": 1434
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "GPT, the",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1446,
      "end_pos": 1466
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "use a left-toright architecture",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1502,
      "end_pos": 1512
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "only attend",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "state-of-the-art methods",
    "type": "other",
    "char_interval": {
      "start_pos": 1541,
      "end_pos": 1565
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "self-attention layers of the",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "token representations",
    "type": "other",
    "char_interval": {
      "start_pos": 1611,
      "end_pos": 1632
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "optimal for sentence-level",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 1684,
      "end_pos": 1695
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "finetuning based",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1715,
      "end_pos": 1718
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "token-level",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 1726,
      "end_pos": 1737
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "tasks such as",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1779,
      "end_pos": 1783
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "incorporate",
    "section": "Feature-based Approach with BERT"
  },
  {
    "text": "transfer learning",
    "type": "other",
    "char_interval": {
      "start_pos": 37,
      "end_pos": 54
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "shown to be effective",
    "section": "Conclusion"
  },
  {
    "text": "language models",
    "type": "other",
    "char_interval": {
      "start_pos": 60,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "for improving many",
    "section": "Conclusion"
  },
  {
    "text": "unsupervised pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 105,
      "end_pos": 130
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "span_text": "tasks. These include sentence",
    "section": "Conclusion"
  },
  {
    "text": "language understanding",
    "type": "task",
    "char_interval": {
      "start_pos": 159,
      "end_pos": 181
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "natural language inference",
    "section": "Conclusion"
  },
  {
    "text": "deep unidirectional architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 267,
      "end_pos": 300
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "analyzing them holistically, as well as",
    "section": "Conclusion"
  },
  {
    "text": "deep bidirectional architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 367,
      "end_pos": 399
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "answering, where models are required",
    "section": "Conclusion"
  },
  {
    "text": "NLP tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 475,
      "end_pos": 484
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "strategies",
    "section": "Conclusion"
  },
  {
    "text": "Masked LM",
    "type": "method",
    "char_interval": {
      "start_pos": 486,
      "end_pos": 495
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "for applying",
    "section": "Conclusion"
  },
  {
    "text": "Masking Procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "trained language",
    "section": "Conclusion"
  },
  {
    "text": "unlabeled sentence",
    "type": "dataset",
    "char_interval": {
      "start_pos": 535,
      "end_pos": 553
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "representations to downstream tasks",
    "section": "Conclusion"
  },
  {
    "text": "random masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 589,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "The feature-based approach",
    "section": "Conclusion"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 674,
      "end_pos": 691
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "include the pre-trained",
    "section": "Conclusion"
  },
  {
    "text": "MASK token",
    "type": "other",
    "char_interval": {
      "start_pos": 766,
      "end_pos": 770
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "the",
    "section": "Conclusion"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "trained language",
    "section": "Conclusion"
  },
  {
    "text": "random word",
    "type": "other",
    "char_interval": {
      "start_pos": 844,
      "end_pos": 855
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "-specific parameters",
    "section": "Conclusion"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "trained language",
    "section": "Conclusion"
  },
  {
    "text": "representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1015,
      "end_pos": 1029
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "pre-training, where",
    "section": "Conclusion"
  },
  {
    "text": "Transformer encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 1108,
      "end_pos": 1127
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "span_text": "representations. We argue that",
    "section": "Conclusion"
  },
  {
    "text": "random words",
    "type": "other",
    "char_interval": {
      "start_pos": 1213,
      "end_pos": 1225
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "especially for the fine",
    "section": "Conclusion"
  },
  {
    "text": "contextual representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1268,
      "end_pos": 1293
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "is that standard language",
    "section": "Conclusion"
  },
  {
    "text": "input token",
    "type": "other",
    "char_interval": {
      "start_pos": 1303,
      "end_pos": 1314
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "are unidirectional",
    "section": "Conclusion"
  },
  {
    "text": "random replacement",
    "type": "other",
    "char_interval": {
      "start_pos": 1338,
      "end_pos": 1356
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "the choice of architectures",
    "section": "Conclusion"
  },
  {
    "text": "language understanding capability",
    "type": "other",
    "char_interval": {
      "start_pos": 1451,
      "end_pos": 1484
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "left-toright architecture, where",
    "section": "Conclusion"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 1591,
      "end_pos": 1600
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "restrictions are",
    "section": "Conclusion"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2154,
      "end_pos": 2178
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 12,
    "span_text": "masks some of the tokens",
    "section": "Conclusion"
  },
  {
    "text": "corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2573,
      "end_pos": 2579
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 14,
    "span_text": "prediction\"",
    "section": "Conclusion"
  },
  {
    "text": "Adam",
    "type": "method",
    "char_interval": {
      "start_pos": 2588,
      "end_pos": 2592
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 14,
    "span_text": "that jointly",
    "section": "Conclusion"
  },
  {
    "text": "training loss",
    "type": "metric",
    "char_interval": {
      "start_pos": 2894,
      "end_pos": 2907
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 15,
    "span_text": "bidirectional",
    "section": "Conclusion"
  },
  {
    "text": "masked LM likelihood",
    "type": "other",
    "char_interval": {
      "start_pos": 2931,
      "end_pos": 2951
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "also in contrast to",
    "section": "Conclusion"
  },
  {
    "text": "next sentence prediction likelihood",
    "type": "other",
    "char_interval": {
      "start_pos": 2965,
      "end_pos": 3000
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": ", which uses a shallow concatenation",
    "section": "Conclusion"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 3014,
      "end_pos": 3023
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 16,
    "span_text": "independently trained",
    "section": "Conclusion"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 3111,
      "end_pos": 3121
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 17,
    "span_text": "the need",
    "section": "Conclusion"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 28,
      "end_pos": 43
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "has been shown",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "batch size",
    "type": "other",
    "char_interval": {
      "start_pos": 103,
      "end_pos": 113
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "processing tasks.",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "learning rate",
    "type": "other",
    "char_interval": {
      "start_pos": 115,
      "end_pos": 128
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "These include sentence",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "number of training epochs",
    "type": "other",
    "char_interval": {
      "start_pos": 134,
      "end_pos": 159
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "sentence-level tasks such as natural",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "dropout probability",
    "type": "other",
    "char_interval": {
      "start_pos": 165,
      "end_pos": 184
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "language inference and",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "hyperparameter",
    "type": "other",
    "char_interval": {
      "start_pos": 221,
      "end_pos": 235
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "the relationships",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "large data sets",
    "type": "dataset",
    "char_interval": {
      "start_pos": 386,
      "end_pos": 401
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "models are required",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "labeled training examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 415,
      "end_pos": 440
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "fine-grained output at the",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "small data sets",
    "type": "dataset",
    "char_interval": {
      "start_pos": 496,
      "end_pos": 511
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "applying pre-trained",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "development set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 677,
      "end_pos": 692
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "the pre-trained",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 712,
      "end_pos": 716
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "additional",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 718,
      "end_pos": 722
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "additional",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 727,
      "end_pos": 738
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "span_text": "features. The fine",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "model architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 886,
      "end_pos": 905
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "the downstream tasks",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "architecture differences",
    "type": "other",
    "char_interval": {
      "start_pos": 967,
      "end_pos": 991
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "approaches share the same objective",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "finetuning approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 1018,
      "end_pos": 1039
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "training, where they",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1057,
      "end_pos": 1079
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 5,
    "span_text": "language models to learn",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "pre-training method",
    "type": "other",
    "char_interval": {
      "start_pos": 1110,
      "end_pos": 1129
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "span_text": "representations. We argue that",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1141,
      "end_pos": 1152
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "techniques restrict",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "Transformer LM",
    "type": "other",
    "char_interval": {
      "start_pos": 1183,
      "end_pos": 1197
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "trained representations",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "large text corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1203,
      "end_pos": 1220
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": ", especially for",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1315,
      "end_pos": 1318
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 7,
    "span_text": "unidirectional",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "bi-directionality",
    "type": "other",
    "char_interval": {
      "start_pos": 1427,
      "end_pos": 1444
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "GPT, the authors",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "pretraining tasks",
    "type": "other",
    "char_interval": {
      "start_pos": 1457,
      "end_pos": 1474
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "toright architecture",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1624,
      "end_pos": 1628
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "sentence-level",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1633,
      "end_pos": 1636
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "tasks",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "Books Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1673,
      "end_pos": 1685
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "applying finetuning",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1753,
      "end_pos": 1762
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "answering, where",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "sentence separator",
    "type": "other",
    "char_interval": {
      "start_pos": 1790,
      "end_pos": 1808
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 9,
    "span_text": "incorporate context from both",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "classifier token",
    "type": "other",
    "char_interval": {
      "start_pos": 1821,
      "end_pos": 1837
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "In this paper,",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "sentence A/B embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1922,
      "end_pos": 1945
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 10,
    "span_text": "Representations from Transformers",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "bidirectionality",
    "type": "other",
    "char_interval": {
      "start_pos": 2479,
      "end_pos": 2495
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 13,
    "span_text": "bidirectional Transformer",
    "section": "A.3 Fine-tuning Procedure"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 32,
      "end_pos": 36
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "been",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "entailment classification",
    "type": "task",
    "char_interval": {
      "start_pos": 315,
      "end_pos": 340
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "tasks such as named entity",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "MNLI Multi-Genre Natural Language Inference",
    "type": "dataset",
    "char_interval": {
      "start_pos": 240,
      "end_pos": 283
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "between sentences by analyzing them holistically",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "binary classification",
    "type": "task",
    "char_interval": {
      "start_pos": 529,
      "end_pos": 550
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "representations to downstream",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "QP Quora Question Pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 500,
      "end_pos": 523
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "pre-trained language representations",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "binary classification",
    "type": "task",
    "char_interval": {
      "start_pos": 529,
      "end_pos": 550
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "representations to downstream",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "QNLI Question Natural Language Inference",
    "type": "dataset",
    "char_interval": {
      "start_pos": 648,
      "end_pos": 688
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "specific architectures that include the pre-trained",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "Stanford Question Answering Dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 709,
      "end_pos": 744
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "span_text": "as additional features. The fine-tuning",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "positive examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 807,
      "end_pos": 824
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Open AI GPT), introduces",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "negative examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 901,
      "end_pos": 918
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "tasks by simply fine",
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks"
  },
  {
    "text": "Stanford Sentiment Treebank",
    "type": "dataset",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 31
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "Language model pre-training has",
    "section": "Sst-2"
  },
  {
    "text": "binary single-sentence classification",
    "type": "task",
    "char_interval": {
      "start_pos": 37,
      "end_pos": 74
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "shown to be effective for improving many",
    "section": "Sst-2"
  },
  {
    "text": "CoLA The Corpus of Linguistic Acceptability",
    "type": "dataset",
    "char_interval": {
      "start_pos": 176,
      "end_pos": 219
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "inference and paraphrasing, which aim to predict",
    "section": "Sst-2"
  },
  {
    "text": "binary single-sentence classification",
    "type": "task",
    "char_interval": {
      "start_pos": 225,
      "end_pos": 262
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "relationships between sentences by analyzing",
    "section": "Sst-2"
  },
  {
    "text": "Semantic Textual Similarity Benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 41
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "Language model pre-training has been shown",
    "section": "Sts-b"
  },
  {
    "text": "sentence pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 61,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "for improving many",
    "section": "Sts-b"
  },
  {
    "text": "MRPC Microsoft Research Paraphrase Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 239,
      "end_pos": 280
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "between sentences by analyzing them holistically",
    "section": "Sts-b"
  },
  {
    "text": "sentence pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 293,
      "end_pos": 307
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "well as token-",
    "section": "Sts-b"
  },
  {
    "text": "RTE Recognizing Textual Entailment",
    "type": "dataset",
    "char_interval": {
      "start_pos": 448,
      "end_pos": 482
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "level. There are two existing strategies",
    "section": "Sts-b"
  },
  {
    "text": "binary entailment task",
    "type": "task",
    "char_interval": {
      "start_pos": 488,
      "end_pos": 510
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "for applying pre-trained",
    "section": "Sts-b"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 522,
      "end_pos": 526
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "representations",
    "section": "Sts-b"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 547,
      "end_pos": 560
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "downstream tasks: feature",
    "section": "Sts-b"
  },
  {
    "text": "WNLI Winograd NLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 562,
      "end_pos": 579
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "feature-based and fine",
    "section": "Sts-b"
  },
  {
    "text": "natural language inference dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 591,
      "end_pos": 625
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "The feature-based approach, such as",
    "section": "Sts-b"
  },
  {
    "text": "GLUE webpage",
    "type": "dataset",
    "char_interval": {
      "start_pos": 631,
      "end_pos": 643
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "uses task-specific",
    "section": "Sts-b"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 812,
      "end_pos": 820
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "AI GPT),",
    "section": "Sts-b"
  },
  {
    "text": "this set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 876,
      "end_pos": 884
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "trained on",
    "section": "Sts-b"
  },
  {
    "text": "GLUE submission",
    "type": "dataset",
    "char_interval": {
      "start_pos": 920,
      "end_pos": 935
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "tuning all pretrained",
    "section": "Sts-b"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 32,
      "end_pos": 36
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "span_text": "been",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "masked language model",
    "type": "other",
    "char_interval": {
      "start_pos": 116,
      "end_pos": 137
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "These include sentence-level",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "MLM",
    "type": "other",
    "char_interval": {
      "start_pos": 139,
      "end_pos": 142
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "level",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "ablation study",
    "type": "other",
    "char_interval": {
      "start_pos": 175,
      "end_pos": 189
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "inference and paraphrasing",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "masking strategies",
    "type": "other",
    "char_interval": {
      "start_pos": 226,
      "end_pos": 244
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "relationships between",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "fine-tuning",
    "type": "task",
    "char_interval": {
      "start_pos": 345,
      "end_pos": 356
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "recognition and",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "NER",
    "type": "task",
    "char_interval": {
      "start_pos": 467,
      "end_pos": 470
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "existing",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "feature-based approach",
    "type": "task",
    "char_interval": {
      "start_pos": 590,
      "end_pos": 612
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 3,
    "span_text": "The feature-based approach",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "Dev results",
    "type": "dataset",
    "char_interval": {
      "start_pos": 437,
      "end_pos": 448
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 1,
    "span_text": "the token level",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 458,
      "end_pos": 462
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "are",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "NER",
    "type": "dataset",
    "char_interval": {
      "start_pos": 467,
      "end_pos": 470
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 2,
    "span_text": "existing",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "MASK",
    "type": "other",
    "char_interval": {
      "start_pos": 734,
      "end_pos": 738
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "The fine",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "SAME",
    "type": "other",
    "char_interval": {
      "start_pos": 810,
      "end_pos": 814
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "Open AI",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "RND",
    "type": "other",
    "char_interval": {
      "start_pos": 858,
      "end_pos": 861
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "parameters",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "MLM",
    "type": "other",
    "char_interval": {
      "start_pos": 805,
      "end_pos": 808
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 4,
    "span_text": "(Open",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "Dev set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1124,
      "end_pos": 1131
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "argue that",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1209,
      "end_pos": 1213
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 6,
    "span_text": "especially",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "MASK strategy",
    "type": "other",
    "char_interval": {
      "start_pos": 1427,
      "end_pos": 1440
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "GPT, the authors",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "NER",
    "type": "task",
    "char_interval": {
      "start_pos": 1500,
      "end_pos": 1503
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "only",
    "section": "C.2 Ablation for Different Masking Procedures"
  },
  {
    "text": "RND strategy",
    "type": "other",
    "char_interval": {
      "start_pos": 1535,
      "end_pos": 1547
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 8,
    "span_text": "the self-attention",
    "section": "C.2 Ablation for Different Masking Procedures"
  }
]