[
  {
    "text": "natural language processing tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 78,
      "end_pos": 111
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Introduction",
    "span_text": "natural language processing tasks"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 127,
      "end_pos": 147
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Introduction",
    "span_text": "sentence-level tasks"
  },
  {
    "text": "natural language inference",
    "type": "task",
    "char_interval": {
      "start_pos": 156,
      "end_pos": 182
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Introduction",
    "pr_score": 0.02513568025733821,
    "span_text": "natural language inference"
  },
  {
    "text": "paraphrasing",
    "type": "task",
    "char_interval": {
      "start_pos": 187,
      "end_pos": 199
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Introduction",
    "pr_score": 0.009062939244979529,
    "span_text": "paraphrasing"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 301,
      "end_pos": 318
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Introduction",
    "span_text": "token-level tasks"
  },
  {
    "text": "named entity recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 327,
      "end_pos": 351
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Introduction",
    "pr_score": 0.012950096442141474,
    "span_text": "named entity recognition"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 356,
      "end_pos": 374
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Introduction",
    "pr_score": 0.029112646817448393,
    "span_text": "question answering"
  },
  {
    "text": "pre-trained language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 500,
      "end_pos": 536
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Introduction",
    "span_text": "pre-trained language representations"
  },
  {
    "text": "feature-based",
    "type": "other",
    "char_interval": {
      "start_pos": 558,
      "end_pos": 571
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Introduction",
    "span_text": "feature-based"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 576,
      "end_pos": 587
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Introduction",
    "span_text": "fine-tuning"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 625,
      "end_pos": 629
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Introduction",
    "pr_score": 0.02263064499680652,
    "span_text": "ELMo"
  },
  {
    "text": "Generative Pre-trained Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 771,
      "end_pos": 805
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Introduction",
    "pr_score": 0.005296974677498111,
    "span_text": "Generative Pre-trained Transformer"
  },
  {
    "text": "language models",
    "type": "other",
    "char_interval": {
      "start_pos": 1058,
      "end_pos": 1073
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Introduction",
    "span_text": "language models"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1420,
      "end_pos": 1431
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Introduction",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "left-toright architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 1451,
      "end_pos": 1476
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Introduction",
    "span_text": "left-toright architecture"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1538,
      "end_pos": 1552
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Introduction",
    "span_text": "self-attention"
  },
  {
    "text": "Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 1567,
      "end_pos": 1578
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Introduction",
    "span_text": "Transformer"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1618,
      "end_pos": 1638
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Introduction",
    "span_text": "sentence-level tasks"
  },
  {
    "text": "finetuning based approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 1680,
      "end_pos": 1707
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Introduction",
    "span_text": "finetuning based approaches"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1711,
      "end_pos": 1728
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Introduction",
    "span_text": "token-level tasks"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 1737,
      "end_pos": 1755
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Introduction",
    "pr_score": 0.029112646817448393,
    "span_text": "question answering"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1894,
      "end_pos": 1898
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "Introduction",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "Bidirectional Encoder Representations from Transformers",
    "type": "other",
    "char_interval": {
      "start_pos": 1900,
      "end_pos": 1955
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "Introduction",
    "span_text": "Bidirectional Encoder Representations from Transformers"
  },
  {
    "text": "masked language model",
    "type": "other",
    "char_interval": {
      "start_pos": 2039,
      "end_pos": 2060
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "Introduction",
    "span_text": "masked language model"
  },
  {
    "text": "Cloze task",
    "type": "task",
    "char_interval": {
      "start_pos": 2108,
      "end_pos": 2118
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "Introduction",
    "span_text": "Cloze task"
  },
  {
    "text": "MLM objective",
    "type": "other",
    "char_interval": {
      "start_pos": 2354,
      "end_pos": 2367
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "Introduction",
    "span_text": "MLM objective"
  },
  {
    "text": "left and the right context",
    "type": "other",
    "char_interval": {
      "start_pos": 2407,
      "end_pos": 2433
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "Introduction",
    "span_text": "left and the right context"
  },
  {
    "text": "deep bidirectional Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 2465,
      "end_pos": 2495
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "Introduction",
    "span_text": "deep bidirectional Transformer"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2554,
      "end_pos": 2578
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 14,
    "section": "Introduction",
    "pr_score": 0.015470645591286416,
    "span_text": "next sentence prediction"
  },
  {
    "text": "bidirectional pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 2715,
      "end_pos": 2741
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Introduction",
    "span_text": "bidirectional pre-training"
  },
  {
    "text": "language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2746,
      "end_pos": 2770
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Introduction",
    "span_text": "language representations"
  },
  {
    "text": "language models",
    "type": "method",
    "char_interval": {
      "start_pos": 2800,
      "end_pos": 2815
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Introduction",
    "span_text": "language models"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 2834,
      "end_pos": 2838
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Introduction",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "masked language models",
    "type": "other",
    "char_interval": {
      "start_pos": 2844,
      "end_pos": 2866
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Introduction",
    "span_text": "masked language models"
  },
  {
    "text": "deep bidirectional representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2888,
      "end_pos": 2922
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Introduction",
    "span_text": "deep bidirectional representations"
  },
  {
    "text": "LMs",
    "type": "other",
    "char_interval": {
      "start_pos": 3059,
      "end_pos": 3062
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Introduction",
    "span_text": "LMs"
  },
  {
    "text": "pre-trained representations",
    "type": "other",
    "char_interval": {
      "start_pos": 3077,
      "end_pos": 3104
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Introduction",
    "span_text": "pre-trained representations"
  },
  {
    "text": "taskspecific architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 3149,
      "end_pos": 3175
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Introduction",
    "span_text": "taskspecific architectures"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 3177,
      "end_pos": 3181
    },
    "attributes": {},
    "sentence_context": "BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 18,
    "section": "Introduction",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "representation model",
    "type": "other",
    "char_interval": {
      "start_pos": 3212,
      "end_pos": 3232
    },
    "attributes": {},
    "sentence_context": "BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 18,
    "section": "Introduction",
    "span_text": "representation model"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 3296,
      "end_pos": 3310
    },
    "attributes": {},
    "sentence_context": "BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 18,
    "section": "Introduction",
    "span_text": "sentence-level tasks"
  },
  {
    "text": "token-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 3315,
      "end_pos": 3332
    },
    "attributes": {},
    "sentence_context": "BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 18,
    "section": "Introduction",
    "span_text": "token-level tasks"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 2834,
      "end_pos": 2838
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Introduction",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "task-specific architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 3353,
      "end_pos": 3380
    },
    "attributes": {},
    "sentence_context": "BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",
    "sentence_index": 18,
    "section": "Introduction",
    "span_text": "task-specific architectures"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 27,
      "end_pos": 39
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Related Work",
    "span_text": "pre-training"
  },
  {
    "text": "language representations",
    "type": "other",
    "char_interval": {
      "start_pos": 48,
      "end_pos": 72
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Related Work",
    "span_text": "language representations"
  },
  {
    "text": "representations",
    "type": "other",
    "char_interval": {
      "start_pos": 27,
      "end_pos": 42
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "representations"
  },
  {
    "text": "word embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 154,
      "end_pos": 169
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "word embeddings"
  },
  {
    "text": "embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 253,
      "end_pos": 263
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "embeddings"
  },
  {
    "text": "word embedding vectors",
    "type": "other",
    "char_interval": {
      "start_pos": 298,
      "end_pos": 320
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "word embedding vectors"
  },
  {
    "text": "language modeling",
    "type": "other",
    "char_interval": {
      "start_pos": 336,
      "end_pos": 353
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "language modeling"
  },
  {
    "text": "sentence embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 548,
      "end_pos": 567
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "sentence embeddings"
  },
  {
    "text": "paragraph embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 571,
      "end_pos": 591
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 0,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "paragraph embeddings"
  },
  {
    "text": "sentence representations",
    "type": "other",
    "char_interval": {
      "start_pos": 602,
      "end_pos": 626
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "sentence representations"
  },
  {
    "text": "denoising autoencoder",
    "type": "other",
    "char_interval": {
      "start_pos": 793,
      "end_pos": 814
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "denoising autoencoder"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 835,
      "end_pos": 839
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.02263064499680652,
    "span_text": "ELMo"
  },
  {
    "text": "word embedding research",
    "type": "other",
    "char_interval": {
      "start_pos": 885,
      "end_pos": 908
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "word embedding research"
  },
  {
    "text": "context-sensitive features",
    "type": "other",
    "char_interval": {
      "start_pos": 951,
      "end_pos": 977
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 0,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "context-sensitive features"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 1019,
      "end_pos": 1033
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "language model"
  },
  {
    "text": "contextual representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1039,
      "end_pos": 1064
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "contextual representation"
  },
  {
    "text": "word embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1184,
      "end_pos": 1199
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "word embeddings"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 1321,
      "end_pos": 1339
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.029112646817448393,
    "span_text": "question answering"
  },
  {
    "text": "sentiment analysis",
    "type": "task",
    "char_interval": {
      "start_pos": 1341,
      "end_pos": 1359
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.010665923930541285,
    "span_text": "sentiment analysis"
  },
  {
    "text": "named entity recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 1365,
      "end_pos": 1389
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.012950096442141474,
    "span_text": "named entity recognition"
  },
  {
    "text": "predict a single word",
    "type": "task",
    "char_interval": {
      "start_pos": 1455,
      "end_pos": 1476
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.006695822041395983,
    "span_text": "predict a single word"
  },
  {
    "text": "LSTMs",
    "type": "method",
    "char_interval": {
      "start_pos": 1516,
      "end_pos": 1521
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.006986944738847982,
    "span_text": "LSTMs"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 1534,
      "end_pos": 1538
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.02263064499680652,
    "span_text": "ELMo"
  },
  {
    "text": "feature-based",
    "type": "other",
    "char_interval": {
      "start_pos": 1555,
      "end_pos": 1568
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "feature-based"
  },
  {
    "text": "deeply bidirectional",
    "type": "other",
    "char_interval": {
      "start_pos": 1577,
      "end_pos": 1597
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 0,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "deeply bidirectional"
  },
  {
    "text": "cloze task",
    "type": "other",
    "char_interval": {
      "start_pos": 1615,
      "end_pos": 1625
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Unsupervised Feature-based Approaches",
    "span_text": "cloze task"
  },
  {
    "text": "text generation",
    "type": "task",
    "char_interval": {
      "start_pos": 1667,
      "end_pos": 1682
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Unsupervised Feature-based Approaches",
    "pr_score": 0.008057945972645213,
    "span_text": "text generation"
  },
  {
    "text": "feature-based approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 12,
      "end_pos": 36
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "feature-based approaches"
  },
  {
    "text": "word embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 89,
      "end_pos": 103
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "word embedding"
  },
  {
    "text": "sentence encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 151,
      "end_pos": 159
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "sentence encoders"
  },
  {
    "text": "document encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 163,
      "end_pos": 180
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "document encoders"
  },
  {
    "text": "contextual token representations",
    "type": "other",
    "char_interval": {
      "start_pos": 195,
      "end_pos": 227
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "contextual token representations"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 572,
      "end_pos": 586
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "language model"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 448,
      "end_pos": 459
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "section": "Unsupervised Fine-tuning Approaches",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "sentence-level tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 151,
      "end_pos": 317
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "sentence-level tasks"
  },
  {
    "text": "GLUE benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 542,
      "end_pos": 556
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Unsupervised Fine-tuning Approaches",
    "pr_score": 0.0058224539490399855,
    "span_text": "GLUE benchmark"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 587,
      "end_pos": 591
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 0,
    "section": "Unsupervised Fine-tuning Approaches",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "language model",
    "type": "other",
    "char_interval": {
      "start_pos": 572,
      "end_pos": 586
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "language model"
  },
  {
    "text": "auto-encoder objectives",
    "type": "other",
    "char_interval": {
      "start_pos": 606,
      "end_pos": 629
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Unsupervised Fine-tuning Approaches",
    "span_text": "auto-encoder objectives"
  },
  {
    "text": "natural language inference",
    "type": "task",
    "char_interval": {
      "start_pos": 103,
      "end_pos": 129
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Transfer Learning from Supervised Data",
    "pr_score": 0.02513568025733821,
    "span_text": "natural language inference"
  },
  {
    "text": "machine translation",
    "type": "task",
    "char_interval": {
      "start_pos": 134,
      "end_pos": 153
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Transfer Learning from Supervised Data",
    "pr_score": 0.009476468202587657,
    "span_text": "machine translation"
  },
  {
    "text": "transfer learning",
    "type": "other",
    "char_interval": {
      "start_pos": 220,
      "end_pos": 237
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Transfer Learning from Supervised Data",
    "span_text": "transfer learning"
  },
  {
    "text": "Ima-ge Net",
    "type": "dataset",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 345
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Transfer Learning from Supervised Data",
    "pr_score": 0.0058224539490399855,
    "span_text": "Ima-ge Net"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 13,
      "end_pos": 17
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Bert",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 105,
      "end_pos": 117
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Bert",
    "span_text": "pre-training"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 122,
      "end_pos": 133
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Bert",
    "span_text": "fine-tuning"
  },
  {
    "text": "unlabeled data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 180,
      "end_pos": 194
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Bert",
    "pr_score": 0.0036494493837929015,
    "span_text": "unlabeled data"
  },
  {
    "text": "pre-training tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 210,
      "end_pos": 228
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Bert",
    "span_text": "pre-training tasks"
  },
  {
    "text": "labeled data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 362,
      "end_pos": 374
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Bert",
    "pr_score": 0.0036494493837929015,
    "span_text": "labeled data"
  },
  {
    "text": "downstream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 384,
      "end_pos": 400
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Bert",
    "span_text": "downstream tasks"
  },
  {
    "text": "question-answering",
    "type": "task",
    "char_interval": {
      "start_pos": 530,
      "end_pos": 548
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Bert",
    "pr_score": 0.004196866791361837,
    "span_text": "question-answering"
  },
  {
    "text": "BERT model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 644,
      "end_pos": 648
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Bert",
    "span_text": "BERT model architecture"
  },
  {
    "text": "Transformer encoder",
    "type": "method",
    "char_interval": {
      "start_pos": 882,
      "end_pos": 901
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Bert",
    "span_text": "Transformer encoder"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1415,
      "end_pos": 1429
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Bert",
    "span_text": "self-attention"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1490,
      "end_pos": 1499
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Bert",
    "pr_score": 0.024008691478147038,
    "span_text": "BERT BASE"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1548,
      "end_pos": 1558
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Bert",
    "pr_score": 0.026976967420331822,
    "span_text": "BERT LARGE"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1612,
      "end_pos": 1623
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Bert",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1674,
      "end_pos": 1678
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Bert",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "bidirectional self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1696,
      "end_pos": 1724
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Bert",
    "span_text": "bidirectional self-attention"
  },
  {
    "text": "GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1736,
      "end_pos": 1739
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Bert",
    "pr_score": 0.04647840132463596,
    "span_text": "GPT"
  },
  {
    "text": "constrained self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 1757,
      "end_pos": 1783
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Bert",
    "span_text": "constrained self-attention"
  },
  {
    "text": "down-stream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 1906,
      "end_pos": 1923
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "Bert",
    "span_text": "down-stream tasks"
  },
  {
    "text": "sentence pair",
    "type": "other",
    "char_interval": {
      "start_pos": 1999,
      "end_pos": 2007
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "Bert",
    "span_text": "sentence pair"
  },
  {
    "text": "Question",
    "type": "other",
    "char_interval": {
      "start_pos": 2039,
      "end_pos": 2047
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "Bert",
    "span_text": "Question"
  },
  {
    "text": "Answer",
    "type": "other",
    "char_interval": {
      "start_pos": 2049,
      "end_pos": 2055
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "Bert",
    "span_text": "Answer"
  },
  {
    "text": "span of contiguous text",
    "type": "other",
    "char_interval": {
      "start_pos": 2136,
      "end_pos": 2159
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 12,
    "section": "Bert",
    "span_text": "span of contiguous text"
  },
  {
    "text": "sequence",
    "type": "other",
    "char_interval": {
      "start_pos": 2207,
      "end_pos": 2215
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 12,
    "section": "Bert",
    "span_text": "sequence"
  },
  {
    "text": "Word Piece embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 2333,
      "end_pos": 2354
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "Bert",
    "span_text": "Word Piece embeddings"
  },
  {
    "text": "classification token",
    "type": "other",
    "char_interval": {
      "start_pos": 2441,
      "end_pos": 2461
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "Bert",
    "span_text": "classification token"
  },
  {
    "text": "classification tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 2574,
      "end_pos": 2594
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 14,
    "section": "Bert",
    "span_text": "classification tasks"
  },
  {
    "text": "Sentence pairs",
    "type": "other",
    "char_interval": {
      "start_pos": 2596,
      "end_pos": 2610
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 14,
    "section": "Bert",
    "span_text": "Sentence pairs"
  },
  {
    "text": "special token",
    "type": "other",
    "char_interval": {
      "start_pos": 2730,
      "end_pos": 2743
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Bert",
    "span_text": "special token"
  },
  {
    "text": "learned embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 2769,
      "end_pos": 2786
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Bert",
    "span_text": "learned embedding"
  },
  {
    "text": "sentence A",
    "type": "other",
    "char_interval": {
      "start_pos": 2835,
      "end_pos": 2845
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Bert",
    "span_text": "sentence A"
  },
  {
    "text": "sentence B",
    "type": "other",
    "char_interval": {
      "start_pos": 2849,
      "end_pos": 2859
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Bert",
    "span_text": "sentence B"
  },
  {
    "text": "input embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 2893,
      "end_pos": 2908
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Bert",
    "span_text": "input embedding"
  },
  {
    "text": "hidden vector",
    "type": "other",
    "char_interval": {
      "start_pos": 2925,
      "end_pos": 2938
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Bert",
    "span_text": "hidden vector"
  },
  {
    "text": "hidden vector",
    "type": "other",
    "char_interval": {
      "start_pos": 2992,
      "end_pos": 3005
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Bert",
    "span_text": "hidden vector"
  },
  {
    "text": "input representation",
    "type": "other",
    "char_interval": {
      "start_pos": 3057,
      "end_pos": 3077
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 0,
    "section": "Bert",
    "span_text": "input representation"
  },
  {
    "text": "token",
    "type": "other",
    "char_interval": {
      "start_pos": 3122,
      "end_pos": 3127
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Bert",
    "span_text": "token"
  },
  {
    "text": "segment",
    "type": "other",
    "char_interval": {
      "start_pos": 3129,
      "end_pos": 3136
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Bert",
    "span_text": "segment"
  },
  {
    "text": "position embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 3142,
      "end_pos": 3161
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Bert",
    "span_text": "position embeddings"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 108,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Pre-training BERT",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "Masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 260,
      "end_pos": 269
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Pre-training BERT",
    "pr_score": 0.013551870030823051,
    "span_text": "Masked LM"
  },
  {
    "text": "Transformer encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 780,
      "end_pos": 799
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Pre-training BERT",
    "span_text": "Transformer encoder"
  },
  {
    "text": "Transformer decoder",
    "type": "other",
    "char_interval": {
      "start_pos": 858,
      "end_pos": 877
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Pre-training BERT",
    "span_text": "Transformer decoder"
  },
  {
    "text": "text generation",
    "type": "task",
    "char_interval": {
      "start_pos": 904,
      "end_pos": 919
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Pre-training BERT",
    "pr_score": 0.008057945972645213,
    "span_text": "text generation"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 1109,
      "end_pos": 1118
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "section": "Pre-training BERT",
    "pr_score": 0.013551870030823051,
    "span_text": "masked LM"
  },
  {
    "text": "Cloze task",
    "type": "task",
    "char_interval": {
      "start_pos": 1165,
      "end_pos": 1175
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "Pre-training BERT",
    "span_text": "Cloze task"
  },
  {
    "text": "denoising auto-encoders",
    "type": "other",
    "char_interval": {
      "start_pos": 1447,
      "end_pos": 1470
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Pre-training BERT",
    "span_text": "denoising auto-encoders"
  },
  {
    "text": "pre-trained model",
    "type": "other",
    "char_interval": {
      "start_pos": 1600,
      "end_pos": 1617
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Pre-training BERT",
    "span_text": "pre-trained model"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1854,
      "end_pos": 1867
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "Pre-training BERT",
    "pr_score": 0.023011866957207434,
    "span_text": "training data"
  },
  {
    "text": "Next Sentence Prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2261,
      "end_pos": 2285
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 12,
    "section": "Pre-training BERT",
    "pr_score": 0.015470645591286416,
    "span_text": "Next Sentence Prediction"
  },
  {
    "text": "Question Answering",
    "type": "task",
    "char_interval": {
      "start_pos": 2332,
      "end_pos": 2350
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "Pre-training BERT",
    "pr_score": 0.029112646817448393,
    "span_text": "Question Answering"
  },
  {
    "text": "Natural Language Inference",
    "type": "task",
    "char_interval": {
      "start_pos": 2360,
      "end_pos": 2386
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "Pre-training BERT",
    "pr_score": 0.02513568025733821,
    "span_text": "Natural Language Inference"
  },
  {
    "text": "monolingual corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2680,
      "end_pos": 2698
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Pre-training BERT",
    "pr_score": 0.0036913750664569808,
    "span_text": "monolingual corpus"
  },
  {
    "text": "pretraining example",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2759,
      "end_pos": 2778
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Pre-training BERT",
    "pr_score": 0.0036913750664569808,
    "span_text": "pretraining example"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2983,
      "end_pos": 3007
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Pre-training BERT",
    "pr_score": 0.015470645591286416,
    "span_text": "next sentence prediction"
  },
  {
    "text": "corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2915,
      "end_pos": 2921
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Pre-training BERT",
    "span_text": "corpus"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2983,
      "end_pos": 3007
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Pre-training BERT",
    "pr_score": 0.015470645591286416,
    "span_text": "next sentence prediction"
  },
  {
    "text": "QA",
    "type": "task",
    "char_interval": {
      "start_pos": 3132,
      "end_pos": 3134
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Pre-training BERT",
    "span_text": "QA"
  },
  {
    "text": "NLI",
    "type": "task",
    "char_interval": {
      "start_pos": 3139,
      "end_pos": 3142
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Pre-training BERT",
    "pr_score": 0.02700169925345402,
    "span_text": "NLI"
  },
  {
    "text": "Position Embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 3144,
      "end_pos": 3163
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Pre-training BERT",
    "span_text": "Position Embeddings"
  },
  {
    "text": "NSP task",
    "type": "task",
    "char_interval": {
      "start_pos": 3168,
      "end_pos": 3176
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Pre-training BERT",
    "span_text": "NSP task"
  },
  {
    "text": "language model pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 70,
      "end_pos": 97
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Pre-training data",
    "span_text": "language model pre-training"
  },
  {
    "text": "Books Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 138,
      "end_pos": 150
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Pre-training data",
    "pr_score": 0.0071942960900806575,
    "span_text": "Books Corpus"
  },
  {
    "text": "English Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 168,
      "end_pos": 185
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Pre-training data",
    "pr_score": 0.004575721576166264,
    "span_text": "English Wikipedia"
  },
  {
    "text": "text passages",
    "type": "other",
    "char_interval": {
      "start_pos": 236,
      "end_pos": 249
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Pre-training data",
    "span_text": "text passages"
  },
  {
    "text": "document-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 313,
      "end_pos": 334
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Pre-training data",
    "pr_score": 0.004575721576166264,
    "span_text": "document-level corpus"
  },
  {
    "text": "shuffled sentence-level corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 349,
      "end_pos": 379
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Pre-training data",
    "pr_score": 0.004575721576166264,
    "span_text": "shuffled sentence-level corpus"
  },
  {
    "text": "Billion Word Benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 392,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Pre-training data",
    "pr_score": 0.004575721576166264,
    "span_text": "Billion Word Benchmark"
  },
  {
    "text": "Fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 0,
      "end_pos": 11
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT",
    "span_text": "Fine-tuning"
  },
  {
    "text": "selfattention mechanism",
    "type": "other",
    "char_interval": {
      "start_pos": 41,
      "end_pos": 64
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT",
    "span_text": "selfattention mechanism"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 72,
      "end_pos": 83
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT",
    "pr_score": 0.027978063578724413,
    "span_text": "Transformer"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 91,
      "end_pos": 95
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "downstream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 110,
      "end_pos": 120
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT",
    "span_text": "downstream tasks"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT",
    "span_text": "text pairs"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT",
    "span_text": "bidirectional cross attention"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 400,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT",
    "span_text": "self-attention"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT",
    "span_text": "bidirectional cross attention"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 494,
      "end_pos": 508
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Fine-tuning BERT",
    "span_text": "self-attention"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT",
    "span_text": "text pairs"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 530,
      "end_pos": 559
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Fine-tuning BERT",
    "span_text": "bidirectional cross attention"
  },
  {
    "text": "taskspecific inputs and outputs",
    "type": "task",
    "char_interval": {
      "start_pos": 620,
      "end_pos": 651
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Fine-tuning BERT",
    "span_text": "taskspecific inputs and outputs"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 657,
      "end_pos": 661
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Fine-tuning BERT",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "sequence tagging",
    "type": "task",
    "char_interval": {
      "start_pos": 970,
      "end_pos": 986
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Fine-tuning BERT",
    "pr_score": 0.0056320046614651235,
    "span_text": "sequence tagging"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 891,
      "end_pos": 909
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Fine-tuning BERT",
    "pr_score": 0.029112646817448393,
    "span_text": "question answering"
  },
  {
    "text": "classification",
    "type": "task",
    "char_interval": {
      "start_pos": 952,
      "end_pos": 966
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT",
    "pr_score": 0.024786285014595864,
    "span_text": "classification"
  },
  {
    "text": "entailment",
    "type": "task",
    "char_interval": {
      "start_pos": 849,
      "end_pos": 859
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Fine-tuning BERT",
    "pr_score": 0.01800916074208592,
    "span_text": "entailment"
  },
  {
    "text": "sentiment analysis",
    "type": "task",
    "char_interval": {
      "start_pos": 1227,
      "end_pos": 1245
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "Fine-tuning BERT",
    "pr_score": 0.010665923930541285,
    "span_text": "sentiment analysis"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 751,
      "end_pos": 763
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Fine-tuning BERT",
    "span_text": "pre-training"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 1273,
      "end_pos": 1284
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Fine-tuning BERT",
    "span_text": "fine-tuning"
  },
  {
    "text": "pre-trained model",
    "type": "method",
    "char_interval": {
      "start_pos": 1457,
      "end_pos": 1474
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Fine-tuning BERT",
    "span_text": "pre-trained model"
  },
  {
    "text": "task-specific details",
    "type": "task",
    "char_interval": {
      "start_pos": 1492,
      "end_pos": 1513
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Fine-tuning BERT",
    "span_text": "task-specific details"
  },
  {
    "text": "Section 4",
    "type": "other",
    "char_interval": {
      "start_pos": 1550,
      "end_pos": 1559
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Fine-tuning BERT",
    "span_text": "Section 4"
  },
  {
    "text": "Appendix A.5",
    "type": "other",
    "char_interval": {
      "start_pos": 1590,
      "end_pos": 1602
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Fine-tuning BERT",
    "span_text": "Appendix A.5"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 28,
      "end_pos": 32
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Experiments",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "NLP tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 59,
      "end_pos": 68
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Experiments",
    "span_text": "NLP tasks"
  },
  {
    "text": "GLUE benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 47,
      "end_pos": 51
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Glue",
    "pr_score": 0.0058224539490399855,
    "span_text": "GLUE benchmark"
  },
  {
    "text": "natural language understanding tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 90,
      "end_pos": 126
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Glue",
    "span_text": "natural language understanding tasks"
  },
  {
    "text": "GLUE datasets",
    "type": "dataset",
    "char_interval": {
      "start_pos": 153,
      "end_pos": 166
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Glue",
    "pr_score": 0.00489739535779576,
    "span_text": "GLUE datasets"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 870,
      "end_pos": 874
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Glue",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 496,
      "end_pos": 516
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Glue",
    "span_text": "classification layer"
  },
  {
    "text": "classification loss",
    "type": "other",
    "char_interval": {
      "start_pos": 591,
      "end_pos": 610
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Glue",
    "span_text": "classification loss"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 727,
      "end_pos": 737
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "section": "Glue",
    "pr_score": 0.007789570442565116,
    "span_text": "GLUE tasks"
  },
  {
    "text": "Dev set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 843,
      "end_pos": 850
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Glue",
    "pr_score": 0.011740179517808718,
    "span_text": "Dev set"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 870,
      "end_pos": 880
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Glue",
    "pr_score": 0.026976967420331822,
    "span_text": "BERT LARGE"
  },
  {
    "text": "fine-tuning data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1109,
      "end_pos": 1125
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "section": "Glue",
    "pr_score": 0.005010431037982722,
    "span_text": "fine-tuning data"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1214,
      "end_pos": 1223
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "Glue",
    "pr_score": 0.024008691478147038,
    "span_text": "BERT BASE"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 1343,
      "end_pos": 1351
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Glue",
    "span_text": "accuracy"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1407,
      "end_pos": 1416
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Glue",
    "pr_score": 0.024008691478147038,
    "span_text": "BERT BASE"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1421,
      "end_pos": 1432
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Glue",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 1466,
      "end_pos": 1484
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Glue",
    "span_text": "model architecture"
  },
  {
    "text": "attention masking",
    "type": "other",
    "char_interval": {
      "start_pos": 1500,
      "end_pos": 1517
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Glue",
    "span_text": "attention masking"
  },
  {
    "text": "GLUE task",
    "type": "task",
    "char_interval": {
      "start_pos": 1560,
      "end_pos": 1569
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Glue",
    "span_text": "GLUE task"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1571,
      "end_pos": 1575
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Glue",
    "pr_score": 0.019994997652237032,
    "span_text": "MNLI"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1577,
      "end_pos": 1581
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 0,
    "section": "Glue",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 1606,
      "end_pos": 1614
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Glue",
    "span_text": "accuracy"
  },
  {
    "text": "GLUE leaderboard",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1644,
      "end_pos": 1660
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Glue",
    "pr_score": 0.0057258340165525605,
    "span_text": "GLUE leaderboard"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1665,
      "end_pos": 1675
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Glue",
    "pr_score": 0.026976967420331822,
    "span_text": "BERT LARGE"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1713,
      "end_pos": 1724
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Glue",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1884,
      "end_pos": 1897
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "Glue",
    "pr_score": 0.023011866957207434,
    "span_text": "training data"
  },
  {
    "text": "model size",
    "type": "other",
    "char_interval": {
      "start_pos": 1913,
      "end_pos": 1923
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "Glue",
    "span_text": "model size"
  },
  {
    "text": "Stanford Question Answering Dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 39
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "SQuAD v1.1",
    "pr_score": 0.012204104221356262,
    "span_text": "Stanford Question Answering Dataset"
  },
  {
    "text": "SQuAD v 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 41,
      "end_pos": 52
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "SQuAD v1.1",
    "pr_score": 0.008808277427975412,
    "span_text": "SQuAD v 1.1"
  },
  {
    "text": "question/answer pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 91,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "SQuAD v1.1",
    "pr_score": 0.0057258340165525605,
    "span_text": "question/answer pairs"
  },
  {
    "text": "GLUE data set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 156,
      "end_pos": 169
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "SQuAD v1.1",
    "pr_score": 0.0057258340165525605,
    "span_text": "GLUE data set"
  },
  {
    "text": "Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 316,
      "end_pos": 325
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "SQuAD v1.1",
    "pr_score": 0.011507643089761847,
    "span_text": "Wikipedia"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 438,
      "end_pos": 456
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "section": "SQuAD v1.1",
    "pr_score": 0.029112646817448393,
    "span_text": "question answering"
  },
  {
    "text": "A embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 564,
      "end_pos": 575
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "SQuAD v1.1",
    "span_text": "A embedding"
  },
  {
    "text": "B embedding",
    "type": "other",
    "char_interval": {
      "start_pos": 602,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "SQuAD v1.1",
    "span_text": "B embedding"
  },
  {
    "text": "start vector",
    "type": "other",
    "char_interval": {
      "start_pos": 635,
      "end_pos": 647
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "SQuAD v1.1",
    "span_text": "start vector"
  },
  {
    "text": "end vector",
    "type": "other",
    "char_interval": {
      "start_pos": 663,
      "end_pos": 673
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "SQuAD v1.1",
    "span_text": "end vector"
  },
  {
    "text": "softmax",
    "type": "other",
    "char_interval": {
      "start_pos": 824,
      "end_pos": 831
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "SQuAD v1.1",
    "span_text": "softmax"
  },
  {
    "text": "start of the answer span",
    "type": "other",
    "char_interval": {
      "start_pos": 738,
      "end_pos": 762
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "SQuAD v1.1",
    "span_text": "start of the answer span"
  },
  {
    "text": "end of the answer span",
    "type": "other",
    "char_interval": {
      "start_pos": 933,
      "end_pos": 955
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 0,
    "section": "SQuAD v1.1",
    "span_text": "end of the answer span"
  },
  {
    "text": "candidate span",
    "type": "other",
    "char_interval": {
      "start_pos": 972,
      "end_pos": 986
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "SQuAD v1.1",
    "span_text": "candidate span"
  },
  {
    "text": "training objective",
    "type": "other",
    "char_interval": {
      "start_pos": 1114,
      "end_pos": 1132
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "section": "SQuAD v1.1",
    "span_text": "training objective"
  },
  {
    "text": "log-likelihoods",
    "type": "other",
    "char_interval": {
      "start_pos": 1151,
      "end_pos": 1166
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "SQuAD v1.1",
    "span_text": "log-likelihoods"
  },
  {
    "text": "correct start and end positions",
    "type": "other",
    "char_interval": {
      "start_pos": 1174,
      "end_pos": 1205
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "SQuAD v1.1",
    "span_text": "correct start and end positions"
  },
  {
    "text": "SQuAD",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1396,
      "end_pos": 1401
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "SQuAD v1.1",
    "pr_score": 0.017685474127195615,
    "span_text": "SQuAD"
  },
  {
    "text": "public data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1502,
      "end_pos": 1513
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "SQuAD v1.1",
    "pr_score": 0.0031621183528232396,
    "span_text": "public data"
  },
  {
    "text": "Trivia QA",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1623,
      "end_pos": 1632
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "SQuAD v1.1",
    "pr_score": 0.0062922717679018934,
    "span_text": "Trivia QA"
  },
  {
    "text": "SQuAD",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1654,
      "end_pos": 1659
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "SQuAD v1.1",
    "pr_score": 0.017685474127195615,
    "span_text": "SQuAD"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1806,
      "end_pos": 1810
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "SQuAD v1.1",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1735,
      "end_pos": 1738
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "SQuAD v1.1",
    "span_text": "F 1"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1762,
      "end_pos": 1765
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "SQuAD v1.1",
    "span_text": "F 1"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1865,
      "end_pos": 1868
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "SQuAD v1.1",
    "span_text": "F 1"
  },
  {
    "text": "tuning data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1875,
      "end_pos": 1886
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "SQuAD v1.1",
    "pr_score": 0.0068351232594227734,
    "span_text": "tuning data"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1908,
      "end_pos": 1911
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "SQuAD v1.1",
    "span_text": "F 1"
  },
  {
    "text": "SQuAD v 2.0",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1978,
      "end_pos": 1989
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "SQuAD v1.1",
    "pr_score": 0.00448526487040586,
    "span_text": "SQuAD v 2.0"
  },
  {
    "text": "SQuAD 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2021,
      "end_pos": 2030
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "SQuAD v1.1",
    "pr_score": 0.00877544108480025,
    "span_text": "SQuAD 1.1"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1806,
      "end_pos": 1810
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "SQuAD v1.1",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "SQuAD v 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2211,
      "end_pos": 2222
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 12,
    "section": "SQuAD v1.1",
    "pr_score": 0.008808277427975412,
    "span_text": "SQuAD v 1.1"
  },
  {
    "text": "Trivia QA",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2604,
      "end_pos": 2613
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 14,
    "section": "SQuAD v1.1",
    "pr_score": 0.0062922717679018934,
    "span_text": "Trivia QA"
  },
  {
    "text": "Trivia QA-Wiki",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2655,
      "end_pos": 2669
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "SQuAD v1.1",
    "pr_score": 0.00448526487040586,
    "span_text": "Trivia QA-Wiki"
  },
  {
    "text": "F1",
    "type": "metric",
    "char_interval": {
      "start_pos": 2918,
      "end_pos": 2921
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "SQuAD v1.1",
    "span_text": "F1"
  },
  {
    "text": "dev set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2898,
      "end_pos": 2905
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "SQuAD v1.1",
    "pr_score": 0.011740179517808718,
    "span_text": "dev set"
  },
  {
    "text": "Trivia QA data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2938,
      "end_pos": 2952
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "SQuAD v1.1",
    "pr_score": 0.00448526487040586,
    "span_text": "Trivia QA data"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 173,
      "end_pos": 176
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "System",
    "span_text": "F 1"
  },
  {
    "text": "SWAG dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 45,
      "end_pos": 49
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Swag",
    "pr_score": 0.0054874288533857795,
    "span_text": "SWAG dataset"
  },
  {
    "text": "grounded commonsense inference",
    "type": "task",
    "char_interval": {
      "start_pos": 121,
      "end_pos": 151
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Swag",
    "pr_score": 0.006310543181393646,
    "span_text": "grounded commonsense inference"
  },
  {
    "text": "sentence-pair completion",
    "type": "task",
    "char_interval": {
      "start_pos": 73,
      "end_pos": 97
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Swag",
    "pr_score": 0.006310543181393646,
    "span_text": "sentence-pair completion"
  },
  {
    "text": "SWAG dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 269,
      "end_pos": 281
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Swag",
    "pr_score": 0.0054874288533857795,
    "span_text": "SWAG dataset"
  },
  {
    "text": "task-specific parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 438,
      "end_pos": 462
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "section": "Swag",
    "span_text": "task-specific parameters"
  },
  {
    "text": "softmax layer",
    "type": "other",
    "char_interval": {
      "start_pos": 601,
      "end_pos": 614
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Swag",
    "span_text": "softmax layer"
  },
  {
    "text": "ESIM+ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 784,
      "end_pos": 793
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Swag",
    "pr_score": 0.006584914624062935,
    "span_text": "ESIM+ELMo"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 739,
      "end_pos": 749
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Swag",
    "pr_score": 0.026976967420331822,
    "span_text": "BERT LARGE"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 815,
      "end_pos": 826
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Swag",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "ablation experiments",
    "type": "other",
    "char_interval": {
      "start_pos": 28,
      "end_pos": 48
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Ablation Studies",
    "span_text": "ablation experiments"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 76,
      "end_pos": 80
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Ablation Studies",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "ablation studies",
    "type": "other",
    "char_interval": {
      "start_pos": 149,
      "end_pos": 165
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Ablation Studies",
    "span_text": "ablation studies"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 62,
      "end_pos": 66
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "pretraining objectives",
    "type": "other",
    "char_interval": {
      "start_pos": 85,
      "end_pos": 107
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks",
    "span_text": "pretraining objectives"
  },
  {
    "text": "pretraining data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 131,
      "end_pos": 147
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks",
    "pr_score": 0.0054874288533857795,
    "span_text": "pretraining data"
  },
  {
    "text": "fine-tuning scheme",
    "type": "other",
    "char_interval": {
      "start_pos": 149,
      "end_pos": 167
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks",
    "span_text": "fine-tuning scheme"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 173,
      "end_pos": 188
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks",
    "span_text": "hyperparameters"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 192,
      "end_pos": 201
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks",
    "pr_score": 0.024008691478147038,
    "span_text": "BERT BASE"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 261,
      "end_pos": 270
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks",
    "pr_score": 0.013551870030823051,
    "span_text": "masked LM"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 295,
      "end_pos": 319
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks",
    "pr_score": 0.015470645591286416,
    "span_text": "next sentence prediction"
  },
  {
    "text": "Left-to-Right (LTR) LM",
    "type": "method",
    "char_interval": {
      "start_pos": 60,
      "end_pos": 82
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "LTR & No NSP:",
    "pr_score": 0.006830975389008748,
    "span_text": "Left-to-Right (LTR) LM"
  },
  {
    "text": "MLM",
    "type": "other",
    "char_interval": {
      "start_pos": 99,
      "end_pos": 102
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "LTR & No NSP:",
    "span_text": "MLM"
  },
  {
    "text": "left-only constraint",
    "type": "other",
    "char_interval": {
      "start_pos": 108,
      "end_pos": 128
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "LTR & No NSP:",
    "span_text": "left-only constraint"
  },
  {
    "text": "pre-train/fine-tune mismatch",
    "type": "other",
    "char_interval": {
      "start_pos": 195,
      "end_pos": 223
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "LTR & No NSP:",
    "span_text": "pre-train/fine-tune mismatch"
  },
  {
    "text": "NSP task",
    "type": "task",
    "char_interval": {
      "start_pos": 315,
      "end_pos": 323
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "LTR & No NSP:",
    "span_text": "NSP task"
  },
  {
    "text": "QNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 588,
      "end_pos": 592
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "LTR & No NSP:",
    "pr_score": 0.012170359103447591,
    "span_text": "QNLI"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 594,
      "end_pos": 598
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "LTR & No NSP:",
    "pr_score": 0.019994997652237032,
    "span_text": "MNLI"
  },
  {
    "text": "SQuAD 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 604,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "LTR & No NSP:",
    "pr_score": 0.00877544108480025,
    "span_text": "SQuAD 1.1"
  },
  {
    "text": "bidirectional representations",
    "type": "other",
    "char_interval": {
      "start_pos": 656,
      "end_pos": 685
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "LTR & No NSP:",
    "span_text": "bidirectional representations"
  },
  {
    "text": "LTR model",
    "type": "method",
    "char_interval": {
      "start_pos": 731,
      "end_pos": 740
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "section": "LTR & No NSP:",
    "span_text": "LTR model"
  },
  {
    "text": "MLM model",
    "type": "method",
    "char_interval": {
      "start_pos": 765,
      "end_pos": 774
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "LTR & No NSP:",
    "span_text": "MLM model"
  },
  {
    "text": "MRPC",
    "type": "dataset",
    "char_interval": {
      "start_pos": 809,
      "end_pos": 813
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "LTR & No NSP:",
    "pr_score": 0.014949916845831397,
    "span_text": "MRPC"
  },
  {
    "text": "SQuAD",
    "type": "dataset",
    "char_interval": {
      "start_pos": 818,
      "end_pos": 823
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "LTR & No NSP:",
    "pr_score": 0.017685474127195615,
    "span_text": "SQuAD"
  },
  {
    "text": "token predictions",
    "type": "task",
    "char_interval": {
      "start_pos": 899,
      "end_pos": 916
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "LTR & No NSP:",
    "pr_score": 0.006546351414466717,
    "span_text": "token predictions"
  },
  {
    "text": "rightside context",
    "type": "other",
    "char_interval": {
      "start_pos": 962,
      "end_pos": 979
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "LTR & No NSP:",
    "span_text": "rightside context"
  },
  {
    "text": "BiLSTM",
    "type": "method",
    "char_interval": {
      "start_pos": 1084,
      "end_pos": 1090
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "LTR & No NSP:",
    "pr_score": 0.008007924658242493,
    "span_text": "BiLSTM"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1272,
      "end_pos": 1282
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "LTR & No NSP:",
    "pr_score": 0.007789570442565116,
    "span_text": "GLUE tasks"
  },
  {
    "text": "LTR",
    "type": "method",
    "char_interval": {
      "start_pos": 1346,
      "end_pos": 1349
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "LTR & No NSP:",
    "pr_score": 0.010160504981582606,
    "span_text": "LTR"
  },
  {
    "text": "RTL models",
    "type": "method",
    "char_interval": {
      "start_pos": 1354,
      "end_pos": 1364
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "LTR & No NSP:",
    "span_text": "RTL models"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 1433,
      "end_pos": 1437
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "LTR & No NSP:",
    "pr_score": 0.02263064499680652,
    "span_text": "ELMo"
  },
  {
    "text": "bidirectional model",
    "type": "method",
    "char_interval": {
      "start_pos": 1496,
      "end_pos": 1515
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "LTR & No NSP:",
    "span_text": "bidirectional model"
  },
  {
    "text": "QA",
    "type": "task",
    "char_interval": {
      "start_pos": 1558,
      "end_pos": 1560
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "LTR & No NSP:",
    "span_text": "QA"
  },
  {
    "text": "RTL model",
    "type": "method",
    "char_interval": {
      "start_pos": 1572,
      "end_pos": 1581
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 0,
    "section": "LTR & No NSP:",
    "span_text": "RTL model"
  },
  {
    "text": "deep bidirectional model",
    "type": "method",
    "char_interval": {
      "start_pos": 1686,
      "end_pos": 1710
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "LTR & No NSP:",
    "span_text": "deep bidirectional model"
  },
  {
    "text": "fine-tuning task",
    "type": "task",
    "char_interval": {
      "start_pos": 56,
      "end_pos": 72
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Effect of Model Size",
    "span_text": "fine-tuning task"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 73,
      "end_pos": 81
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Effect of Model Size",
    "span_text": "accuracy"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 106,
      "end_pos": 110
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Effect of Model Size",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 219,
      "end_pos": 234
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Model Size",
    "span_text": "hyperparameters"
  },
  {
    "text": "training procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 239,
      "end_pos": 257
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Model Size",
    "span_text": "training procedure"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 303,
      "end_pos": 313
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Model Size",
    "pr_score": 0.007789570442565116,
    "span_text": "GLUE tasks"
  },
  {
    "text": "Dev Set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 373,
      "end_pos": 380
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Model Size",
    "pr_score": 0.011740179517808718,
    "span_text": "Dev Set"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 381,
      "end_pos": 389
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Effect of Model Size",
    "span_text": "accuracy"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 476,
      "end_pos": 484
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Effect of Model Size",
    "span_text": "accuracy"
  },
  {
    "text": "MRPC",
    "type": "dataset",
    "char_interval": {
      "start_pos": 532,
      "end_pos": 536
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Effect of Model Size",
    "pr_score": 0.014949916845831397,
    "span_text": "MRPC"
  },
  {
    "text": "labeled training examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 558,
      "end_pos": 583
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Effect of Model Size",
    "pr_score": 0.006870256839936939,
    "span_text": "labeled training examples"
  },
  {
    "text": "pre-training tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 625,
      "end_pos": 643
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Effect of Model Size",
    "span_text": "pre-training tasks"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 842,
      "end_pos": 853
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Effect of Model Size",
    "pr_score": 0.027978063578724413,
    "span_text": "Transformer"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 894,
      "end_pos": 904
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Effect of Model Size",
    "span_text": "parameters"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1014,
      "end_pos": 1024
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Effect of Model Size",
    "span_text": "parameters"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1039,
      "end_pos": 1048
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Effect of Model Size",
    "pr_score": 0.024008691478147038,
    "span_text": "BERT BASE"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1063,
      "end_pos": 1073
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Effect of Model Size",
    "span_text": "parameters"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1078,
      "end_pos": 1088
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Effect of Model Size",
    "pr_score": 0.026976967420331822,
    "span_text": "BERT LARGE"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1103,
      "end_pos": 1113
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Effect of Model Size",
    "span_text": "parameters"
  },
  {
    "text": "machine translation",
    "type": "task",
    "char_interval": {
      "start_pos": 1234,
      "end_pos": 1253
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 0,
    "section": "Effect of Model Size",
    "pr_score": 0.009476468202587657,
    "span_text": "machine translation"
  },
  {
    "text": "language modeling",
    "type": "task",
    "char_interval": {
      "start_pos": 1258,
      "end_pos": 1275
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Effect of Model Size",
    "span_text": "language modeling"
  },
  {
    "text": "LM perplexity",
    "type": "metric",
    "char_interval": {
      "start_pos": 1306,
      "end_pos": 1319
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Effect of Model Size",
    "pr_score": 0.0040316126980110304,
    "span_text": "LM perplexity"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1332,
      "end_pos": 1345
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Effect of Model Size",
    "pr_score": 0.023011866957207434,
    "span_text": "training data"
  },
  {
    "text": "model sizes",
    "type": "other",
    "char_interval": {
      "start_pos": 1464,
      "end_pos": 1475
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Effect of Model Size",
    "span_text": "model sizes"
  },
  {
    "text": "pre-trained",
    "type": "other",
    "char_interval": {
      "start_pos": 1582,
      "end_pos": 1593
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Effect of Model Size",
    "span_text": "pre-trained"
  },
  {
    "text": "downstream task",
    "type": "other",
    "char_interval": {
      "start_pos": 1627,
      "end_pos": 1642
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Effect of Model Size",
    "span_text": "downstream task"
  },
  {
    "text": "pre-trained bi-LM",
    "type": "other",
    "char_interval": {
      "start_pos": 1668,
      "end_pos": 1685
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Effect of Model Size",
    "span_text": "pre-trained bi-LM"
  },
  {
    "text": "featurebased approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1918,
      "end_pos": 1939
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "Effect of Model Size",
    "span_text": "featurebased approach"
  },
  {
    "text": "downstream tasks",
    "type": "other",
    "char_interval": {
      "start_pos": 2006,
      "end_pos": 2022
    },
    "attributes": {},
    "sentence_context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task.",
    "sentence_index": 11,
    "section": "Effect of Model Size",
    "span_text": "downstream tasks"
  },
  {
    "text": "pre-trained representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2173,
      "end_pos": 2200
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 12,
    "section": "Effect of Model Size",
    "span_text": "pre-trained representations"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 11,
      "end_pos": 15
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "fine-tuning approach",
    "type": "other",
    "char_interval": {
      "start_pos": 55,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Feature-based Approach with BERT",
    "span_text": "fine-tuning approach"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 92,
      "end_pos": 112
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Feature-based Approach with BERT",
    "span_text": "classification layer"
  },
  {
    "text": "pre-trained model",
    "type": "other",
    "char_interval": {
      "start_pos": 129,
      "end_pos": 146
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Feature-based Approach with BERT",
    "span_text": "pre-trained model"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 225,
      "end_pos": 247
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Feature-based Approach with BERT",
    "span_text": "feature-based approach"
  },
  {
    "text": "Transformer encoder architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 387,
      "end_pos": 419
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Feature-based Approach with BERT",
    "span_text": "Transformer encoder architecture"
  },
  {
    "text": "task-specific model architecture",
    "type": "other",
    "char_interval": {
      "start_pos": 445,
      "end_pos": 477
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "section": "Feature-based Approach with BERT",
    "span_text": "task-specific model architecture"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 588,
      "end_pos": 601
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.023011866957207434,
    "span_text": "training data"
  },
  {
    "text": "Named Entity Recognition",
    "type": "task",
    "char_interval": {
      "start_pos": 769,
      "end_pos": 793
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.012950096442141474,
    "span_text": "Named Entity Recognition"
  },
  {
    "text": "CoNL-2003",
    "type": "dataset",
    "char_interval": {
      "start_pos": 759,
      "end_pos": 768
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.0038741520205472697,
    "span_text": "CoNL-2003"
  },
  {
    "text": "NER",
    "type": "method",
    "char_interval": {
      "start_pos": 795,
      "end_pos": 798
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.03575852680580005,
    "span_text": "NER"
  },
  {
    "text": "Word Piece model",
    "type": "other",
    "char_interval": {
      "start_pos": 853,
      "end_pos": 869
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Feature-based Approach with BERT",
    "span_text": "Word Piece model"
  },
  {
    "text": "data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 931,
      "end_pos": 935
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Feature-based Approach with BERT",
    "span_text": "data"
  },
  {
    "text": "tagging task",
    "type": "task",
    "char_interval": {
      "start_pos": 989,
      "end_pos": 1001
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Feature-based Approach with BERT",
    "span_text": "tagging task"
  },
  {
    "text": "CRF layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1019,
      "end_pos": 1028
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Feature-based Approach with BERT",
    "span_text": "CRF layer"
  },
  {
    "text": "token-level classifier",
    "type": "other",
    "char_interval": {
      "start_pos": 1113,
      "end_pos": 1135
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "section": "Feature-based Approach with BERT",
    "span_text": "token-level classifier"
  },
  {
    "text": "NER label set",
    "type": "other",
    "char_interval": {
      "start_pos": 1145,
      "end_pos": 1158
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "Feature-based Approach with BERT",
    "span_text": "NER label set"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1209,
      "end_pos": 1231
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "Feature-based Approach with BERT",
    "span_text": "feature-based approach"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1324,
      "end_pos": 1328
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "contextual embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1336,
      "end_pos": 1357
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Feature-based Approach with BERT",
    "span_text": "contextual embeddings"
  },
  {
    "text": "BiLSTM",
    "type": "method",
    "char_interval": {
      "start_pos": 1428,
      "end_pos": 1434
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.008007924658242493,
    "span_text": "BiLSTM"
  },
  {
    "text": "classification layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1446,
      "end_pos": 1466
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Feature-based Approach with BERT",
    "span_text": "classification layer"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1502,
      "end_pos": 1512
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.026976967420331822,
    "span_text": "BERT LARGE"
  },
  {
    "text": "state-of-the-art methods",
    "type": "other",
    "char_interval": {
      "start_pos": 1541,
      "end_pos": 1565
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Feature-based Approach with BERT",
    "span_text": "state-of-the-art methods"
  },
  {
    "text": "token representations",
    "type": "other",
    "char_interval": {
      "start_pos": 1611,
      "end_pos": 1632
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Feature-based Approach with BERT",
    "span_text": "token representations"
  },
  {
    "text": "Transformer",
    "type": "other",
    "char_interval": {
      "start_pos": 1684,
      "end_pos": 1695
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Feature-based Approach with BERT",
    "span_text": "Transformer"
  },
  {
    "text": "F 1",
    "type": "metric",
    "char_interval": {
      "start_pos": 1715,
      "end_pos": 1718
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Feature-based Approach with BERT",
    "span_text": "F 1"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 1726,
      "end_pos": 1737
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Feature-based Approach with BERT",
    "span_text": "fine-tuning"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1779,
      "end_pos": 1783
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Feature-based Approach with BERT",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "transfer learning",
    "type": "other",
    "char_interval": {
      "start_pos": 37,
      "end_pos": 54
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Conclusion",
    "span_text": "transfer learning"
  },
  {
    "text": "language models",
    "type": "other",
    "char_interval": {
      "start_pos": 60,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Conclusion",
    "span_text": "language models"
  },
  {
    "text": "unsupervised pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 105,
      "end_pos": 130
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Conclusion",
    "span_text": "unsupervised pre-training"
  },
  {
    "text": "language understanding",
    "type": "task",
    "char_interval": {
      "start_pos": 159,
      "end_pos": 181
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Conclusion",
    "pr_score": 0.007625173821309537,
    "span_text": "language understanding"
  },
  {
    "text": "deep unidirectional architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 267,
      "end_pos": 300
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Conclusion",
    "span_text": "deep unidirectional architectures"
  },
  {
    "text": "deep bidirectional architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 367,
      "end_pos": 399
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Conclusion",
    "span_text": "deep bidirectional architectures"
  },
  {
    "text": "NLP tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 475,
      "end_pos": 484
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Conclusion",
    "span_text": "NLP tasks"
  },
  {
    "text": "Masked LM",
    "type": "method",
    "char_interval": {
      "start_pos": 486,
      "end_pos": 495
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Conclusion",
    "pr_score": 0.014141081771293619,
    "span_text": "Masked LM"
  },
  {
    "text": "Masking Procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Conclusion",
    "span_text": "Masking Procedure"
  },
  {
    "text": "unlabeled sentence",
    "type": "dataset",
    "char_interval": {
      "start_pos": 535,
      "end_pos": 553
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Conclusion",
    "pr_score": 0.003081741686216042,
    "span_text": "unlabeled sentence"
  },
  {
    "text": "random masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 589,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Conclusion",
    "span_text": "random masking procedure"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 674,
      "end_pos": 691
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Conclusion",
    "span_text": "masking procedure"
  },
  {
    "text": "MASK token",
    "type": "other",
    "char_interval": {
      "start_pos": 766,
      "end_pos": 770
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Conclusion",
    "span_text": "MASK token"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Conclusion",
    "span_text": "masking procedure"
  },
  {
    "text": "random word",
    "type": "other",
    "char_interval": {
      "start_pos": 844,
      "end_pos": 855
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Conclusion",
    "span_text": "random word"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Conclusion",
    "span_text": "masking procedure"
  },
  {
    "text": "representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1015,
      "end_pos": 1029
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "Conclusion",
    "span_text": "representation"
  },
  {
    "text": "Transformer encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 1108,
      "end_pos": 1127
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "section": "Conclusion",
    "span_text": "Transformer encoder"
  },
  {
    "text": "random words",
    "type": "other",
    "char_interval": {
      "start_pos": 1213,
      "end_pos": 1225
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "Conclusion",
    "span_text": "random words"
  },
  {
    "text": "contextual representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1268,
      "end_pos": 1293
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Conclusion",
    "span_text": "contextual representation"
  },
  {
    "text": "input token",
    "type": "other",
    "char_interval": {
      "start_pos": 1303,
      "end_pos": 1314
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Conclusion",
    "span_text": "input token"
  },
  {
    "text": "random replacement",
    "type": "other",
    "char_interval": {
      "start_pos": 1338,
      "end_pos": 1356
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "Conclusion",
    "span_text": "random replacement"
  },
  {
    "text": "language understanding capability",
    "type": "other",
    "char_interval": {
      "start_pos": 1451,
      "end_pos": 1484
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "Conclusion",
    "span_text": "language understanding capability"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 1591,
      "end_pos": 1600
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "Conclusion",
    "pr_score": 0.013551870030823051,
    "span_text": "masked LM"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2154,
      "end_pos": 2178
    },
    "attributes": {},
    "sentence_context": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.",
    "sentence_index": 12,
    "section": "Conclusion",
    "pr_score": 0.015470645591286416,
    "span_text": "next sentence prediction"
  },
  {
    "text": "corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2573,
      "end_pos": 2579
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 14,
    "section": "Conclusion",
    "span_text": "corpus"
  },
  {
    "text": "Adam",
    "type": "method",
    "char_interval": {
      "start_pos": 2588,
      "end_pos": 2592
    },
    "attributes": {},
    "sentence_context": "In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.",
    "sentence_index": 14,
    "section": "Conclusion",
    "pr_score": 0.004743567792463866,
    "span_text": "Adam"
  },
  {
    "text": "training loss",
    "type": "metric",
    "char_interval": {
      "start_pos": 2894,
      "end_pos": 2907
    },
    "attributes": {},
    "sentence_context": "The contributions of our paper are as follows: We demonstrate the importance of bidirectional pre-training for language representations.., which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.",
    "sentence_index": 15,
    "section": "Conclusion",
    "pr_score": 0.004150621818405883,
    "span_text": "training loss"
  },
  {
    "text": "masked LM likelihood",
    "type": "other",
    "char_interval": {
      "start_pos": 2931,
      "end_pos": 2951
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Conclusion",
    "span_text": "masked LM likelihood"
  },
  {
    "text": "next sentence prediction likelihood",
    "type": "other",
    "char_interval": {
      "start_pos": 2965,
      "end_pos": 3000
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Conclusion",
    "span_text": "next sentence prediction likelihood"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 3014,
      "end_pos": 3023
    },
    "attributes": {},
    "sentence_context": "This is also in contrast to Peters et al., which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.",
    "sentence_index": 16,
    "section": "Conclusion",
    "pr_score": 0.024008691478147038,
    "span_text": "BERT BASE"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 3111,
      "end_pos": 3121
    },
    "attributes": {},
    "sentence_context": "We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.",
    "sentence_index": 17,
    "section": "Conclusion",
    "pr_score": 0.026976967420331822,
    "span_text": "BERT LARGE"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 28,
      "end_pos": 43
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "hyperparameters"
  },
  {
    "text": "batch size",
    "type": "other",
    "char_interval": {
      "start_pos": 103,
      "end_pos": 113
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "batch size"
  },
  {
    "text": "learning rate",
    "type": "other",
    "char_interval": {
      "start_pos": 115,
      "end_pos": 128
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "learning rate"
  },
  {
    "text": "number of training epochs",
    "type": "other",
    "char_interval": {
      "start_pos": 134,
      "end_pos": 159
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "number of training epochs"
  },
  {
    "text": "dropout probability",
    "type": "other",
    "char_interval": {
      "start_pos": 165,
      "end_pos": 184
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "dropout probability"
  },
  {
    "text": "hyperparameter",
    "type": "other",
    "char_interval": {
      "start_pos": 221,
      "end_pos": 235
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "hyperparameter"
  },
  {
    "text": "large data sets",
    "type": "dataset",
    "char_interval": {
      "start_pos": 386,
      "end_pos": 401
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.004070024775409032,
    "span_text": "large data sets"
  },
  {
    "text": "labeled training examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 415,
      "end_pos": 440
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.006870256839936939,
    "span_text": "labeled training examples"
  },
  {
    "text": "small data sets",
    "type": "dataset",
    "char_interval": {
      "start_pos": 496,
      "end_pos": 511
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.004070024775409032,
    "span_text": "small data sets"
  },
  {
    "text": "development set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 677,
      "end_pos": 692
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.006688599289323428,
    "span_text": "development set"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 712,
      "end_pos": 716
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 718,
      "end_pos": 722
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.02263064499680652,
    "span_text": "ELMo"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 727,
      "end_pos": 738
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "model architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 886,
      "end_pos": 905
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "model architectures"
  },
  {
    "text": "architecture differences",
    "type": "other",
    "char_interval": {
      "start_pos": 967,
      "end_pos": 991
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "architecture differences"
  },
  {
    "text": "finetuning approaches",
    "type": "other",
    "char_interval": {
      "start_pos": 1018,
      "end_pos": 1039
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "finetuning approaches"
  },
  {
    "text": "feature-based approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1057,
      "end_pos": 1079
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 5,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "feature-based approach"
  },
  {
    "text": "pre-training method",
    "type": "other",
    "char_interval": {
      "start_pos": 1110,
      "end_pos": 1129
    },
    "attributes": {},
    "sentence_context": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.",
    "sentence_index": 0,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "pre-training method"
  },
  {
    "text": "Open AI GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1141,
      "end_pos": 1152
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.04647840132463596,
    "span_text": "Open AI GPT"
  },
  {
    "text": "Transformer LM",
    "type": "other",
    "char_interval": {
      "start_pos": 1183,
      "end_pos": 1197
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "Transformer LM"
  },
  {
    "text": "large text corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1203,
      "end_pos": 1220
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.004016442764658139,
    "span_text": "large text corpus"
  },
  {
    "text": "GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1315,
      "end_pos": 1318
    },
    "attributes": {},
    "sentence_context": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
    "sentence_index": 7,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.04647840132463596,
    "span_text": "GPT"
  },
  {
    "text": "bi-directionality",
    "type": "other",
    "char_interval": {
      "start_pos": 1427,
      "end_pos": 1444
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "bi-directionality"
  },
  {
    "text": "pretraining tasks",
    "type": "other",
    "char_interval": {
      "start_pos": 1457,
      "end_pos": 1474
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "pretraining tasks"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1624,
      "end_pos": 1628
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "GPT",
    "type": "method",
    "char_interval": {
      "start_pos": 1633,
      "end_pos": 1636
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.04647840132463596,
    "span_text": "GPT"
  },
  {
    "text": "Books Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1673,
      "end_pos": 1685
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.0071942960900806575,
    "span_text": "Books Corpus"
  },
  {
    "text": "Wikipedia",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1753,
      "end_pos": 1762
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "A.3 Fine-tuning Procedure",
    "pr_score": 0.011507643089761847,
    "span_text": "Wikipedia"
  },
  {
    "text": "sentence separator",
    "type": "other",
    "char_interval": {
      "start_pos": 1790,
      "end_pos": 1808
    },
    "attributes": {},
    "sentence_context": "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.",
    "sentence_index": 9,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "sentence separator"
  },
  {
    "text": "classifier token",
    "type": "other",
    "char_interval": {
      "start_pos": 1821,
      "end_pos": 1837
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "classifier token"
  },
  {
    "text": "sentence A/B embeddings",
    "type": "other",
    "char_interval": {
      "start_pos": 1922,
      "end_pos": 1945
    },
    "attributes": {},
    "sentence_context": "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.",
    "sentence_index": 10,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "sentence A/B embeddings"
  },
  {
    "text": "bidirectionality",
    "type": "other",
    "char_interval": {
      "start_pos": 2479,
      "end_pos": 2495
    },
    "attributes": {},
    "sentence_context": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.",
    "sentence_index": 13,
    "section": "A.3 Fine-tuning Procedure",
    "span_text": "bidirectionality"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 32,
      "end_pos": 36
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "entailment classification",
    "type": "task",
    "char_interval": {
      "start_pos": 315,
      "end_pos": 340
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.009140275900270431,
    "span_text": "entailment classification"
  },
  {
    "text": "MNLI Multi-Genre Natural Language Inference",
    "type": "dataset",
    "char_interval": {
      "start_pos": 240,
      "end_pos": 283
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.007948066000235158,
    "span_text": "MNLI Multi-Genre Natural Language Inference"
  },
  {
    "text": "binary classification",
    "type": "task",
    "char_interval": {
      "start_pos": 529,
      "end_pos": 550
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.009140275900270431,
    "span_text": "binary classification"
  },
  {
    "text": "QP Quora Question Pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 500,
      "end_pos": 523
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.007948066000235158,
    "span_text": "QP Quora Question Pairs"
  },
  {
    "text": "binary classification",
    "type": "task",
    "char_interval": {
      "start_pos": 529,
      "end_pos": 550
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.009140275900270431,
    "span_text": "binary classification"
  },
  {
    "text": "QNLI Question Natural Language Inference",
    "type": "dataset",
    "char_interval": {
      "start_pos": 648,
      "end_pos": 688
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.007948066000235158,
    "span_text": "QNLI Question Natural Language Inference"
  },
  {
    "text": "Stanford Question Answering Dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 709,
      "end_pos": 744
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 0,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.012204104221356262,
    "span_text": "Stanford Question Answering Dataset"
  },
  {
    "text": "positive examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 807,
      "end_pos": 824
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.007948066000235158,
    "span_text": "positive examples"
  },
  {
    "text": "negative examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 901,
      "end_pos": 918
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "A.5 Illustrations of Fine-tuning on Different Tasks",
    "pr_score": 0.007948066000235158,
    "span_text": "negative examples"
  },
  {
    "text": "Stanford Sentiment Treebank",
    "type": "dataset",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 31
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Sst-2",
    "pr_score": 0.007948066000235158,
    "span_text": "Stanford Sentiment Treebank"
  },
  {
    "text": "binary single-sentence classification",
    "type": "task",
    "char_interval": {
      "start_pos": 37,
      "end_pos": 74
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Sst-2",
    "pr_score": 0.009140275900270431,
    "span_text": "binary single-sentence classification"
  },
  {
    "text": "CoLA The Corpus of Linguistic Acceptability",
    "type": "dataset",
    "char_interval": {
      "start_pos": 176,
      "end_pos": 219
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Sst-2",
    "pr_score": 0.007948066000235158,
    "span_text": "CoLA The Corpus of Linguistic Acceptability"
  },
  {
    "text": "binary single-sentence classification",
    "type": "task",
    "char_interval": {
      "start_pos": 225,
      "end_pos": 262
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Sst-2",
    "pr_score": 0.009140275900270431,
    "span_text": "binary single-sentence classification"
  },
  {
    "text": "Semantic Textual Similarity Benchmark",
    "type": "dataset",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 41
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Sts-b",
    "pr_score": 0.007948066000235158,
    "span_text": "Semantic Textual Similarity Benchmark"
  },
  {
    "text": "sentence pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 61,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "Sts-b",
    "pr_score": 0.011985618503695299,
    "span_text": "sentence pairs"
  },
  {
    "text": "MRPC Microsoft Research Paraphrase Corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 239,
      "end_pos": 280
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Sts-b",
    "pr_score": 0.007948066000235158,
    "span_text": "MRPC Microsoft Research Paraphrase Corpus"
  },
  {
    "text": "sentence pairs",
    "type": "dataset",
    "char_interval": {
      "start_pos": 293,
      "end_pos": 307
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "Sts-b",
    "pr_score": 0.011985618503695299,
    "span_text": "sentence pairs"
  },
  {
    "text": "RTE Recognizing Textual Entailment",
    "type": "dataset",
    "char_interval": {
      "start_pos": 448,
      "end_pos": 482
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 0,
    "section": "Sts-b",
    "pr_score": 0.005727663335896112,
    "span_text": "RTE Recognizing Textual Entailment"
  },
  {
    "text": "binary entailment task",
    "type": "task",
    "char_interval": {
      "start_pos": 488,
      "end_pos": 510
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Sts-b",
    "span_text": "binary entailment task"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 522,
      "end_pos": 526
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Sts-b",
    "pr_score": 0.019994997652237032,
    "span_text": "MNLI"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 547,
      "end_pos": 560
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Sts-b",
    "pr_score": 0.023011866957207434,
    "span_text": "training data"
  },
  {
    "text": "WNLI Winograd NLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 562,
      "end_pos": 579
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "Sts-b",
    "pr_score": 0.00038461538461538467,
    "span_text": "WNLI Winograd NLI"
  },
  {
    "text": "natural language inference dataset",
    "type": "dataset",
    "char_interval": {
      "start_pos": 591,
      "end_pos": 625
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Sts-b",
    "pr_score": 0.005727663335896112,
    "span_text": "natural language inference dataset"
  },
  {
    "text": "GLUE webpage",
    "type": "dataset",
    "char_interval": {
      "start_pos": 631,
      "end_pos": 643
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "Sts-b",
    "pr_score": 0.005727663335896112,
    "span_text": "GLUE webpage"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 812,
      "end_pos": 820
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Sts-b",
    "span_text": "accuracy"
  },
  {
    "text": "this set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 876,
      "end_pos": 884
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Sts-b",
    "span_text": "this set"
  },
  {
    "text": "GLUE submission",
    "type": "dataset",
    "char_interval": {
      "start_pos": 920,
      "end_pos": 935
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "Sts-b",
    "pr_score": 0.005727663335896112,
    "span_text": "GLUE submission"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 32,
      "end_pos": 36
    },
    "attributes": {},
    "sentence_context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks.",
    "sentence_index": 0,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "masked language model",
    "type": "other",
    "char_interval": {
      "start_pos": 116,
      "end_pos": 137
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "masked language model"
  },
  {
    "text": "MLM",
    "type": "other",
    "char_interval": {
      "start_pos": 139,
      "end_pos": 142
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "MLM"
  },
  {
    "text": "ablation study",
    "type": "other",
    "char_interval": {
      "start_pos": 175,
      "end_pos": 189
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "ablation study"
  },
  {
    "text": "masking strategies",
    "type": "other",
    "char_interval": {
      "start_pos": 226,
      "end_pos": 244
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "masking strategies"
  },
  {
    "text": "fine-tuning",
    "type": "task",
    "char_interval": {
      "start_pos": 345,
      "end_pos": 356
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.05977418724387893,
    "span_text": "fine-tuning"
  },
  {
    "text": "NER",
    "type": "task",
    "char_interval": {
      "start_pos": 467,
      "end_pos": 470
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.03426858818889172,
    "span_text": "NER"
  },
  {
    "text": "feature-based approach",
    "type": "task",
    "char_interval": {
      "start_pos": 590,
      "end_pos": 612
    },
    "attributes": {},
    "sentence_context": "The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.",
    "sentence_index": 3,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "feature-based approach"
  },
  {
    "text": "Dev results",
    "type": "dataset",
    "char_interval": {
      "start_pos": 437,
      "end_pos": 448
    },
    "attributes": {},
    "sentence_context": "These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.",
    "sentence_index": 1,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.005727663335896112,
    "span_text": "Dev results"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 458,
      "end_pos": 462
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.019994997652237032,
    "span_text": "MNLI"
  },
  {
    "text": "NER",
    "type": "dataset",
    "char_interval": {
      "start_pos": 467,
      "end_pos": 470
    },
    "attributes": {},
    "sentence_context": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.",
    "sentence_index": 2,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.02979877233816671,
    "span_text": "NER"
  },
  {
    "text": "MASK",
    "type": "other",
    "char_interval": {
      "start_pos": 734,
      "end_pos": 738
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "MASK"
  },
  {
    "text": "SAME",
    "type": "other",
    "char_interval": {
      "start_pos": 810,
      "end_pos": 814
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "SAME"
  },
  {
    "text": "RND",
    "type": "other",
    "char_interval": {
      "start_pos": 858,
      "end_pos": 861
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "RND"
  },
  {
    "text": "MLM",
    "type": "other",
    "char_interval": {
      "start_pos": 805,
      "end_pos": 808
    },
    "attributes": {},
    "sentence_context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (Open AI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.",
    "sentence_index": 4,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "MLM"
  },
  {
    "text": "Dev set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1124,
      "end_pos": 1131
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.011740179517808718,
    "span_text": "Dev set"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 1209,
      "end_pos": 1213
    },
    "attributes": {},
    "sentence_context": "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.",
    "sentence_index": 6,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.06770711857503955,
    "span_text": "BERT"
  },
  {
    "text": "MASK strategy",
    "type": "other",
    "char_interval": {
      "start_pos": 1427,
      "end_pos": 1440
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "MASK strategy"
  },
  {
    "text": "NER",
    "type": "task",
    "char_interval": {
      "start_pos": 1500,
      "end_pos": 1503
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "C.2 Ablation for Different Masking Procedures",
    "pr_score": 0.03426858818889172,
    "span_text": "NER"
  },
  {
    "text": "RND strategy",
    "type": "other",
    "char_interval": {
      "start_pos": 1535,
      "end_pos": 1547
    },
    "attributes": {},
    "sentence_context": "For example, in Open AI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.",
    "sentence_index": 8,
    "section": "C.2 Ablation for Different Masking Procedures",
    "span_text": "RND strategy"
  }
]