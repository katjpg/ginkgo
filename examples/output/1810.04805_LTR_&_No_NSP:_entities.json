[
  {
    "text": "Left-to-Right (LTR) LM",
    "type": "method",
    "char_interval": {
      "start_pos": 60,
      "end_pos": 82
    },
    "attributes": {},
    "sentence_context": "A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM.",
    "sentence_index": 0,
    "section": "LTR & No NSP:"
  },
  {
    "text": "MLM",
    "type": "other",
    "char_interval": {
      "start_pos": 99,
      "end_pos": 102
    },
    "attributes": {},
    "sentence_context": "A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM.",
    "sentence_index": 0,
    "section": "LTR & No NSP:"
  },
  {
    "text": "left-only constraint",
    "type": "other",
    "char_interval": {
      "start_pos": 108,
      "end_pos": 128
    },
    "attributes": {},
    "sentence_context": "The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance.",
    "sentence_index": 1,
    "section": "LTR & No NSP:"
  },
  {
    "text": "pre-train/fine-tune mismatch",
    "type": "other",
    "char_interval": {
      "start_pos": 195,
      "end_pos": 223
    },
    "attributes": {},
    "sentence_context": "The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance.",
    "sentence_index": 1,
    "section": "LTR & No NSP:"
  },
  {
    "text": "NSP task",
    "type": "task",
    "char_interval": {
      "start_pos": 315,
      "end_pos": 323
    },
    "attributes": {},
    "sentence_context": "Additionally, this model was pre-trained without the NSP task.",
    "sentence_index": 2,
    "section": "LTR & No NSP:"
  },
  {
    "text": "QNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 588,
      "end_pos": 592
    },
    "attributes": {},
    "sentence_context": "In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1.",
    "sentence_index": 5,
    "section": "LTR & No NSP:"
  },
  {
    "text": "MNLI",
    "type": "dataset",
    "char_interval": {
      "start_pos": 594,
      "end_pos": 598
    },
    "attributes": {},
    "sentence_context": "In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1.",
    "sentence_index": 5,
    "section": "LTR & No NSP:"
  },
  {
    "text": "SQuAD 1.1",
    "type": "dataset",
    "char_interval": {
      "start_pos": 604,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1.",
    "sentence_index": 5,
    "section": "LTR & No NSP:"
  },
  {
    "text": "bidirectional representations",
    "type": "other",
    "char_interval": {
      "start_pos": 656,
      "end_pos": 685
    },
    "attributes": {},
    "sentence_context": "Next, we evaluate the impact of training bidirectional representations by comparing \"No NSP\" to \"LTR & No NSP\".",
    "sentence_index": 6,
    "section": "LTR & No NSP:"
  },
  {
    "text": "LTR model",
    "type": "method",
    "char_interval": {
      "start_pos": 731,
      "end_pos": 740
    },
    "attributes": {},
    "sentence_context": "The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.",
    "sentence_index": 7,
    "section": "LTR & No NSP:"
  },
  {
    "text": "MLM model",
    "type": "method",
    "char_interval": {
      "start_pos": 765,
      "end_pos": 774
    },
    "attributes": {},
    "sentence_context": "The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.",
    "sentence_index": 7,
    "section": "LTR & No NSP:"
  },
  {
    "text": "MRPC",
    "type": "dataset",
    "char_interval": {
      "start_pos": 809,
      "end_pos": 813
    },
    "attributes": {},
    "sentence_context": "The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.",
    "sentence_index": 7,
    "section": "LTR & No NSP:"
  },
  {
    "text": "SQuAD",
    "type": "dataset",
    "char_interval": {
      "start_pos": 818,
      "end_pos": 823
    },
    "attributes": {},
    "sentence_context": "The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.",
    "sentence_index": 7,
    "section": "LTR & No NSP:"
  },
  {
    "text": "token predictions",
    "type": "task",
    "char_interval": {
      "start_pos": 899,
      "end_pos": 916
    },
    "attributes": {},
    "sentence_context": "For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context.",
    "sentence_index": 8,
    "section": "LTR & No NSP:"
  },
  {
    "text": "rightside context",
    "type": "other",
    "char_interval": {
      "start_pos": 962,
      "end_pos": 979
    },
    "attributes": {},
    "sentence_context": "For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context.",
    "sentence_index": 8,
    "section": "LTR & No NSP:"
  },
  {
    "text": "BiLSTM",
    "type": "method",
    "char_interval": {
      "start_pos": 1084,
      "end_pos": 1090
    },
    "attributes": {},
    "sentence_context": "In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top.",
    "sentence_index": 9,
    "section": "LTR & No NSP:"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1272,
      "end_pos": 1282
    },
    "attributes": {},
    "sentence_context": "The BiLSTM hurts performance on the GLUE tasks.",
    "sentence_index": 11,
    "section": "LTR & No NSP:"
  },
  {
    "text": "LTR",
    "type": "method",
    "char_interval": {
      "start_pos": 1346,
      "end_pos": 1349
    },
    "attributes": {},
    "sentence_context": "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does.",
    "sentence_index": 12,
    "section": "LTR & No NSP:"
  },
  {
    "text": "RTL models",
    "type": "method",
    "char_interval": {
      "start_pos": 1354,
      "end_pos": 1364
    },
    "attributes": {},
    "sentence_context": "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does.",
    "sentence_index": 12,
    "section": "LTR & No NSP:"
  },
  {
    "text": "ELMo",
    "type": "method",
    "char_interval": {
      "start_pos": 1433,
      "end_pos": 1437
    },
    "attributes": {},
    "sentence_context": "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does.",
    "sentence_index": 12,
    "section": "LTR & No NSP:"
  },
  {
    "text": "bidirectional model",
    "type": "method",
    "char_interval": {
      "start_pos": 1496,
      "end_pos": 1515
    },
    "attributes": {},
    "sentence_context": "However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.",
    "sentence_index": 13,
    "section": "LTR & No NSP:"
  },
  {
    "text": "QA",
    "type": "task",
    "char_interval": {
      "start_pos": 1558,
      "end_pos": 1560
    },
    "attributes": {},
    "sentence_context": "However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.",
    "sentence_index": 13,
    "section": "LTR & No NSP:"
  },
  {
    "text": "RTL model",
    "type": "method",
    "char_interval": {
      "start_pos": 1572,
      "end_pos": 1581
    },
    "attributes": {},
    "sentence_context": "However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.",
    "sentence_index": 13,
    "section": "LTR & No NSP:"
  },
  {
    "text": "deep bidirectional model",
    "type": "method",
    "char_interval": {
      "start_pos": 1686,
      "end_pos": 1710
    },
    "attributes": {},
    "sentence_context": "However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.",
    "sentence_index": 13,
    "section": "LTR & No NSP:"
  }
]