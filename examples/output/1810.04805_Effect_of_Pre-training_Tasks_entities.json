[
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 62,
      "end_pos": 66
    },
    "attributes": {},
    "sentence_context": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE:",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "pretraining objectives",
    "type": "other",
    "char_interval": {
      "start_pos": 85,
      "end_pos": 107
    },
    "attributes": {},
    "sentence_context": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE:",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "pretraining data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 131,
      "end_pos": 147
    },
    "attributes": {},
    "sentence_context": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE:",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "fine-tuning scheme",
    "type": "other",
    "char_interval": {
      "start_pos": 149,
      "end_pos": 167
    },
    "attributes": {},
    "sentence_context": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE:",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 173,
      "end_pos": 188
    },
    "attributes": {},
    "sentence_context": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE:",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 192,
      "end_pos": 201
    },
    "attributes": {},
    "sentence_context": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE:",
    "sentence_index": 0,
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 261,
      "end_pos": 270
    },
    "attributes": {},
    "sentence_context": "No NSP: A bidirectional model which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 295,
      "end_pos": 319
    },
    "attributes": {},
    "sentence_context": "No NSP: A bidirectional model which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.",
    "sentence_index": 1,
    "section": "Effect of Pre-training Tasks"
  }
]