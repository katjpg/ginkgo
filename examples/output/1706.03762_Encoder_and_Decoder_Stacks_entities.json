[
  {
    "text": "encoder",
    "type": "object",
    "char_interval": {
      "start_pos": 0,
      "end_pos": 7
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "layers",
    "type": "object",
    "char_interval": {
      "start_pos": 63,
      "end_pos": 69
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "multi-head self-attention mechanism",
    "type": "other",
    "char_interval": {
      "start_pos": 117,
      "end_pos": 152
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "fully connected feed-forward network",
    "type": "other",
    "char_interval": {
      "start_pos": 195,
      "end_pos": 231
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "residual connection",
    "type": "other",
    "char_interval": {
      "start_pos": 245,
      "end_pos": 264
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "layer normalization",
    "type": "other",
    "char_interval": {
      "start_pos": 316,
      "end_pos": 335
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "Layer Norm",
    "type": "other",
    "char_interval": {
      "start_pos": 381,
      "end_pos": 391
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "outputs",
    "type": "object",
    "char_interval": {
      "start_pos": 353,
      "end_pos": 359
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "decoder",
    "type": "object",
    "char_interval": {
      "start_pos": 629,
      "end_pos": 636
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "layers",
    "type": "object",
    "char_interval": {
      "start_pos": 63,
      "end_pos": 69
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "sub-layers",
    "type": "object",
    "char_interval": {
      "start_pos": 292,
      "end_pos": 302
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "multi-head attention",
    "type": "other",
    "char_interval": {
      "start_pos": 816,
      "end_pos": 836
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "output",
    "type": "object",
    "char_interval": {
      "start_pos": 846,
      "end_pos": 852
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "residual connections",
    "type": "other",
    "char_interval": {
      "start_pos": 909,
      "end_pos": 929
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "layer normalization",
    "type": "other",
    "char_interval": {
      "start_pos": 973,
      "end_pos": 992
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "self-attention sub-layer",
    "type": "other",
    "char_interval": {
      "start_pos": 1013,
      "end_pos": 1037
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "masking",
    "type": "other",
    "char_interval": {
      "start_pos": 1125,
      "end_pos": 1132
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "output embeddings",
    "type": "object",
    "char_interval": {
      "start_pos": 1162,
      "end_pos": 1179
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "predictions",
    "type": "object",
    "char_interval": {
      "start_pos": 1225,
      "end_pos": 1236
    },
    "section": "Encoder and Decoder Stacks"
  },
  {
    "text": "outputs",
    "type": "object",
    "char_interval": {
      "start_pos": 1281,
      "end_pos": 1288
    },
    "section": "Encoder and Decoder Stacks"
  }
]