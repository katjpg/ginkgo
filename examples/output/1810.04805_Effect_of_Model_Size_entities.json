[
  {
    "text": "fine-tuning task",
    "type": "task",
    "char_interval": {
      "start_pos": 56,
      "end_pos": 72
    },
    "attributes": {},
    "sentence_context": "In this section, we explore the effect of model size on fine-tuning task accuracy.",
    "sentence_index": 0,
    "section": "Effect of Model Size"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 73,
      "end_pos": 81
    },
    "attributes": {},
    "sentence_context": "In this section, we explore the effect of model size on fine-tuning task accuracy.",
    "sentence_index": 0,
    "section": "Effect of Model Size"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 106,
      "end_pos": 110
    },
    "attributes": {},
    "sentence_context": "We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.",
    "sentence_index": 1,
    "section": "Effect of Model Size"
  },
  {
    "text": "hyperparameters",
    "type": "other",
    "char_interval": {
      "start_pos": 219,
      "end_pos": 234
    },
    "attributes": {},
    "sentence_context": "We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.",
    "sentence_index": 1,
    "section": "Effect of Model Size"
  },
  {
    "text": "training procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 239,
      "end_pos": 257
    },
    "attributes": {},
    "sentence_context": "We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.",
    "sentence_index": 1,
    "section": "Effect of Model Size"
  },
  {
    "text": "GLUE tasks",
    "type": "dataset",
    "char_interval": {
      "start_pos": 303,
      "end_pos": 313
    },
    "attributes": {},
    "sentence_context": "Results on selected GLUE tasks are shown in Table 6.",
    "sentence_index": 2,
    "section": "Effect of Model Size"
  },
  {
    "text": "Dev Set",
    "type": "dataset",
    "char_interval": {
      "start_pos": 373,
      "end_pos": 380
    },
    "attributes": {},
    "sentence_context": "In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning.",
    "sentence_index": 3,
    "section": "Effect of Model Size"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 381,
      "end_pos": 389
    },
    "attributes": {},
    "sentence_context": "In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning.",
    "sentence_index": 3,
    "section": "Effect of Model Size"
  },
  {
    "text": "accuracy",
    "type": "metric",
    "char_interval": {
      "start_pos": 476,
      "end_pos": 484
    },
    "attributes": {},
    "sentence_context": "We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks.",
    "sentence_index": 4,
    "section": "Effect of Model Size"
  },
  {
    "text": "MRPC",
    "type": "dataset",
    "char_interval": {
      "start_pos": 532,
      "end_pos": 536
    },
    "attributes": {},
    "sentence_context": "We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks.",
    "sentence_index": 4,
    "section": "Effect of Model Size"
  },
  {
    "text": "labeled training examples",
    "type": "dataset",
    "char_interval": {
      "start_pos": 558,
      "end_pos": 583
    },
    "attributes": {},
    "sentence_context": "We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks.",
    "sentence_index": 4,
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-training tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 625,
      "end_pos": 643
    },
    "attributes": {},
    "sentence_context": "We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks.",
    "sentence_index": 4,
    "section": "Effect of Model Size"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 842,
      "end_pos": 853
    },
    "attributes": {},
    "sentence_context": "For example, the largest Transformer explored in Vaswani et al. is with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters.",
    "sentence_index": 6,
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 894,
      "end_pos": 904
    },
    "attributes": {},
    "sentence_context": "For example, the largest Transformer explored in Vaswani et al. is with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters.",
    "sentence_index": 6,
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1014,
      "end_pos": 1024
    },
    "attributes": {},
    "sentence_context": "For example, the largest Transformer explored in Vaswani et al. is with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters.",
    "sentence_index": 6,
    "section": "Effect of Model Size"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 1039,
      "end_pos": 1048
    },
    "attributes": {},
    "sentence_context": "By contrast, BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters.",
    "sentence_index": 7,
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1063,
      "end_pos": 1073
    },
    "attributes": {},
    "sentence_context": "By contrast, BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters.",
    "sentence_index": 7,
    "section": "Effect of Model Size"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 1078,
      "end_pos": 1088
    },
    "attributes": {},
    "sentence_context": "By contrast, BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters.",
    "sentence_index": 7,
    "section": "Effect of Model Size"
  },
  {
    "text": "parameters",
    "type": "other",
    "char_interval": {
      "start_pos": 1103,
      "end_pos": 1113
    },
    "attributes": {},
    "sentence_context": "By contrast, BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters.",
    "sentence_index": 7,
    "section": "Effect of Model Size"
  },
  {
    "text": "machine translation",
    "type": "task",
    "char_interval": {
      "start_pos": 1234,
      "end_pos": 1253
    },
    "attributes": {},
    "sentence_context": "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6.",
    "sentence_index": 8,
    "section": "Effect of Model Size"
  },
  {
    "text": "language modeling",
    "type": "task",
    "char_interval": {
      "start_pos": 1258,
      "end_pos": 1275
    },
    "attributes": {},
    "sentence_context": "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6.",
    "sentence_index": 8,
    "section": "Effect of Model Size"
  },
  {
    "text": "LM perplexity",
    "type": "metric",
    "char_interval": {
      "start_pos": 1306,
      "end_pos": 1319
    },
    "attributes": {},
    "sentence_context": "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6.",
    "sentence_index": 8,
    "section": "Effect of Model Size"
  },
  {
    "text": "training data",
    "type": "dataset",
    "char_interval": {
      "start_pos": 1332,
      "end_pos": 1345
    },
    "attributes": {},
    "sentence_context": "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6.",
    "sentence_index": 8,
    "section": "Effect of Model Size"
  },
  {
    "text": "model sizes",
    "type": "other",
    "char_interval": {
      "start_pos": 1464,
      "end_pos": 1475
    },
    "attributes": {},
    "sentence_context": "However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.. presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements.",
    "sentence_index": 9,
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-trained",
    "type": "other",
    "char_interval": {
      "start_pos": 1582,
      "end_pos": 1593
    },
    "attributes": {},
    "sentence_context": "However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.. presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements.",
    "sentence_index": 9,
    "section": "Effect of Model Size"
  },
  {
    "text": "downstream task",
    "type": "other",
    "char_interval": {
      "start_pos": 1627,
      "end_pos": 1642
    },
    "attributes": {},
    "sentence_context": "However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.. presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements.",
    "sentence_index": 9,
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-trained bi-LM",
    "type": "other",
    "char_interval": {
      "start_pos": 1668,
      "end_pos": 1685
    },
    "attributes": {},
    "sentence_context": "However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.. presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements.",
    "sentence_index": 9,
    "section": "Effect of Model Size"
  },
  {
    "text": "featurebased approach",
    "type": "other",
    "char_interval": {
      "start_pos": 1918,
      "end_pos": 1939
    },
    "attributes": {},
    "sentence_context": "Both of these prior works used a featurebased approach -we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.",
    "sentence_index": 10,
    "section": "Effect of Model Size"
  },
  {
    "text": "downstream tasks",
    "type": "other",
    "char_interval": {
      "start_pos": 2006,
      "end_pos": 2022
    },
    "attributes": {},
    "sentence_context": "Both of these prior works used a featurebased approach -we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.",
    "sentence_index": 10,
    "section": "Effect of Model Size"
  },
  {
    "text": "pre-trained representations",
    "type": "other",
    "char_interval": {
      "start_pos": 2173,
      "end_pos": 2200
    },
    "attributes": {},
    "sentence_context": "Both of these prior works used a featurebased approach -we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.",
    "sentence_index": 10,
    "section": "Effect of Model Size"
  }
]