[
  {
    "text": "Fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 0,
      "end_pos": 11
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "selfattention mechanism",
    "type": "other",
    "char_interval": {
      "start_pos": 41,
      "end_pos": 64
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 72,
      "end_pos": 83
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 91,
      "end_pos": 95
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "downstream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 110,
      "end_pos": 120
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 400,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 494,
      "end_pos": 508
    },
    "attributes": {},
    "sentence_context": "For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.",
    "sentence_index": 0,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 530,
      "end_pos": 559
    },
    "attributes": {},
    "sentence_context": "For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 1,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "taskspecific inputs and outputs",
    "type": "task",
    "char_interval": {
      "start_pos": 620,
      "end_pos": 651
    },
    "attributes": {},
    "sentence_context": "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.",
    "sentence_index": 2,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 657,
      "end_pos": 661
    },
    "attributes": {},
    "sentence_context": "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.",
    "sentence_index": 2,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "sequence tagging",
    "type": "task",
    "char_interval": {
      "start_pos": 970,
      "end_pos": 986
    },
    "attributes": {},
    "sentence_context": "At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging.",
    "sentence_index": 3,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 891,
      "end_pos": 909
    },
    "attributes": {},
    "sentence_context": "At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging.",
    "sentence_index": 3,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "classification",
    "type": "task",
    "char_interval": {
      "start_pos": 952,
      "end_pos": 966
    },
    "attributes": {},
    "sentence_context": "At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging.",
    "sentence_index": 3,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "entailment",
    "type": "task",
    "char_interval": {
      "start_pos": 849,
      "end_pos": 859
    },
    "attributes": {},
    "sentence_context": "At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging.",
    "sentence_index": 3,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "sentiment analysis",
    "type": "task",
    "char_interval": {
      "start_pos": 1227,
      "end_pos": 1245
    },
    "attributes": {},
    "sentence_context": "At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.",
    "sentence_index": 4,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 751,
      "end_pos": 763
    },
    "attributes": {},
    "sentence_context": "At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging.",
    "sentence_index": 3,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 1273,
      "end_pos": 1284
    },
    "attributes": {},
    "sentence_context": "Compared to pre-training, fine-tuning is relatively inexpensive.",
    "sentence_index": 5,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "pre-trained model",
    "type": "method",
    "char_interval": {
      "start_pos": 1457,
      "end_pos": 1474
    },
    "attributes": {},
    "sentence_context": "All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4.",
    "sentence_index": 6,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "task-specific details",
    "type": "task",
    "char_interval": {
      "start_pos": 1492,
      "end_pos": 1513
    },
    "attributes": {},
    "sentence_context": "All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4.",
    "sentence_index": 6,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Section 4",
    "type": "other",
    "char_interval": {
      "start_pos": 1550,
      "end_pos": 1559
    },
    "attributes": {},
    "sentence_context": "All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4.",
    "sentence_index": 6,
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Appendix A.5",
    "type": "other",
    "char_interval": {
      "start_pos": 1590,
      "end_pos": 1602
    },
    "attributes": {},
    "sentence_context": "More details can be found in Appendix A.5.",
    "sentence_index": 7,
    "section": "Fine-tuning BERT"
  }
]