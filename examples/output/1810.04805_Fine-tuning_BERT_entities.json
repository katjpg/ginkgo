[
  {
    "text": "Fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 0,
      "end_pos": 11
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 0,
    "span_text": "Fine-tuning",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "selfattention mechanism",
    "type": "other",
    "char_interval": {
      "start_pos": 41,
      "end_pos": 64
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 0,
    "span_text": "selfattention mechanism",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 72,
      "end_pos": 83
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 0,
    "span_text": "Transformer",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 91,
      "end_pos": 95
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 0,
    "span_text": "BERT",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "downstream tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 110,
      "end_pos": 120
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 0,
    "span_text": "downstream",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 0,
    "span_text": "text pairs",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.",
    "sentence_index": 1,
    "span_text": "bidirectional cross attention",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 400,
      "end_pos": 414
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.",
    "sentence_index": 1,
    "span_text": "self-attention",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 335,
      "end_pos": 364
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.",
    "sentence_index": 1,
    "span_text": "bidirectional cross attention",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "self-attention",
    "type": "other",
    "char_interval": {
      "start_pos": 494,
      "end_pos": 508
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.",
    "sentence_index": 1,
    "span_text": "self-attention",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "text pairs",
    "type": "task",
    "char_interval": {
      "start_pos": 162,
      "end_pos": 172
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
    "sentence_index": 0,
    "span_text": "text pairs",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "bidirectional cross attention",
    "type": "other",
    "char_interval": {
      "start_pos": 530,
      "end_pos": 559
    },
    "attributes": {},
    "sentence_context": "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.",
    "sentence_index": 1,
    "span_text": "bidirectional cross attention",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "taskspecific inputs and outputs",
    "type": "task",
    "char_interval": {
      "start_pos": 620,
      "end_pos": 651
    },
    "attributes": {},
    "sentence_context": "For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging.",
    "sentence_index": 2,
    "span_text": "taskspecific inputs and outputs",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "BERT",
    "type": "method",
    "char_interval": {
      "start_pos": 657,
      "end_pos": 661
    },
    "attributes": {},
    "sentence_context": "For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as.;.. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging.",
    "sentence_index": 2,
    "span_text": "BERT",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "sequence tagging",
    "type": "task",
    "char_interval": {
      "start_pos": 970,
      "end_pos": 986
    },
    "attributes": {},
    "sentence_context": "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.",
    "sentence_index": 3,
    "span_text": "sequence tagging",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "question answering",
    "type": "task",
    "char_interval": {
      "start_pos": 891,
      "end_pos": 909
    },
    "attributes": {},
    "sentence_context": "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.",
    "sentence_index": 3,
    "span_text": "question answering",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "classification",
    "type": "task",
    "char_interval": {
      "start_pos": 952,
      "end_pos": 966
    },
    "attributes": {},
    "sentence_context": "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.",
    "sentence_index": 3,
    "span_text": "classification",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "entailment",
    "type": "task",
    "char_interval": {
      "start_pos": 849,
      "end_pos": 859
    },
    "attributes": {},
    "sentence_context": "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.",
    "sentence_index": 3,
    "span_text": "entailment",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "sentiment analysis",
    "type": "task",
    "char_interval": {
      "start_pos": 1227,
      "end_pos": 1245
    },
    "attributes": {},
    "sentence_context": "At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis. Compared to pre-training, fine-tuning is relatively inexpensive.",
    "sentence_index": 4,
    "span_text": "sentiment analysis",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 751,
      "end_pos": 763
    },
    "attributes": {},
    "sentence_context": "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.",
    "sentence_index": 3,
    "span_text": "pre-training",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "fine-tuning",
    "type": "other",
    "char_interval": {
      "start_pos": 1273,
      "end_pos": 1284
    },
    "attributes": {},
    "sentence_context": "At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis. Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4.",
    "sentence_index": 5,
    "span_text": "fine-tuning",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "pre-trained model",
    "type": "method",
    "char_interval": {
      "start_pos": 1457,
      "end_pos": 1474
    },
    "attributes": {},
    "sentence_context": "Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.",
    "sentence_index": 6,
    "span_text": "pre-trained model.7We",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "task-specific details",
    "type": "task",
    "char_interval": {
      "start_pos": 1492,
      "end_pos": 1513
    },
    "attributes": {},
    "sentence_context": "Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.",
    "sentence_index": 6,
    "span_text": "task-specific details",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Section 4",
    "type": "other",
    "char_interval": {
      "start_pos": 1550,
      "end_pos": 1559
    },
    "attributes": {},
    "sentence_context": "Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.",
    "sentence_index": 6,
    "span_text": "Section 4",
    "section": "Fine-tuning BERT"
  },
  {
    "text": "Appendix A.5",
    "type": "other",
    "char_interval": {
      "start_pos": 1590,
      "end_pos": 1602
    },
    "attributes": {},
    "sentence_context": "All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.",
    "sentence_index": 7,
    "span_text": "Appendix A.5",
    "section": "Fine-tuning BERT"
  }
]