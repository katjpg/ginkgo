[
  {
    "text": "Transformer",
    "type": "method",
    "char_interval": {
      "start_pos": 4,
      "end_pos": 15
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "multi-head attention",
    "type": "other",
    "char_interval": {
      "start_pos": 21,
      "end_pos": 41
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "encoder-decoder attention",
    "type": "task",
    "char_interval": {
      "start_pos": 73,
      "end_pos": 98
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "queries",
    "type": "other",
    "char_interval": {
      "start_pos": 112,
      "end_pos": 119
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "keys",
    "type": "other",
    "char_interval": {
      "start_pos": 173,
      "end_pos": 177
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "values",
    "type": "other",
    "char_interval": {
      "start_pos": 182,
      "end_pos": 188
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 217,
      "end_pos": 224
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "every position in the decoder",
    "type": "generic",
    "char_interval": {
      "start_pos": 238,
      "end_pos": 267
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "all positions in the input sequence",
    "type": "generic",
    "char_interval": {
      "start_pos": 283,
      "end_pos": 318
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "encoder-decoder attention mechanisms",
    "type": "other",
    "char_interval": {
      "start_pos": 344,
      "end_pos": 380
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "sequence-to-sequence models",
    "type": "method",
    "char_interval": {
      "start_pos": 384,
      "end_pos": 411
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "self-attention layers",
    "type": "other",
    "char_interval": {
      "start_pos": 452,
      "end_pos": 473
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "keys",
    "type": "other",
    "char_interval": {
      "start_pos": 512,
      "end_pos": 516
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "values",
    "type": "other",
    "char_interval": {
      "start_pos": 518,
      "end_pos": 524
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "queries",
    "type": "other",
    "char_interval": {
      "start_pos": 529,
      "end_pos": 536
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "the encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 613,
      "end_pos": 624
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "Each position in the encoder",
    "type": "generic",
    "char_interval": {
      "start_pos": 626,
      "end_pos": 654
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "all positions in the previous layer of the encoder",
    "type": "generic",
    "char_interval": {
      "start_pos": 669,
      "end_pos": 719
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "self-attention layers",
    "type": "other",
    "char_interval": {
      "start_pos": 734,
      "end_pos": 755
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "each position in the decoder",
    "type": "generic",
    "char_interval": {
      "start_pos": 777,
      "end_pos": 805
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "all positions in the decoder",
    "type": "generic",
    "char_interval": {
      "start_pos": 819,
      "end_pos": 847
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "leftward information flow",
    "type": "task",
    "char_interval": {
      "start_pos": 902,
      "end_pos": 927
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "auto-regressive property",
    "type": "other",
    "char_interval": {
      "start_pos": 959,
      "end_pos": 983
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "scaled dot-product attention",
    "type": "method",
    "char_interval": {
      "start_pos": 1013,
      "end_pos": 1041
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "softmax",
    "type": "other",
    "char_interval": {
      "start_pos": 1104,
      "end_pos": 1111
    },
    "section": "Applications of Attention in our Model"
  },
  {
    "text": "values",
    "type": "other",
    "char_interval": {
      "start_pos": 1077,
      "end_pos": 1083
    },
    "section": "Applications of Attention in our Model"
  }
]