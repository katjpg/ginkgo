[
  {
    "text": "transfer learning",
    "type": "other",
    "char_interval": {
      "start_pos": 37,
      "end_pos": 54
    },
    "attributes": {},
    "sentence_context": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems.",
    "sentence_index": 0,
    "section": "Conclusion"
  },
  {
    "text": "language models",
    "type": "other",
    "char_interval": {
      "start_pos": 60,
      "end_pos": 75
    },
    "attributes": {},
    "sentence_context": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems.",
    "sentence_index": 0,
    "section": "Conclusion"
  },
  {
    "text": "unsupervised pre-training",
    "type": "other",
    "char_interval": {
      "start_pos": 105,
      "end_pos": 130
    },
    "attributes": {},
    "sentence_context": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems.",
    "sentence_index": 0,
    "section": "Conclusion"
  },
  {
    "text": "language understanding",
    "type": "task",
    "char_interval": {
      "start_pos": 159,
      "end_pos": 181
    },
    "attributes": {},
    "sentence_context": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems.",
    "sentence_index": 0,
    "section": "Conclusion"
  },
  {
    "text": "deep unidirectional architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 267,
      "end_pos": 300
    },
    "attributes": {},
    "sentence_context": "In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures.",
    "sentence_index": 1,
    "section": "Conclusion"
  },
  {
    "text": "deep bidirectional architectures",
    "type": "other",
    "char_interval": {
      "start_pos": 367,
      "end_pos": 399
    },
    "attributes": {},
    "sentence_context": "Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.",
    "sentence_index": 2,
    "section": "Conclusion"
  },
  {
    "text": "NLP tasks",
    "type": "task",
    "char_interval": {
      "start_pos": 475,
      "end_pos": 484
    },
    "attributes": {},
    "sentence_context": "Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.",
    "sentence_index": 2,
    "section": "Conclusion"
  },
  {
    "text": "Masked LM",
    "type": "method",
    "char_interval": {
      "start_pos": 486,
      "end_pos": 495
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "Masking Procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "unlabeled sentence",
    "type": "dataset",
    "char_interval": {
      "start_pos": 535,
      "end_pos": 553
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "random masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 589,
      "end_pos": 613
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 674,
      "end_pos": 691
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "MASK token",
    "type": "other",
    "char_interval": {
      "start_pos": 766,
      "end_pos": 770
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "random word",
    "type": "other",
    "char_interval": {
      "start_pos": 844,
      "end_pos": 855
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "masking procedure",
    "type": "other",
    "char_interval": {
      "start_pos": 504,
      "end_pos": 521
    },
    "attributes": {},
    "sentence_context": "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple 10% of the time: Keep the word unchanged, e.g., my dog is hairy \u2192 my dog is hairy.",
    "sentence_index": 3,
    "section": "Conclusion"
  },
  {
    "text": "representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1015,
      "end_pos": 1029
    },
    "attributes": {},
    "sentence_context": "The purpose of this is to bias the representation towards the actual observed word.",
    "sentence_index": 4,
    "section": "Conclusion"
  },
  {
    "text": "Transformer encoder",
    "type": "other",
    "char_interval": {
      "start_pos": 1108,
      "end_pos": 1127
    },
    "attributes": {},
    "sentence_context": "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token.",
    "sentence_index": 5,
    "section": "Conclusion"
  },
  {
    "text": "random words",
    "type": "other",
    "char_interval": {
      "start_pos": 1213,
      "end_pos": 1225
    },
    "attributes": {},
    "sentence_context": "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token.",
    "sentence_index": 5,
    "section": "Conclusion"
  },
  {
    "text": "contextual representation",
    "type": "other",
    "char_interval": {
      "start_pos": 1268,
      "end_pos": 1293
    },
    "attributes": {},
    "sentence_context": "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token.",
    "sentence_index": 5,
    "section": "Conclusion"
  },
  {
    "text": "input token",
    "type": "other",
    "char_interval": {
      "start_pos": 1303,
      "end_pos": 1314
    },
    "attributes": {},
    "sentence_context": "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token.",
    "sentence_index": 5,
    "section": "Conclusion"
  },
  {
    "text": "random replacement",
    "type": "other",
    "char_interval": {
      "start_pos": 1338,
      "end_pos": 1356
    },
    "attributes": {},
    "sentence_context": "Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model's language understanding capability.",
    "sentence_index": 6,
    "section": "Conclusion"
  },
  {
    "text": "language understanding capability",
    "type": "other",
    "char_interval": {
      "start_pos": 1451,
      "end_pos": 1484
    },
    "attributes": {},
    "sentence_context": "Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model's language understanding capability.",
    "sentence_index": 6,
    "section": "Conclusion"
  },
  {
    "text": "masked LM",
    "type": "task",
    "char_interval": {
      "start_pos": 1591,
      "end_pos": 1600
    },
    "attributes": {},
    "sentence_context": "Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as \"sentences\" even though they are typically much longer than single sentences (but can be shorter also).",
    "sentence_index": 8,
    "section": "Conclusion"
  },
  {
    "text": "next sentence prediction",
    "type": "task",
    "char_interval": {
      "start_pos": 2154,
      "end_pos": 2178
    },
    "attributes": {},
    "sentence_context": "The first sentence receives the A embedding and the second receives the B embedding.50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the \"next sentence prediction\" task.",
    "sentence_index": 9,
    "section": "Conclusion"
  },
  {
    "text": "corpus",
    "type": "dataset",
    "char_interval": {
      "start_pos": 2573,
      "end_pos": 2579
    },
    "attributes": {},
    "sentence_context": "We train with batch size of 256 sequences (256 sequences 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus.",
    "sentence_index": 12,
    "section": "Conclusion"
  },
  {
    "text": "Adam",
    "type": "method",
    "char_interval": {
      "start_pos": 2588,
      "end_pos": 2592
    },
    "attributes": {},
    "sentence_context": "We use Adam with learning rate of 1e-4, \u03b2 1 = 0.9, \u03b2 2 = 0.999, L 2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate.",
    "sentence_index": 13,
    "section": "Conclusion"
  },
  {
    "text": "training loss",
    "type": "metric",
    "char_interval": {
      "start_pos": 2894,
      "end_pos": 2907
    },
    "attributes": {},
    "sentence_context": "The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.",
    "sentence_index": 16,
    "section": "Conclusion"
  },
  {
    "text": "masked LM likelihood",
    "type": "other",
    "char_interval": {
      "start_pos": 2931,
      "end_pos": 2951
    },
    "attributes": {},
    "sentence_context": "The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.",
    "sentence_index": 16,
    "section": "Conclusion"
  },
  {
    "text": "next sentence prediction likelihood",
    "type": "other",
    "char_interval": {
      "start_pos": 2965,
      "end_pos": 3000
    },
    "attributes": {},
    "sentence_context": "The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.",
    "sentence_index": 16,
    "section": "Conclusion"
  },
  {
    "text": "BERT BASE",
    "type": "method",
    "char_interval": {
      "start_pos": 3014,
      "end_pos": 3023
    },
    "attributes": {},
    "sentence_context": "Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total).",
    "sentence_index": 17,
    "section": "Conclusion"
  },
  {
    "text": "BERT LARGE",
    "type": "method",
    "char_interval": {
      "start_pos": 3111,
      "end_pos": 3121
    },
    "attributes": {},
    "sentence_context": "13Training of BERT LARGE was performed on 16 Cloud TPUs (64 TPU chips total).",
    "sentence_index": 18,
    "section": "Conclusion"
  }
]