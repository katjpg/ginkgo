{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path modified: {str(project_root) in sys.path}\")\n",
    "\n",
    "try:\n",
    "    import client\n",
    "    import config\n",
    "    import models\n",
    "    import parsers\n",
    "    import nlp\n",
    "    print(\"All project modules are now accessible\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Module import failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.arxiv import ArXivClient\n",
    "from client.grobid import GROBIDClient\n",
    "from models.grobid import Form, File\n",
    "from parsers.tei import Parser\n",
    "from config.llm import LangExtractConfig, GeminiConfig\n",
    "from config.nlp import NLPConfig\n",
    "from nlp.structural import SectionProcessor\n",
    "from nlp.semantic import SemanticExtractor\n",
    "from nlp.syntactic import parse\n",
    "from nlp.entity_pairs import create_pairs, filter_by_type\n",
    "from nlp.relation import RelationExtractor\n",
    "from utils.clean_text import preprocess_section\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873bc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"2103.15348\"\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de8f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_client = ArXivClient()\n",
    "metadata = arxiv_client.get_metadata(arxiv_id)\n",
    "pdf_path = output_dir / f\"{arxiv_id}.pdf\"\n",
    "arxiv_client.download_pdf(arxiv_id, str(pdf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24202d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90851"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_client = GROBIDClient()\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "form = Form(\n",
    "    file=File(payload=pdf_bytes, file_name=f\"{arxiv_id}.pdf\"),\n",
    "    consolidate_citations=1,\n",
    "    consolidate_header=1,\n",
    "    segment_sentences=True\n",
    ")\n",
    "\n",
    "response = grobid_client.process_pdf(form)\n",
    "tei_path = output_dir / f\"{arxiv_id}.tei.xml\"\n",
    "tei_path.write_bytes(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98831392",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(response.content)\n",
    "article = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7f2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_data = []\n",
    "for section in article.sections:\n",
    "    section_text = \"\"\n",
    "    for paragraph in section.paragraphs:\n",
    "        section_text += paragraph.plain_text + \" \"\n",
    "    \n",
    "    clean_text = preprocess_section(section_text.strip())\n",
    "    sections_data.append({\n",
    "        \"title\": section.title,\n",
    "        \"raw_text\": section_text.strip(),\n",
    "        \"clean_text\": clean_text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7847d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "langextract_config = LangExtractConfig()\n",
    "gemini_config = GeminiConfig()\n",
    "nlp_config = NLPConfig()\n",
    "semantic_extractor = SemanticExtractor(langextract_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7934b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langextract as lx\n",
    "from llm.prompts.langextract import PROMPT, EXAMPLES\n",
    "from config.nlp import normalize_section\n",
    "\n",
    "all_entities = []\n",
    "\n",
    "for section_data in sections_data:\n",
    "    normalized_title = normalize_section(section_data[\"title\"], nlp_config.patterns)\n",
    "    section_config = nlp_config.sections.get(normalized_title, nlp_config.sections[\"default\"])\n",
    "    \n",
    "    result = lx.extract(\n",
    "        text_or_documents=section_data[\"clean_text\"],\n",
    "        prompt_description=PROMPT,\n",
    "        examples=EXAMPLES,\n",
    "        model_id=langextract_config.model_id,\n",
    "        api_key=langextract_config.api_key,\n",
    "        extraction_passes=section_config.extraction_passes,\n",
    "        max_workers=langextract_config.max_workers,\n",
    "        max_char_buffer=section_config.max_char_buffer,\n",
    "    )\n",
    "    \n",
    "    section_entities = []\n",
    "    for extraction in result.extractions:\n",
    "        entity_dict = {\n",
    "            \"text\": extraction.extraction_text,\n",
    "            \"type\": extraction.extraction_class,\n",
    "            \"char_interval\": (\n",
    "                {\n",
    "                    \"start_pos\": extraction.char_interval.start_pos,\n",
    "                    \"end_pos\": extraction.char_interval.end_pos,\n",
    "                }\n",
    "                if extraction.char_interval\n",
    "                else None\n",
    "            ),\n",
    "            \"section\": section_data[\"title\"]\n",
    "        }\n",
    "        section_entities.append(entity_dict)\n",
    "    \n",
    "    title = section_data[\"title\"].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    section_json = json.dumps(section_entities, indent=2)\n",
    "    section_path = output_dir / f\"{arxiv_id}_{title}_entities.json\"\n",
    "    section_path.write_text(section_json)\n",
    "    \n",
    "    all_entities.extend(section_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bde373d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104995"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities_json = json.dumps(all_entities, indent=2)\n",
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "all_entities_path.write_text(all_entities_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d65e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_by_section = {}\n",
    "for entity in all_entities:\n",
    "    section = entity[\"section\"]\n",
    "    if section not in entities_by_section:\n",
    "        entities_by_section[section] = []\n",
    "    entities_by_section[section].append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c9965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_extractor = RelationExtractor(gemini_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d951e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Section 1/13] Introduction\n",
      "  Entities: 80\n",
      "  [1/20] Deep Learning -> document image analy ✓ used_for\n",
      "  [2/20] Deep Learning -> document image class ✓ used_for\n",
      "  [3/20] Deep Learning -> layout detection ✓ used_for\n",
      "  [4/20] Deep Learning -> table detection ✓ used_for\n",
      "  [5/20] Deep Learning -> scene text detection ✓ used_for\n",
      "  [6/20] table detection -> document digitizatio ✗\n",
      "  [7/20] scene text detection -> document digitizatio ✗\n",
      "  [8/20] A generalized learni -> complicated rules ✓ improves_upon\n",
      "  [9/20] A generalized learni -> traditional methods ✓ improves_upon\n",
      "  [10/20] A generalized learni -> document digitizatio ✗\n",
      "  [11/20] A generalized learni -> Tensor Flow ✗\n",
      "  [12/20] complicated rules -> document digitizatio ✗\n",
      "  [13/20] complicated rules -> Tensor Flow ✗\n",
      "  [14/20] complicated rules -> PyTorch ✗\n",
      "  [15/20] traditional methods -> document digitizatio ✗\n",
      "  [16/20] traditional methods -> Tensor Flow ✓ compared_with\n",
      "  [17/20] traditional methods -> PyTorch ✗\n",
      "  [18/20] Existing models -> Tensor Flow ✓ based_on\n",
      "  [19/20] Existing models -> PyTorch ✓ based_on\n",
      "  [20/20] Existing models -> high-level parameter ✗\n",
      "  Found: 10 relations\n",
      "\n",
      "[Section 2/13] Related Work\n",
      "  Entities: 90\n",
      "  [1/20] DL models -> layout analysis task ✓ used_for\n",
      "  [2/20] DL models -> dhSegment ✓ based_on\n",
      "  [3/20] DL models -> fully convolutional  ✗\n",
      "  [4/20] DL models -> segmentation tasks ✓ used_for\n",
      "  [5/20] datasets -> historical documents ✗\n",
      "  [6/20] layout analysis task -> historical documents ✗\n",
      "  [7/20] dhSegment -> fully convolutional  ✓ based_on\n",
      "  [8/20] dhSegment -> segmentation tasks ✓ used_for\n",
      "  [9/20] dhSegment -> historical documents ✓ applied_to\n",
      "  [10/20] dhSegment -> Object detection-bas ✗\n",
      "  [11/20] dhSegment -> Faster R-CN ✗\n",
      "  [12/20] fully convolutional  -> segmentation tasks ✓ used_for\n",
      "  [13/20] fully convolutional  -> historical documents ✓ applied_to\n",
      "  [14/20] fully convolutional  -> Object detection-bas ✗\n",
      "  [15/20] fully convolutional  -> Faster R-CN ✗\n",
      "  [16/20] fully convolutional  -> Mask R-CN ✗\n",
      "  [17/20] segmentation tasks -> historical documents ✓ evaluated_on\n",
      "  [18/20] segmentation tasks -> document elements ✗\n",
      "  [19/20] Object detection-bas -> Faster R-CN ✗\n",
      "  [20/20] Object detection-bas -> Mask R-CN ✗\n",
      "  Found: 9 relations\n",
      "\n",
      "[Section 3/13] The Core LayoutParser Library\n",
      "  Entities: 40\n",
      "  [1/20] Layout Parser -> DLbased document ima ✓ used_for\n",
      "  [2/20] Layout Parser -> pre-trained ✗\n",
      "  [3/20] Layout Parser -> self-trained DL mode ✗\n",
      "  [4/20] Layout Parser -> layout detection ✓ used_for\n",
      "  [5/20] DLbased document ima -> The detected layout  ✗\n",
      "  [6/20] The layout detection -> pre-trained ✗\n",
      "  [7/20] The layout detection -> self-trained DL mode ✗\n",
      "  [8/20] The layout detection -> layout detection ✓ used_for\n",
      "  [9/20] The layout detection -> The detected layout  ✗\n",
      "  [10/20] The layout detection -> PubLayNet ✓ evaluated_on\n",
      "  [11/20] pre-trained -> self-trained DL mode ✗\n",
      "  [12/20] pre-trained -> layout detection ✓ used_for\n",
      "  [13/20] pre-trained -> The detected layout  ✗\n",
      "  [14/20] pre-trained -> Layouts of modern sc ✗\n",
      "  [15/20] self-trained DL mode -> layout detection ✓ used_for\n",
      "  [16/20] self-trained DL mode -> The detected layout  ✗\n",
      "  [17/20] self-trained DL mode -> PubLayNet ✗\n",
      "  [18/20] self-trained DL mode -> Layouts of modern sc ✓ applied_to\n",
      "  [19/20] self-trained DL mode -> PRImA ✗\n",
      "  [20/20] layout detection -> The detected layout  ✗\n",
      "  Found: 7 relations\n",
      "\n",
      "[Section 4/13] Layout Detection Models\n",
      "  Entities: 40\n",
      "  [1/20] Layout Parser -> document image ✓ applied_to\n",
      "  [2/20] Layout Parser -> rectangular boxes ✗\n",
      "  [3/20] Layout Parser -> content regions ✓ applied_to\n",
      "  [4/20] Layout Parser -> traditional methods ✓ compared_with\n",
      "  [5/20] a layout model -> document image ✓ applied_to\n",
      "  [6/20] a layout model -> rectangular boxes ✓ used_for\n",
      "  [7/20] a layout model -> content regions ✓ applied_to\n",
      "  [8/20] a layout model -> traditional methods ✓ compared_with\n",
      "  [9/20] a layout model -> deep convolutional n ✓ based_on\n",
      "  [10/20] traditional methods -> object detection pro ✓ used_for\n",
      "  [11/20] traditional methods -> Faster R-CN ✓ compared_with\n",
      "  [12/20] traditional methods -> Mask R-CN ✓ compared_with\n",
      "  [13/20] deep convolutional n -> object detection pro ✓ used_for\n",
      "  [14/20] deep convolutional n -> Faster R-CN ✓ based_on\n",
      "  [15/20] deep convolutional n -> Mask R-CN ✓ based_on\n",
      "  [16/20] object detection pro -> accuracy ✓ evaluated_on\n",
      "  [17/20] state-of-the-art mod -> Faster R-CN ✗\n",
      "  [18/20] state-of-the-art mod -> Mask R-CN ✗\n",
      "  [19/20] state-of-the-art mod -> accuracy ✓ evaluated_on\n",
      "  [20/20] state-of-the-art mod -> layout detection ✓ used_for\n",
      "  Found: 17 relations\n",
      "\n",
      "[Section 5/13] Layout Data Structures\n",
      "  Entities: 58\n",
      "  [1/20] Layout Parser -> data structures ✓ based_on\n",
      "  [2/20] Layout Parser -> operations ✗\n",
      "  [3/20] Layout Parser -> layout elements ✗\n",
      "  [4/20] Layout Parser -> document image analy ✓ used_for\n",
      "  [5/20] Layout Parser -> post-processing ✗\n",
      "  [6/20] data structures -> layout elements ✓ applied_to\n",
      "  [7/20] data structures -> document image analy ✓ used_for\n",
      "  [8/20] data structures -> layout analysis mode ✗\n",
      "  [9/20] operations -> layout elements ✓ applied_to\n",
      "  [10/20] operations -> document image analy ✓ used_for\n",
      "  [11/20] operations -> layout analysis mode ✗\n",
      "  [12/20] document image analy -> model outputs ✗\n",
      "  [13/20] post-processing -> layout analysis mode ✗\n",
      "  [14/20] post-processing -> DL model ✗\n",
      "  [15/20] post-processing -> model outputs ✓ applied_to\n",
      "  [16/20] post-processing -> document digitizatio ✓ used_for\n",
      "  [17/20] layout analysis mode -> DL model ✗\n",
      "  [18/20] layout analysis mode -> model outputs ✗\n",
      "  [19/20] layout analysis mode -> document digitizatio ✓ used_for\n",
      "  [20/20] layout analysis mode -> Coordinate system ✗\n",
      "  Found: 9 relations\n",
      "\n",
      "[Section 6/13] Ocr\n",
      "  Entities: 24\n",
      "  [1/20] Layout Parser -> unified interface ✗\n",
      "  [2/20] Layout Parser -> OCR tools ✓ applied_to\n",
      "  [3/20] Layout Parser -> APIs ✗\n",
      "  [4/20] Layout Parser -> protocols ✗\n",
      "  [5/20] Layout Parser -> pipeline ✓ applied_to\n",
      "  [6/20] unified interface -> comparisons ✓ used_for\n",
      "  [7/20] OCR tools -> comparisons ✓ compared_with\n",
      "  [8/20] APIs -> comparisons ✗\n",
      "  [9/20] APIs -> wrappers ✓ applied_to\n",
      "  [10/20] protocols -> comparisons ✗\n",
      "  [11/20] protocols -> wrappers ✗\n",
      "  [12/20] pipeline -> comparisons ✗\n",
      "  [13/20] pipeline -> wrappers ✗\n",
      "  [14/20] the available tools -> wrappers ✓ applied_to\n",
      "  [15/20] the available tools -> OCR engines ✗\n",
      "  [16/20] the available tools -> syntax ✗\n",
      "  [17/20] the available tools -> plug-and-play style ✗\n",
      "  [18/20] the available tools -> OCR modules ✓ compared_with\n",
      "  [19/20] wrappers -> OCR engines ✗\n",
      "  [20/20] wrappers -> syntax ✗\n",
      "  Found: 7 relations\n",
      "\n",
      "[Section 7/13] Storage and visualization\n",
      "  Entities: 19\n",
      "  [1/20] DIA -> image-based document ✗\n",
      "  [2/20] DIA -> structured database ✓ used_for\n",
      "  [3/20] DIA -> Layout Parser ✗\n",
      "  [4/20] DIA -> layout data ✗\n",
      "  [5/20] DIA -> JSON ✗\n",
      "  [6/20] Layout Parser -> layout data ✗\n",
      "  [7/20] Layout Parser -> JSON ✗\n",
      "  [8/20] Layout Parser -> csv ✗\n",
      "  [9/20] Layout Parser -> METS/ALTO XML format ✗\n",
      "  [10/20] Layout Parser -> datasets ✗\n",
      "  [11/20] JSON -> datasets ✗\n",
      "  [12/20] csv -> datasets ✗\n",
      "  [13/20] METS/ALTO XML format -> datasets ✗\n",
      "  [14/20] METS/ALTO XML format -> layout models ✗\n",
      "  [15/20] layout analysis-spec -> layout models ✗\n",
      "  [16/20] layout analysis-spec -> layout detection ✗\n",
      "  [17/20] COCO -> layout models ✗\n",
      "  [18/20] COCO -> document image ✗\n",
      "  [19/20] Page Format -> layout models ✗\n",
      "  [20/20] Page Format -> layout detection ✗\n",
      "  Found: 1 relations\n",
      "\n",
      "[Section 8/13] Customized Model Training\n",
      "  Entities: 37\n",
      "  [1/20] Layout Parser -> document analysis ta ✗\n",
      "  [2/20] Layout Parser -> document images ✗\n",
      "  [3/20] Layout Parser -> datasets ✗\n",
      "  [4/20] Layout Parser -> layout models ✗\n",
      "  [5/20] Layout Parser -> layout detection acc ✗\n",
      "  [6/20] document analysis ta -> document images ✗\n",
      "  [7/20] document analysis ta -> datasets ✗\n",
      "  [8/20] document analysis ta -> layout detection acc ✗\n",
      "  [9/20] document analysis ta -> Training data ✗\n",
      "  [10/20] datasets -> layout detection acc ✗\n",
      "  [11/20] datasets -> Training data ✗\n",
      "  [12/20] layout models -> layout detection acc ✗\n",
      "  [13/20] layout models -> Training data ✗\n",
      "  [14/20] layout models -> data annotation ✗\n",
      "  [15/20] layout models -> model training ✗\n",
      "  [16/20] layout detection acc -> Training data ✗\n",
      "  [17/20] these challenges -> data annotation ✗\n",
      "  [18/20] these challenges -> model training ✗\n",
      "  [19/20] these challenges -> toolkit ✗\n",
      "  [20/20] these challenges -> document layouts ✗\n",
      "  Found: 0 relations\n",
      "\n",
      "[Section 9/13] LayoutParser Community Platform\n",
      "  Entities: 31\n",
      "  [1/20] Layout Parser -> reusability ✗\n",
      "  [2/20] Layout Parser -> layout detection mod ✗\n",
      "  [3/20] Layout Parser -> full digitization pi ✗\n",
      "  [4/20] Layout Parser -> deep learning librar ✗\n",
      "  [5/20] Layout Parser -> community model hub ✗\n",
      "  [6/20] reusability -> layout detection mod ✗\n",
      "  [7/20] reusability -> full digitization pi ✗\n",
      "  [8/20] reusability -> layout models ✗\n",
      "  [9/20] full digitization pi -> deep learning librar ✗\n",
      "  [10/20] full digitization pi -> community model hub ✗\n",
      "  [11/20] full digitization pi -> layout models ✗\n",
      "  [12/20] full digitization pi -> self-trained models ✗\n",
      "  [13/20] deep learning librar -> layout models ✗\n",
      "  [14/20] deep learning librar -> self-trained models ✗\n",
      "  [15/20] community model hub -> layout models ✗\n",
      "  [16/20] community model hub -> self-trained models ✗\n",
      "  [17/20] community model hub -> Layout Parser pre-tr ✗\n",
      "  [18/20] the model hub -> Layout Parser pre-tr ✗\n",
      "  [19/20] the model hub -> News Navigator datas ✗\n",
      "  [20/20] the model hub -> DL models ✗\n",
      "  Found: 0 relations\n",
      "\n",
      "[Section 10/13] Use Cases\n",
      "  Entities: 22\n",
      "  [1/20] Layout Parser -> document digitizatio ✗\n",
      "  [2/20] Layout Parser -> Large-scale document ✗\n",
      "  [3/20] Layout Parser -> precision ✗\n",
      "  [4/20] Layout Parser -> efficiency ✗\n",
      "  [5/20] Layout Parser -> robustness ✗\n",
      "  [6/20] document digitizatio -> Large-scale document ✗\n",
      "  [7/20] document digitizatio -> target documents ✗\n",
      "  [8/20] Large-scale document -> precision ✗\n",
      "  [9/20] Large-scale document -> efficiency ✗\n",
      "  [10/20] Large-scale document -> robustness ✗\n",
      "  [11/20] Large-scale document -> target documents ✗\n",
      "  [12/20] precision -> target documents ✗\n",
      "  [13/20] efficiency -> target documents ✗\n",
      "  [14/20] robustness -> target documents ✗\n",
      "  [15/20] complicated structur -> layout detection mod ✗\n",
      "  [16/20] complicated structur -> simple documents ✗\n",
      "  [17/20] layout detection mod -> accuracy ✗\n",
      "  [18/20] layout detection mod -> Light-weight pipelin ✗\n",
      "  [19/20] layout detection mod -> simple documents ✗\n",
      "  [20/20] layout detection mod -> development ease ✗\n",
      "  Found: 0 relations\n",
      "\n",
      "[Section 11/13] A Comprehensive Historical Document Digitization Pipeline\n",
      "  Entities: 92\n",
      "  [1/20] digitization of hist -> valuable data ✗\n",
      "  [2/20] digitization of hist -> social, economic, an ✗\n",
      "  [3/20] scan noises -> structured represent ✗\n",
      "  [4/20] scan noises -> Layout Parser ✗\n",
      "  [5/20] page wearing -> structured represent ✗\n",
      "  [6/20] page wearing -> Layout Parser ✗\n",
      "  [7/20] page wearing -> comprehensive pipeli ✗\n",
      "  [8/20] complicated layout s -> structured represent ✗\n",
      "  [9/20] complicated layout s -> Layout Parser ✗\n",
      "  [10/20] complicated layout s -> comprehensive pipeli ✗\n",
      "  [11/20] complicated layout s -> high-quality structu ✗\n",
      "  [12/20] structured represent -> high-quality structu ✗\n",
      "  [13/20] structured represent -> historical Japanese  ✗\n",
      "  [14/20] this example -> Layout Parser ✗\n",
      "  [15/20] this example -> comprehensive pipeli ✗\n",
      "  [16/20] this example -> high-quality structu ✗\n",
      "  [17/20] this example -> historical Japanese  ✗\n",
      "  [18/20] this example -> complicated layouts ✗\n",
      "  [19/20] Layout Parser -> comprehensive pipeli ✗\n",
      "  [20/20] Layout Parser -> high-quality structu ✗\n",
      "  Found: 0 relations\n",
      "\n",
      "[Section 12/13] A light-weight Visual Table Extractor\n",
      "  Entities: 33\n",
      "  [1/20] table extraction -> table structures ✗\n",
      "  [2/20] table extraction -> born-digital PDF doc ✗\n",
      "  [3/20] document digitizatio -> table structures ✗\n",
      "  [4/20] document digitizatio -> born-digital PDF doc ✗\n",
      "  [5/20] complicated models -> born-digital PDF doc ✗\n",
      "  [6/20] complicated models -> Layout Parser ✗\n",
      "  [7/20] complicated models -> visual table extract ✗\n",
      "  [8/20] complicated models -> legal docket tables ✗\n",
      "  [9/20] Layout Parser -> visual table extract ✗\n",
      "  [10/20] Layout Parser -> legal docket tables ✗\n",
      "  [11/20] Layout Parser -> existing resources ✗\n",
      "  [12/20] Layout Parser -> pre-trained layout d ✗\n",
      "  [13/20] visual table extract -> legal docket tables ✗\n",
      "  [14/20] visual table extract -> existing resources ✗\n",
      "  [15/20] visual table extract -> pre-trained layout d ✗\n",
      "  [16/20] visual table extract -> table regions ✗\n",
      "  [17/20] existing resources -> pre-trained layout d ✗\n",
      "  [18/20] existing resources -> table regions ✗\n",
      "  [19/20] existing resources -> simple rules ✗\n",
      "  [20/20] existing resources -> rows ✗\n",
      "  Found: 0 relations\n",
      "\n",
      "[Section 13/13] Conclusion\n",
      "  Entities: 11\n",
      "  [1/20] Layout Parser -> deep learning-based  ✗\n",
      "  [2/20] Layout Parser -> documents with compl ✗\n",
      "  [3/20] Layout Parser -> DL models ✗\n",
      "  [4/20] Layout Parser -> document image datas ✗\n",
      "  [5/20] deep learning-based  -> documents with compl ✗\n",
      "  [6/20] deep learning-based  -> document image datas ✗\n",
      "  [7/20] The off-the-shelf li -> documents with compl ✗\n",
      "  [8/20] The off-the-shelf li -> DL models ✗\n",
      "  [9/20] The off-the-shelf li -> document image datas ✗\n",
      "  [10/20] The off-the-shelf li -> DIA pipelines ✗\n",
      "  [11/20] The off-the-shelf li -> code reproducibility ✗\n",
      "  [12/20] DL models -> document image datas ✗\n",
      "  [13/20] DL models -> DIA pipelines ✗\n",
      "  [14/20] DL models -> code reproducibility ✗\n",
      "  [15/20] DL models -> reusability ✗\n",
      "  [16/20] DIA pipelines -> multi-modal document ✗\n",
      "  [17/20] code reproducibility -> multi-modal document ✗\n",
      "  [18/20] reusability -> multi-modal document ✗\n",
      "  [19/20] the library -> multi-modal document ✗\n",
      "  Found: 0 relations\n"
     ]
    }
   ],
   "source": [
    "all_relations = []\n",
    "MAX_PAIRS_PER_SECTION = 20\n",
    "\n",
    "for section_idx, (section_title, section_entities) in enumerate(entities_by_section.items()):\n",
    "    if len(section_entities) < 2:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[Section {section_idx+1}/{len(entities_by_section)}] {section_title}\")\n",
    "    print(f\"  Entities: {len(section_entities)}\")\n",
    "    \n",
    "    section_data = next(s for s in sections_data if s[\"title\"] == section_title)\n",
    "    doc = parse(section_data[\"clean_text\"])\n",
    "    \n",
    "    pairs_processed = 0\n",
    "    relations_found = 0\n",
    "    \n",
    "    for i, e1 in enumerate(section_entities):\n",
    "        if pairs_processed >= MAX_PAIRS_PER_SECTION:\n",
    "            break\n",
    "            \n",
    "        for e2 in section_entities[i+1:min(i+6, len(section_entities))]:\n",
    "            if pairs_processed >= MAX_PAIRS_PER_SECTION:\n",
    "                break\n",
    "                \n",
    "            type_tuple = (e1[\"type\"].upper(), e2[\"type\"].upper())\n",
    "            if type_tuple not in VALID_TYPE_PAIRS:\n",
    "                continue\n",
    "            \n",
    "            span1 = find_entity_in_doc(e1[\"text\"], doc)\n",
    "            span2 = find_entity_in_doc(e2[\"text\"], doc)\n",
    "            \n",
    "            syntax = \"no pattern\"\n",
    "            sentence = section_data[\"clean_text\"][:500]\n",
    "            \n",
    "            if span1 and span2 and span1.sent == span2.sent:\n",
    "                syntax = verbalize_path(span1.root, span2.root)\n",
    "                sentence = span1.sent.text\n",
    "            \n",
    "            pair = {\n",
    "                \"head\": {\"text\": e1[\"text\"], \"type\": e1[\"type\"].upper()},\n",
    "                \"tail\": {\"text\": e2[\"text\"], \"type\": e2[\"type\"].upper()},\n",
    "                \"sentence\": sentence,\n",
    "                \"syntax\": syntax\n",
    "            }\n",
    "            \n",
    "            result = relation_extractor._classify(pair)\n",
    "            pairs_processed += 1\n",
    "            \n",
    "            print(f\"  [{pairs_processed}/{MAX_PAIRS_PER_SECTION}] {e1['text'][:20]} -> {e2['text'][:20]}\", end=\"\")\n",
    "            \n",
    "            if result[\"relation\"] != \"NONE\":\n",
    "                relations_found += 1\n",
    "                print(f\" ✓ {result['relation']}\")\n",
    "                \n",
    "                all_relations.append({\n",
    "                    \"head\": e1[\"text\"],\n",
    "                    \"head_type\": e1[\"type\"],\n",
    "                    \"tail\": e2[\"text\"],\n",
    "                    \"tail_type\": e2[\"type\"],\n",
    "                    \"relation\": result[\"relation\"],\n",
    "                    \"confidence\": result[\"confidence\"],\n",
    "                    \"section\": section_title,\n",
    "                    \"evidence\": sentence,\n",
    "                    \"syntax\": syntax,\n",
    "                    \"reasoning\": result.get(\"reasoning\", \"\")\n",
    "                })\n",
    "            else:\n",
    "                print(\" ✗\")\n",
    "    \n",
    "    print(f\"  Found: {relations_found} relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_json = json.dumps(all_relations, indent=2)\n",
    "relations_path = output_dir / f\"{arxiv_id}_relations.json\"\n",
    "relations_path.write_text(relations_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1fa4b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Deep Learning',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 0, 'end_pos': 13},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document image analysis',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 80, 'end_pos': 103},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document image classification',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 126, 'end_pos': 155},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'layout detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 204, 'end_pos': 220},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'table detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 229, 'end_pos': 244},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'scene text detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 254, 'end_pos': 274},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'A generalized learning-based framework',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 279, 'end_pos': 317},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'complicated rules',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 380, 'end_pos': 397},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'traditional methods',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 428, 'end_pos': 447},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document digitization projects',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 541, 'end_pos': 571},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Existing models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 750, 'end_pos': 765},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Tensor Flow',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 811, 'end_pos': 822},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'PyTorch',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 829, 'end_pos': 836},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'high-level parameters',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 850, 'end_pos': 871},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'implementation details',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 893, 'end_pos': 915},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'these methods',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1089, 'end_pos': 1102},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Document images',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1167, 'end_pos': 1182},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'patterns',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1213, 'end_pos': 1221},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'customized training',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1242, 'end_pos': 1261},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'detection accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 1303, 'end_pos': 1321},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'infrastructure',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1358, 'end_pos': 1372},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document image datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1404, 'end_pos': 1427},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'fine-tuning',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1432, 'end_pos': 1443},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 're-training',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1447, 'end_pos': 1458},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'the models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1459, 'end_pos': 1469},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'a sequence of models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1494, 'end_pos': 1514},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'final outputs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1550, 'end_pos': 1563},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document analyses',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1625, 'end_pos': 1642},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'these pipelines',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1670, 'end_pos': 1685},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'full pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1826, 'end_pos': 1840},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1934, 'end_pos': 1947},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'unified toolkit',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1959, 'end_pos': 1974},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DL-based document image analysis and processing',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1986, 'end_pos': 2033},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'aforementioned challenges',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2050, 'end_pos': 2075},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'off-the-shelf toolkit',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2137, 'end_pos': 2158},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DL models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2172, 'end_pos': 2181},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'layout detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2186, 'end_pos': 2202},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'character recognition',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2204, 'end_pos': 2225},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DIA tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2237, 'end_pos': 2246},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'pre-trained neural network models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2283, 'end_pos': 2316},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Model Zoo',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 2318, 'end_pos': 2327},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document image data annotation',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2405, 'end_pos': 2435},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'model tuning',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2440, 'end_pos': 2452},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DL model hub',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2503, 'end_pos': 2515},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'community platform',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2520, 'end_pos': 2538},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DIA models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2593, 'end_pos': 2603},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2608, 'end_pos': 2617},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'reusability',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2630, 'end_pos': 2641},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'reproducibility',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2643, 'end_pos': 2658},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'extensibility',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2664, 'end_pos': 2677},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'The library',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2690, 'end_pos': 2701},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Python APIs',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2734, 'end_pos': 2745},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'generalizability',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2766, 'end_pos': 2782},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'versatility',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2787, 'end_pos': 2798},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'pip',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2832, 'end_pos': 2835},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document image data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2875, 'end_pos': 2894},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DIA pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2938, 'end_pos': 2951},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'this tool',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 3023, 'end_pos': 3032},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'end-users',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3059, 'end_pos': 3068},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'applications',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 3099, 'end_pos': 3111},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'industry',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3120, 'end_pos': 3128},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'academic research',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3133, 'end_pos': 3150},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DL model reusability',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 3216, 'end_pos': 3236},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'natural language processing',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 3263, 'end_pos': 3290},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'computer vision',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3301, 'end_pos': 3316},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'digitization projects',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 3438, 'end_pos': 3459},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'precision',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3473, 'end_pos': 3482},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'efficiency',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3484, 'end_pos': 3494},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'robustness',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3500, 'end_pos': 3510},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'document processing tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 3546, 'end_pos': 3571},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'efficacy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3584, 'end_pos': 3592},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'flexibility',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3597, 'end_pos': 3608},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'deep learning models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 3687, 'end_pos': 3707},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'text-based layout analysis methods',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3729, 'end_pos': 3763},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Layout Parser library',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3887, 'end_pos': 3908},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DL Model Zoo',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3910, 'end_pos': 3922},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'customized model training',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 3928, 'end_pos': 3953},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DL model hub',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3990, 'end_pos': 4002},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'community platform',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 4007, 'end_pos': 4025},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DIA projects',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 4128, 'end_pos': 4140},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'DL models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 18, 'end_pos': 27},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 32, 'end_pos': 40},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'layout analysis tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 65, 'end_pos': 86},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'dhSegment',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 92, 'end_pos': 101},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'fully convolutional networks',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 115, 'end_pos': 143},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'segmentation tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 152, 'end_pos': 170},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'historical documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 174, 'end_pos': 194},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Object detection-based methods',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 196, 'end_pos': 226},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Faster R-CN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 232, 'end_pos': 243},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Mask R-CN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 252, 'end_pos': 261},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'document elements',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 291, 'end_pos': 308},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'detecting tables',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 317, 'end_pos': 333},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Graph Neural Networks',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 357, 'end_pos': 378},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'table detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 406, 'end_pos': 421},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'these models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 436, 'end_pos': 448},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'unified framework',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 502, 'end_pos': 519},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'open-source tools',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 596, 'end_pos': 613},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'document image processing',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 618, 'end_pos': 643},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'document image analysis',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 657, 'end_pos': 680},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'traditional rule-based methods',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 756, 'end_pos': 786},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'OCR-D project',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 869, 'end_pos': 874},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'toolkit',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 927, 'end_pos': 934},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DIA',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 939, 'end_pos': 942},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'the platform',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 964, 'end_pos': 976},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Neudecker et al.',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 990, 'end_pos': 1005},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'analyzing historical documents',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1031, 'end_pos': 1061},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'The Document Layout Analysis project',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1110, 'end_pos': 1146},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'born-digital PDF documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1171, 'end_pos': 1197},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'PDF data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1223, 'end_pos': 1231},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DeepLayout',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1251, 'end_pos': 1261},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Detectron2-PubLayNet',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1268, 'end_pos': 1288},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'deep learning models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1307, 'end_pos': 1327},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'layout analysis datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1339, 'end_pos': 1363},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DIA pipeline',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1393, 'end_pos': 1405},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Document Analysis and Exploitation platform',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1411, 'end_pos': 1445},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DeepDIVA project',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1473, 'end_pos': 1489},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'reproducibility',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1512, 'end_pos': 1527},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DIA methods',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1531, 'end_pos': 1542},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'OCR engines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1597, 'end_pos': 1608},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Tesseract',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1614, 'end_pos': 1623},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'easyOCR',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1629, 'end_pos': 1636},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'paddleOCR',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1644, 'end_pos': 1653},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DIA tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1722, 'end_pos': 1731},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'libraries',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1809, 'end_pos': 1818},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'reusability',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1853, 'end_pos': 1864},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DL',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1881, 'end_pos': 1883},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Dectectron2',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1900, 'end_pos': 1911},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1952, 'end_pos': 1965},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'document image',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1980, 'end_pos': 1994},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2005, 'end_pos': 2018},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'layout detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2069, 'end_pos': 2085},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'OCR',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2087, 'end_pos': 2090},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'visualization',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2092, 'end_pos': 2105},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'storage',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2111, 'end_pos': 2118},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'layout data structure',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2151, 'end_pos': 2172},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'layout annotation',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2241, 'end_pos': 2258},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'model training functions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2263, 'end_pos': 2287},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'model accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 2303, 'end_pos': 2317},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'target samples',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2325, 'end_pos': 2339},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'community platform',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2345, 'end_pos': 2363},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DIA models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2392, 'end_pos': 2402},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'digitization pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2413, 'end_pos': 2435},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'reusability',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2447, 'end_pos': 2458},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'reproducibility',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2463, 'end_pos': 2478},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'AllenNLP',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2594, 'end_pos': 2602},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'transformers',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2610, 'end_pos': 2622},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DL-based support',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2669, 'end_pos': 2685},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'computer vision',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2734, 'end_pos': 2749},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'natural language processing problems',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2754, 'end_pos': 2790},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DIA tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2854, 'end_pos': 2863},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'model hubs',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2946, 'end_pos': 2956},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Torch Hub',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2965, 'end_pos': 2974},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Tensor Flow Hub',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2983, 'end_pos': 2998},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'pretrained models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3029, 'end_pos': 3046},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'document processing pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 3063, 'end_pos': 3092},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'document data collections',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 3152, 'end_pos': 3177},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'DL models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3211, 'end_pos': 3220},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'PRImA',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 3244, 'end_pos': 3249},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'magazine layouts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3253, 'end_pos': 3269},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'PubLayNet',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 3272, 'end_pos': 3281},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'academic paper layouts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3286, 'end_pos': 3308},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Table Bank',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 3311, 'end_pos': 3321},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'tables',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3326, 'end_pos': 3332},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'academic papers',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3336, 'end_pos': 3351},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Newspaper Navigator Dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 3354, 'end_pos': 3381},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'newspaper figure layouts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3389, 'end_pos': 3413},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'HJDataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 3419, 'end_pos': 3428},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'historical Japanese document layouts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3433, 'end_pos': 3469},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'these datasets',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 3504, 'end_pos': 3518},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Layout Parser model zoo',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3550, 'end_pos': 3573},\n",
       "  'section': 'Related Work'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 15, 'end_pos': 28},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'DLbased document image analysis',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 74, 'end_pos': 105},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'The layout detection models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 189, 'end_pos': 216},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'pre-trained',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 230, 'end_pos': 241},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'self-trained DL models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 245, 'end_pos': 267},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'layout detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 272, 'end_pos': 288},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'The detected layout information',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 322, 'end_pos': 353},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'PubLayNet',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 388, 'end_pos': 397},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Layouts of modern scientific documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 410, 'end_pos': 448},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'PRImA',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 449, 'end_pos': 454},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Layouts of scanned modern magazines and scientific reports',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 461, 'end_pos': 519},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Newspaper',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 520, 'end_pos': 529},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Layouts of scanned US newspapers from the 20th century',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 537, 'end_pos': 591},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Table Bank',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 592, 'end_pos': 602},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Tableregion on modern scientific and business document',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 611, 'end_pos': 665},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'HJDataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 666, 'end_pos': 675},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Layouts of history Japanese documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 687, 'end_pos': 724},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 831, 'end_pos': 839},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'computational cost',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 844, 'end_pos': 862},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'base model',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 870, 'end_pos': 880},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'large model',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 887, 'end_pos': 898},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'ResNet 50',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 923, 'end_pos': 932},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'ResNet 101',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 936, 'end_pos': 946},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'architectures',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1010, 'end_pos': 1023},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Faster R-CN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1030, 'end_pos': 1041},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Mask R-CN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1054, 'end_pos': 1063},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'The platform',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1193, 'end_pos': 1205},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'model zoo',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1266, 'end_pos': 1275},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'layout data structures',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1294, 'end_pos': 1316},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'efficiency',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1342, 'end_pos': 1352},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'versatility',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1357, 'end_pos': 1368},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'OCR models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1428, 'end_pos': 1438},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'unified API',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1447, 'end_pos': 1458},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'OCR module',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1475, 'end_pos': 1485},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'utility functions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1523, 'end_pos': 1540},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'visualization',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1549, 'end_pos': 1562},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'storage',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1567, 'end_pos': 1574},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'layout data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1582, 'end_pos': 1593},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'layout data annotation',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1679, 'end_pos': 1701},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'model training',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1706, 'end_pos': 1720},\n",
       "  'section': 'The Core LayoutParser Library'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3, 'end_pos': 16},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'a layout model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 18, 'end_pos': 32},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'document image',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 41, 'end_pos': 55},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'rectangular boxes',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 92, 'end_pos': 109},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'content regions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 125, 'end_pos': 140},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'traditional methods',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 157, 'end_pos': 176},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'deep convolutional neural networks',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 191, 'end_pos': 225},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'object detection problem',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 313, 'end_pos': 337},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'state-of-the-art models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 342, 'end_pos': 365},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'Faster R-CN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 371, 'end_pos': 382},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'Mask R-CN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 391, 'end_pos': 400},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 454, 'end_pos': 462},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'layout detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 531, 'end_pos': 547},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'Detectron2',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 575, 'end_pos': 585},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'API',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 610, 'end_pos': 613},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'Python',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 680, 'end_pos': 686},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'lp. Dete ctro n2Lay outM odel',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 774, 'end_pos': 803},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'PubLayNet',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 814, 'end_pos': 823},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'pre-trained model weights',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 944, 'end_pos': 969},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'various datasets',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 976, 'end_pos': 992},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'languages',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1012, 'end_pos': 1021},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'time periods',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1023, 'end_pos': 1035},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'document types',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1041, 'end_pos': 1055},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'domain shift',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1064, 'end_pos': 1076},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'prediction performance',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1085, 'end_pos': 1107},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1130, 'end_pos': 1136},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'target samples',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1152, 'end_pos': 1166},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'training dataset',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1209, 'end_pos': 1225},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'document structures',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1230, 'end_pos': 1249},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'layouts',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1254, 'end_pos': 1261},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'domains',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1288, 'end_pos': 1295},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'a dataset',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1341, 'end_pos': 1350},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'test samples',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1366, 'end_pos': 1378},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'semantic syntax',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1382, 'end_pos': 1397},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'dataset name',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1474, 'end_pos': 1486},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'model name',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1491, 'end_pos': 1501},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'model-architecture-name',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1523, 'end_pos': 1546},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'pre-trained models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1598, 'end_pos': 1616},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'trained models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1717, 'end_pos': 1731},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'customized layout models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1906, 'end_pos': 1930},\n",
       "  'section': 'Layout Detection Models'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 22, 'end_pos': 35},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'data structures',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 73, 'end_pos': 88},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'operations',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 93, 'end_pos': 103},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'layout elements',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 163, 'end_pos': 178},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'document image analysis',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 183, 'end_pos': 206},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'post-processing',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 226, 'end_pos': 241},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'layout analysis model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 249, 'end_pos': 270},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'this',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 343, 'end_pos': 347},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'DL model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 367, 'end_pos': 375},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'model outputs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 439, 'end_pos': 452},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'document digitization pipeline',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 603, 'end_pos': 633},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Coordinate system',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 721, 'end_pos': 738},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'TextBlock',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 744, 'end_pos': 753},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Layout',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 763, 'end_pos': 769},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'layout data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 824, 'end_pos': 835},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'APIs',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 850, 'end_pos': 854},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'transformations',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 873, 'end_pos': 888},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'these classes',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 906, 'end_pos': 919},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'layout information',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 966, 'end_pos': 984},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Coordinate data structures',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1012, 'end_pos': 1038},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Interval',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1088, 'end_pos': 1096},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Rectangle',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1101, 'end_pos': 1110},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': '1D or 2D regions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1165, 'end_pos': 1181},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'document',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1191, 'end_pos': 1199},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'parameters',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1237, 'end_pos': 1247},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Quadrilateral class',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1251, 'end_pos': 1270},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'rectangular regions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1339, 'end_pos': 1358},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'corner points',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1413, 'end_pos': 1426},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'degrees of freedom',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1461, 'end_pos': 1479},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'shift',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1537, 'end_pos': 1542},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'pad',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1544, 'end_pos': 1547},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'scale',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1553, 'end_pos': 1558},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'intersect',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1580, 'end_pos': 1589},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'union',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1591, 'end_pos': 1596},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'is_in',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1602, 'end_pos': 1607},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'segment of the image',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1678, 'end_pos': 1698},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'image cropping operations',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1786, 'end_pos': 1811},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'crop_image',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1812, 'end_pos': 1822},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'coordinate transformations',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1827, 'end_pos': 1853},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'relative_to',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1859, 'end_pos': 1870},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'condition_on',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1875, 'end_pos': 1887},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'relative representations',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1933, 'end_pos': 1957},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'these operations',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2021, 'end_pos': 2037},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Coordinates',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2050, 'end_pos': 2061},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'TextBlock class',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2080, 'end_pos': 2095},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'positional and extra features',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2117, 'end_pos': 2146},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'layout elements',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2161, 'end_pos': 2176},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'reading orders',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2210, 'end_pos': 2224},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'parent field',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2241, 'end_pos': 2253},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'parent object',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2274, 'end_pos': 2287},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Layout class',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2291, 'end_pos': 2303},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'list of TextBlocks',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2329, 'end_pos': 2347},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'processing the elements in batch',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2361, 'end_pos': 2393},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'hierarchical layout structures',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2432, 'end_pos': 2462},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'They',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2464, 'end_pos': 2468},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'operations and transformations',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2486, 'end_pos': 2516},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Coordinate classes',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2524, 'end_pos': 2542},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'learning and deployment effort',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2560, 'end_pos': 2590},\n",
       "  'section': 'Layout Data Structures'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 0, 'end_pos': 13},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'unified interface',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 25, 'end_pos': 42},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'OCR tools',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 56, 'end_pos': 65},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'APIs',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 164, 'end_pos': 168},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'protocols',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 172, 'end_pos': 181},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'pipeline',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 258, 'end_pos': 266},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'comparisons',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 297, 'end_pos': 308},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'the available tools',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 315, 'end_pos': 334},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'wrappers',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 431, 'end_pos': 439},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'OCR engines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 455, 'end_pos': 466},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'syntax',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 497, 'end_pos': 503},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'plug-and-play style',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 534, 'end_pos': 553},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'OCR modules',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 640, 'end_pos': 651},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'Tesseract Agent',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 669, 'end_pos': 684},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'OCR software',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 722, 'end_pos': 734},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'image',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 763, 'end_pos': 768},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'OCR outputs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 775, 'end_pos': 786},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'layout data structures',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 829, 'end_pos': 851},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'digitization pipeline',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 896, 'end_pos': 917},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'Tesseract',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 956, 'end_pos': 965},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'Google Cloud Vision OCR engines',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 970, 'end_pos': 1001},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'DL-based CN-RN OCR model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1035, 'end_pos': 1059},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'Connectionist Temporal Classification loss',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1080, 'end_pos': 1117},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'customized datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1206, 'end_pos': 1225},\n",
       "  'section': 'Ocr'},\n",
       " {'text': 'DIA',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 16, 'end_pos': 19},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'image-based document data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 40, 'end_pos': 65},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'structured database',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 73, 'end_pos': 92},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 94, 'end_pos': 107},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'layout data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 127, 'end_pos': 138},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'JSON',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 167, 'end_pos': 171},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'csv',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 173, 'end_pos': 176},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'METS/ALTO XML format',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 211, 'end_pos': 231},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'datasets',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 252, 'end_pos': 260},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'layout analysis-specific formats',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 266, 'end_pos': 298},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'COCO',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 304, 'end_pos': 308},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'Page Format',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 321, 'end_pos': 332},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'layout models',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 350, 'end_pos': 363},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'layout detection',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 400, 'end_pos': 416},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'integrated API',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 505, 'end_pos': 519},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'document image',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 582, 'end_pos': 596},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'it',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 616, 'end_pos': 618},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'meta information',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 660, 'end_pos': 676},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'features',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 681, 'end_pos': 689},\n",
       "  'section': 'Storage and visualization'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 35, 'end_pos': 48},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'document analysis tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 125, 'end_pos': 148},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'document images',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 157, 'end_pos': 172},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 215, 'end_pos': 223},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'layout models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 237, 'end_pos': 250},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'layout detection accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 271, 'end_pos': 296},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'Training data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 298, 'end_pos': 311},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'these challenges',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 380, 'end_pos': 396},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'data annotation',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 454, 'end_pos': 469},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'model training',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 485, 'end_pos': 499},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'toolkit',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 530, 'end_pos': 537},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'document layouts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 563, 'end_pos': 579},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'object-level active learning',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 586, 'end_pos': 614},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'layout detection model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 641, 'end_pos': 663},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'labeling',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 683, 'end_pos': 691},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'layout objects',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 717, 'end_pos': 731},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'image',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 744, 'end_pos': 749},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'regions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 823, 'end_pos': 830},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'a layout dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 937, 'end_pos': 953},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'labeling budget',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1013, 'end_pos': 1028},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'training dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1040, 'end_pos': 1056},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'training the layout models',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1112, 'end_pos': 1138},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'Fine-tuning',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1140, 'end_pos': 1151},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'training models',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1168, 'end_pos': 1183},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'newly-labeled dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1195, 'end_pos': 1216},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1233, 'end_pos': 1242},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'pre-trained weights',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1257, 'end_pos': 1276},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'Training from scratch',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1278, 'end_pos': 1299},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'source dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1324, 'end_pos': 1338},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'target',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1343, 'end_pos': 1349},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'training set',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1390, 'end_pos': 1402},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'large-scale datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1499, 'end_pos': 1519},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'Image Net',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1525, 'end_pos': 1534},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'domains',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1567, 'end_pos': 1574},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'model performance',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 1592, 'end_pos': 1609},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'API',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1634, 'end_pos': 1637},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'benchmark datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1716, 'end_pos': 1734},\n",
       "  'section': 'Customized Model Training'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 17, 'end_pos': 30},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'reusability',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 48, 'end_pos': 59},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'layout detection models',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 63, 'end_pos': 86},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'full digitization pipelines',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 91, 'end_pos': 118},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'deep learning libraries',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 145, 'end_pos': 168},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'community model hub',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 197, 'end_pos': 216},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'layout models',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 234, 'end_pos': 247},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'self-trained models',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 276, 'end_pos': 295},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'the model hub',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 299, 'end_pos': 312},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'these models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 318, 'end_pos': 330},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'Layout Parser pre-trained models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 397, 'end_pos': 429},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 444, 'end_pos': 453},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'News Navigator dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 469, 'end_pos': 491},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'DL models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 543, 'end_pos': 552},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'document digitization pipelines',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 604, 'end_pos': 635},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'the pipeline',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 660, 'end_pos': 672},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 738, 'end_pos': 746},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'pipelines',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 759, 'end_pos': 768},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'academic papers',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 793, 'end_pos': 808},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'the Layout Parser community platform',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 876, 'end_pos': 912},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'layout pipelines',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 941, 'end_pos': 957},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'techniques',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 997, 'end_pos': 1007},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'project page',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1054, 'end_pos': 1066},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'source code',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1086, 'end_pos': 1097},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'documentation',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1099, 'end_pos': 1112},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'approaches',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1136, 'end_pos': 1146},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'discussion panel',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1150, 'end_pos': 1166},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'core Layout Parser library',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1219, 'end_pos': 1245},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'reusable components',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1270, 'end_pos': 1289},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'shared pipelines',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1303, 'end_pos': 1319},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'problems',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1357, 'end_pos': 1365},\n",
       "  'section': 'LayoutParser Community Platform'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 22, 'end_pos': 35},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'document digitization pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 101, 'end_pos': 132},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'Large-scale document processing',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 134, 'end_pos': 165},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'precision',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 177, 'end_pos': 186},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'efficiency',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 188, 'end_pos': 198},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'robustness',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 204, 'end_pos': 214},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'target documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 220, 'end_pos': 236},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'complicated structures',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 246, 'end_pos': 268},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'layout detection models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 304, 'end_pos': 327},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 351, 'end_pos': 359},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'Light-weight pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 361, 'end_pos': 383},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'simple documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 409, 'end_pos': 425},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'development ease',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 447, 'end_pos': 463},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'speed',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 465, 'end_pos': 470},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'flexibility',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 475, 'end_pos': 486},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'model training',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 542, 'end_pos': 556},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'such pipelines',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 680, 'end_pos': 694},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'structured document data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 740, 'end_pos': 764},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'downstream tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 775, 'end_pos': 791},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'source code',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 797, 'end_pos': 808},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'these projects',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 813, 'end_pos': 827},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'Layout Parser community hub',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 862, 'end_pos': 889},\n",
       "  'section': 'Use Cases'},\n",
       " {'text': 'digitization of historical documents',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 4, 'end_pos': 40},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'valuable data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 52, 'end_pos': 65},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'social, economic, and historical questions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 104, 'end_pos': 146},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'scan noises',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 159, 'end_pos': 170},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'page wearing',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 172, 'end_pos': 184},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'complicated layout structures',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 208, 'end_pos': 237},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'structured representation of historical document scans',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 251, 'end_pos': 305},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'this example',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 341, 'end_pos': 353},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 355, 'end_pos': 368},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'comprehensive pipeline',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 391, 'end_pos': 413},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'high-quality structured data',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 445, 'end_pos': 473},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'historical Japanese firm financial tables',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 479, 'end_pos': 520},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'complicated layouts',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 526, 'end_pos': 545},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'The pipeline',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 547, 'end_pos': 559},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'layout models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 572, 'end_pos': 585},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'document structures',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 618, 'end_pos': 637},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'customized OCR engines',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 646, 'end_pos': 668},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'character recognition accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 683, 'end_pos': 713},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'the document',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 740, 'end_pos': 752},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'columns of text',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 762, 'end_pos': 777},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'archaic printing technology',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 854, 'end_pos': 881},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'rule-based methods',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 976, 'end_pos': 994},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'words',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1016, 'end_pos': 1021},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'white spaces',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1039, 'end_pos': 1051},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'objects',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1100, 'end_pos': 1107},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'layout type',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1137, 'end_pos': 1148},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'object detection models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1200, 'end_pos': 1223},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'individual columns',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1255, 'end_pos': 1273},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'tokens',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1278, 'end_pos': 1284},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'training set',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 1308, 'end_pos': 1320},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'active learning based annotation tool',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1393, 'end_pos': 1430},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'The models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1453, 'end_pos': 1463},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'categories',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1491, 'end_pos': 1501},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'regions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1506, 'end_pos': 1513},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'visual features',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1558, 'end_pos': 1573},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'layout data structure',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1579, 'end_pos': 1600},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'reading orders',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1704, 'end_pos': 1718},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'horizontal position',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1732, 'end_pos': 1751},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'model predictions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1825, 'end_pos': 1842},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'layout detection accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 1928, 'end_pos': 1953},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'AP',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 1975, 'end_pos': 1977},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'column detection model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2016, 'end_pos': 2038},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'token detection model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2083, 'end_pos': 2104},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'character recognition methods',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2123, 'end_pos': 2152},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'this document',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2201, 'end_pos': 2214},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'our experiments',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2219, 'end_pos': 2234},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'tokens',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2280, 'end_pos': 2286},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'character recognition recall rate',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 2300, 'end_pos': 2333},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'OCR models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2352, 'end_pos': 2362},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'densely-arranged texts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2389, 'end_pos': 2411},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'this challenge',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2425, 'end_pos': 2439},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'document reorganization algorithm',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2453, 'end_pos': 2486},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'text',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2507, 'end_pos': 2511},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'token bounding boxes',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2525, 'end_pos': 2545},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'layout analysis',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 2562, 'end_pos': 2577},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'image of dense text',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2622, 'end_pos': 2641},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'OCR APIs',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2664, 'end_pos': 2672},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'transaction costs',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2698, 'end_pos': 2715},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'flexible coordinate system',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2721, 'end_pos': 2747},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2751, 'end_pos': 2764},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'OCR results',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2790, 'end_pos': 2801},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'historical documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2883, 'end_pos': 2903},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'fonts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2918, 'end_pos': 2923},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'glyphs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 2939, 'end_pos': 2945},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 2980, 'end_pos': 2988},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'modern texts',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3014, 'end_pos': 3026},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'flat font',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3056, 'end_pos': 3065},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'numbers',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3087, 'end_pos': 3094},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'pipeline approach',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3215, 'end_pos': 3232},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'recognition accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3269, 'end_pos': 3289},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'characters',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3318, 'end_pos': 3328},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'layout model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3408, 'end_pos': 3420},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'number regions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3433, 'end_pos': 3447},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'images',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 3509, 'end_pos': 3515},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'these regions',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 3523, 'end_pos': 3536},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'self-trained OCR model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3584, 'end_pos': 3606},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'CN-RN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3618, 'end_pos': 3623},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'The model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 3628, 'end_pos': 3637},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'Jaccard score',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3701, 'end_pos': 3714},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'Levinstein distances',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 3736, 'end_pos': 3756},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'token prediction',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 3763, 'end_pos': 3779},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'test set',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 3787, 'end_pos': 3795},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'digitization pipeline',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3864, 'end_pos': 3885},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'large-scale digitization',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 3890, 'end_pos': 3914},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'The pipeline',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 3936, 'end_pos': 3948},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'complicated rules',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 3971, 'end_pos': 3988},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'traditional methods',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 3997, 'end_pos': 4016},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'outliers',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 4066, 'end_pos': 4074},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'The DL models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 4076, 'end_pos': 4089},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'fine-grained results',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 4104, 'end_pos': 4124},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'page reorganization',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 4162, 'end_pos': 4181},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'OCR',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 4186, 'end_pos': 4189},\n",
       "  'section': 'A Comprehensive Historical Document Digitization Pipeline'},\n",
       " {'text': 'table extraction',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 47, 'end_pos': 63},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'document digitization tasks',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 100, 'end_pos': 127},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'table structures',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 213, 'end_pos': 229},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'complicated models',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 263, 'end_pos': 281},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'born-digital PDF documents',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 323, 'end_pos': 349},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 380, 'end_pos': 393},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'visual table extractor',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 433, 'end_pos': 455},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'legal docket tables',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 460, 'end_pos': 479},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'existing resources',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 490, 'end_pos': 508},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'The extractor',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 530, 'end_pos': 543},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'pre-trained layout detection model',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 551, 'end_pos': 585},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'table regions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 606, 'end_pos': 619},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'simple rules',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 629, 'end_pos': 641},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'rows',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 658, 'end_pos': 662},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'columns',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 671, 'end_pos': 678},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'PDF image',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 686, 'end_pos': 695},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'Mask R-CN',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 697, 'end_pos': 706},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'PubLayNet dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 726, 'end_pos': 743},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'Layout Parser Model Zoo',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 757, 'end_pos': 780},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'detecting table regions',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 797, 'end_pos': 820},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'model predictions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 839, 'end_pos': 856},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'overlapping predictions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 888, 'end_pos': 911},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'line detection functions',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1043, 'end_pos': 1067},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'utility module',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1113, 'end_pos': 1127},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'the pipeline',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1148, 'end_pos': 1160},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'row clustering method',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1218, 'end_pos': 1239},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'y coordinates',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1274, 'end_pos': 1287},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'token bounding boxes',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1291, 'end_pos': 1311},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'OCR engines',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1365, 'end_pos': 1376},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'non-maximal suppression algorithm',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1380, 'end_pos': 1413},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'duplicated rows',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1432, 'end_pos': 1447},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'detect tables',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1516, 'end_pos': 1529},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'structured table representation',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1637, 'end_pos': 1668},\n",
       "  'section': 'A light-weight Visual Table Extractor'},\n",
       " {'text': 'Layout Parser',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 0, 'end_pos': 13},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'deep learning-based document image analysis',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 51, 'end_pos': 94},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'The off-the-shelf library',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 96, 'end_pos': 121},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'documents with complicated structures',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 214, 'end_pos': 251},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'DL models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 337, 'end_pos': 346},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'document image datasets',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 357, 'end_pos': 380},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'DIA pipelines',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 453, 'end_pos': 466},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'code reproducibility',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 502, 'end_pos': 522},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'reusability',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 527, 'end_pos': 538},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'the library',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 587, 'end_pos': 598},\n",
       "  'section': 'Conclusion'},\n",
       " {'text': 'multi-modal document modeling',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 683, 'end_pos': 712},\n",
       "  'section': 'Conclusion'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d0c6514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': 'Deep Learning',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'document image analysis',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classification[11, arXiv:2103.15348v2[cs.',\n",
       "  'syntax': \"using 'for'\",\n",
       "  'reasoning': \"The context states 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks', directly indicating that Deep Learning is used to perform or solve document image analysis tasks.\"},\n",
       " {'head': 'Deep Learning',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'document image classification',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classification[11, arXiv:2103.15348v2[cs.',\n",
       "  'syntax': \"using 'for', 'of'\",\n",
       "  'reasoning': \"The context states 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classification'. The phrase 'for ... tasks including' clearly indicates that Deep Learning is used to perform or solve the task of document image classification.\"},\n",
       " {'head': 'Deep Learning',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'layout detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classification[11, arXiv:2103.15348v2[cs. CV] 21 Jun 2021 37], layout detection[38,22], table detection[26], and scene text detection[4]. A generalized learning-based framework dramatically reduces the need for the manual specification of complicated rules, which is the status quo with traditional methods. DL has the potential to transform DIA pipelines and',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including ... layout detection', which indicates that Deep Learning is used to perform the task of layout detection.\"},\n",
       " {'head': 'Deep Learning',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'table detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classification[11, arXiv:2103.15348v2[cs. CV] 21 Jun 2021 37], layout detection[38,22], table detection[26], and scene text detection[4]. A generalized learning-based framework dramatically reduces the need for the manual specification of complicated rules, which is the status quo with traditional methods. DL has the potential to transform DIA pipelines and',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': 'The text explicitly states that \"Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including ... table detection\", indicating that Deep Learning is employed to perform the task of table detection.'},\n",
       " {'head': 'Deep Learning',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'scene text detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classification[11, arXiv:2103.15348v2[cs. CV] 21 Jun 2021 37], layout detection[38,22], table detection[26], and scene text detection[4]. A generalized learning-based framework dramatically reduces the need for the manual specification of complicated rules, which is the status quo with traditional methods. DL has the potential to transform DIA pipelines and',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including ... scene text detection,' directly indicating that Deep Learning is employed to perform or solve the task of scene text detection.\"},\n",
       " {'head': 'A generalized learning-based framework',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'complicated rules',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'improves_upon',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'A generalized learning-based framework dramatically reduces the need for the manual specification of complicated rules, which is the status quo with traditional methods.',\n",
       "  'syntax': \"via 'reduces'\",\n",
       "  'reasoning': \"The framework 'reduces the need for' complicated rules, implying it offers a superior or more efficient approach compared to methods that require such rules, thus improving upon the previous state or method.\"},\n",
       " {'head': 'A generalized learning-based framework',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'traditional methods',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'improves_upon',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'A generalized learning-based framework dramatically reduces the need for the manual specification of complicated rules, which is the status quo with traditional methods.',\n",
       "  'syntax': \"via 'reduces'; using 'for', 'of'\",\n",
       "  'reasoning': \"The context states that the 'generalized learning-based framework' 'dramatically reduces the need for the manual specification of complicated rules,' which is the 'status quo with traditional methods.' This indicates that the framework offers an improvement over traditional methods by addressing a limitation.\"},\n",
       " {'head': 'traditional methods',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'Tensor Flow',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classification[11, arXiv:2103.15348v2[cs. CV] 21 Jun 2021 37], layout detection[38,22], table detection[26], and scene text detection[4]. A generalized learning-based framework dramatically reduces the need for the manual specification of complicated rules, which is the status quo with traditional methods. DL has the potential to transform DIA pipelines and',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text contrasts 'Deep Learning(DL)-based approaches' (of which Tensor Flow is an example) with 'traditional methods', implying a comparison where DL methods are presented as an improved alternative to traditional methods.\"},\n",
       " {'head': 'Existing models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'Tensor Flow',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Existing models are developed using distinct frameworks like Tensor Flow[1] or PyTorch[24], and the high-level parameters can be obfuscated by implementation details[8].',\n",
       "  'syntax': \"via 'developed'\",\n",
       "  'reasoning': \"The context states that 'Existing models are developed using distinct frameworks like Tensor Flow', which implies that Tensor Flow serves as the underlying framework or basis for these models.\"},\n",
       " {'head': 'Existing models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'PyTorch',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Existing models are developed using distinct frameworks like Tensor Flow[1] or PyTorch[24], and the high-level parameters can be obfuscated by implementation details[8].',\n",
       "  'syntax': \"via 'developed'\",\n",
       "  'reasoning': \"The context states that 'Existing models are developed using ... PyTorch', indicating that PyTorch serves as the foundational framework or tool upon which these models are built. This aligns with the 'based_on' relation, where one entity is derived from or built upon another.\"},\n",
       " {'head': 'DL models',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'layout analysis tasks',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'Recently, various DL models and datasets have been developed for layout analysis tasks.',\n",
       "  'syntax': \"via 'developed'\",\n",
       "  'reasoning': \"The context states that 'DL models... have been developed for layout analysis tasks,' indicating that DL models are intended to perform or solve these tasks. This aligns with the 'used_for' relation where a method solves a task.\"},\n",
       " {'head': 'DL models',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'dhSegment',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'Recently, various DL models and datasets have been developed for layout analysis tasks. The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents. Object detection-based methods like Faster R-CN[28] and Mask R-CN[12] are used for identifying document elements[38] and detecting tables[30,26]. Most recently, Graph Neural Networks[29] have also been used in table detection[27]. However, these models are usually implemented individually and there is n',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text introduces 'various DL models' and then describes 'dhSegment' as a method that 'utilizes fully convolutional networks'. Fully convolutional networks are a type of deep learning model, indicating that dhSegment is an instance of a DL model and is therefore fundamentally based on the principles and architectures of deep learning.\"},\n",
       " {'head': 'DL models',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'segmentation tasks',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'Recently, various DL models and datasets have been developed for layout analysis tasks. The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents. Object detection-based methods like Faster R-CN[28] and Mask R-CN[12] are used for identifying document elements[38] and detecting tables[30,26]. Most recently, Graph Neural Networks[29] have also been used in table detection[27]. However, these models are usually implemented individually and there is n',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that specific DL models (e.g., 'fully convolutional networks') are 'utilizes... for segmentation tasks', directly indicating that these models are employed to perform or solve these tasks.\"},\n",
       " {'head': 'dhSegment',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'fully convolutional networks',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents.',\n",
       "  'syntax': \"via 'utilizes'\",\n",
       "  'reasoning': \"The context states that 'dhSegment utilizes fully convolutional networks', indicating that dhSegment is built upon or incorporates fully convolutional networks as a fundamental component for its operation.\"},\n",
       " {'head': 'dhSegment',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'segmentation tasks',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents.',\n",
       "  'syntax': \"via 'utilizes'\",\n",
       "  'reasoning': \"The context states that 'dhSegment utilizes ... for segmentation tasks', directly indicating that the method is employed to perform the specified task, which aligns with the 'used_for' relation definition.\"},\n",
       " {'head': 'dhSegment',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'historical documents',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents.',\n",
       "  'syntax': \"via 'utilizes'\",\n",
       "  'reasoning': \"The context states that 'dhSegment utilizes ... for segmentation tasks on historical documents', indicating that the method 'dhSegment' is applied to or processes the 'historical documents'.\"},\n",
       " {'head': 'fully convolutional networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'segmentation tasks',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents.',\n",
       "  'syntax': \"via 'utilizes'\",\n",
       "  'reasoning': \"The context states that 'fully convolutional networks' are 'utilized for segmentation tasks', directly indicating that the method is employed to perform the given task.\"},\n",
       " {'head': 'fully convolutional networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'historical documents',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents.',\n",
       "  'syntax': \"via 'utilizes'\",\n",
       "  'reasoning': \"The context states that 'fully convolutional networks' are utilized 'for segmentation tasks on historical documents'. This indicates that the method (fully convolutional networks) processes or is applied to the object (historical documents) to perform a task.\"},\n",
       " {'head': 'segmentation tasks',\n",
       "  'head_type': 'task',\n",
       "  'tail': 'historical documents',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Related Work',\n",
       "  'evidence': 'The dhSegment[22] utilizes fully convolutional networks[20] for segmentation tasks on historical documents.',\n",
       "  'syntax': \"via 'utilizes'\",\n",
       "  'reasoning': \"The relation describes a task ('segmentation tasks') being performed on a specific type of data ('historical documents'). Similar to Example 9 ('Sentiment analysis was performed on the IMDb reviews dataset' -> evaluated_on), 'historical documents' functions as the dataset or domain data upon which the segmentation task is carried out.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'DLbased document image analysis',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'The Core LayoutParser Library',\n",
       "  'evidence': 'At the core of Layout Parser is an off-the-shelf toolkit that streamlines DLbased document image analysis.',\n",
       "  'syntax': \"using 'At'\",\n",
       "  'reasoning': \"The context states that Layout Parser's core toolkit 'streamlines' DLbased document image analysis, indicating that the method is used to perform or facilitate the specified task.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'layout detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'The Core LayoutParser Library',\n",
       "  'evidence': 'At the core of Layout Parser is an off-the-shelf toolkit that streamlines DLbased document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered PubLayNet[38] F / M M Layouts of modern scientific documents PRImA[3] M -Layouts of scanned modern magazines and',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that 'The layout detection models enable using pre-trained or self-trained DL models for layout detection', indicating that Layout Parser (which contains these models) is used to perform the task of layout detection.\"},\n",
       " {'head': 'The layout detection models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'layout detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'The Core LayoutParser Library',\n",
       "  'evidence': 'Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code.',\n",
       "  'syntax': \"via 'enable'\",\n",
       "  'reasoning': \"The context states that 'The layout detection models enable using ... for layout detection'. This indicates that the models (head) are utilized to perform the task (tail) of layout detection, fitting the 'used_for' definition.\"},\n",
       " {'head': 'The layout detection models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'PubLayNet',\n",
       "  'tail_type': 'dataset',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'The Core LayoutParser Library',\n",
       "  'evidence': 'At the core of Layout Parser is an off-the-shelf toolkit that streamlines DLbased document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered PubLayNet[38] F / M M Layouts of modern scientific documents PRImA[3] M -Layouts of scanned modern magazines and',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"PubLayNet is a dataset containing layout information. Although the text states 'detected layout information is stored in carefully engineered PubLayNet' rather than explicitly 'models are evaluated on PubLayNet', it is a common practice for layout detection models to be evaluated against such benchmark datasets to assess their performance. The relation 'evaluated_on' is the most fitting for a method-dataset interaction where the dataset is used for performance assessment.\"},\n",
       " {'head': 'pre-trained',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'layout detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'The Core LayoutParser Library',\n",
       "  'evidence': 'Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code.',\n",
       "  'syntax': \"via 'enable'\",\n",
       "  'reasoning': \"The context explicitly states that 'pre-trained DL models' are used 'for layout detection', which directly aligns with the 'used_for' relation where a method performs a task.\"},\n",
       " {'head': 'self-trained DL models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'layout detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'The Core LayoutParser Library',\n",
       "  'evidence': 'Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code.',\n",
       "  'syntax': \"via 'using'\",\n",
       "  'reasoning': \"The context explicitly states 'using pre-trained or self-trained DL models for layout detection', which directly indicates that the method (self-trained DL models) is employed to perform the task (layout detection).\"},\n",
       " {'head': 'self-trained DL models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'Layouts of modern scientific documents',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'The Core LayoutParser Library',\n",
       "  'evidence': 'At the core of Layout Parser is an off-the-shelf toolkit that streamlines DLbased document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered PubLayNet[38] F / M M Layouts of modern scientific documents PRImA[3] M -Layouts of scanned modern magazines and',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that 'self-trained DL models' are used for 'layout detection'. 'Layouts of modern scientific documents' represent the type of data (object/domain) that these models would process to perform layout detection, as exemplified by the mention of PubLayNet containing such layouts. Thus, the models are applied to these documents.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'document image',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions.',\n",
       "  'syntax': \"via 'takes'; using 'In'\",\n",
       "  'reasoning': \"The context states that 'a layout model takes a document image as an input', indicating that the Layout Parser method processes document images. This aligns with the 'applied_to' relation, where a method processes an object or domain data.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'content regions',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions.',\n",
       "  'syntax': \"via 'takes'; using 'In'\",\n",
       "  'reasoning': \"The context states that Layout Parser 'generates a list of rectangular boxes for the target content regions', indicating that the method processes or operates on these content regions to identify or delineate them.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'traditional methods',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'Different from traditional methods' when describing Layout Parser's approach, indicating a comparison of methodologies.\"},\n",
       " {'head': 'a layout model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'document image',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions.',\n",
       "  'syntax': \"via 'takes'; using 'In'\",\n",
       "  'reasoning': \"The context states 'a layout model takes a document image as an input', indicating that the model processes the document image. This aligns with the 'applied_to' relation where a method processes an object.\"},\n",
       " {'head': 'a layout model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'rectangular boxes',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions.',\n",
       "  'syntax': \"via 'takes'; using 'In'\",\n",
       "  'reasoning': \"The context states that 'a layout model ... generates a list of rectangular boxes', indicating that the model performs the task of generating these boxes. This aligns with the 'used_for' relation where a method performs a task.\"},\n",
       " {'head': 'a layout model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'content regions',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions.',\n",
       "  'syntax': \"via 'takes'; using 'In'\",\n",
       "  'reasoning': \"The layout model processes document images to identify and generate boxes for 'content regions', which are specific objects within the document. This aligns with the 'applied_to' relation where a method processes an object or domain.\"},\n",
       " {'head': 'a layout model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'traditional methods',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'Different from traditional methods,' which directly compares the layout model to traditional methods by highlighting their distinct approaches (deep CNNs vs. manually curated rules).\"},\n",
       " {'head': 'a layout model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'deep convolutional neural networks',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that 'it [a layout model] relies on deep convolutional neural networks', indicating that the layout model is built upon or uses these networks as its fundamental mechanism.\"},\n",
       " {'head': 'traditional methods',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'object detection problem',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that the task of identifying content regions is 'formulated as an object detection problem' and then contrasts the new method with 'traditional methods' in how it approaches this task. This implies that traditional methods were also employed to solve this problem, thus being 'used for' it, albeit with a different approach.\"},\n",
       " {'head': 'traditional methods',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'Faster R-CN',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states 'Different from traditional methods, it relies on deep convolutional neural networks...' where 'it' refers to the approach using models like Faster R-CN, directly comparing the new approach to traditional methods.\"},\n",
       " {'head': 'traditional methods',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'Mask R-CN',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states 'Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules...' where 'it' refers to the approach using models like Mask R-CN. This directly compares the new approach (including Mask R-CN) with traditional methods.\"},\n",
       " {'head': 'deep convolutional neural networks',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'object detection problem',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that the process of identifying content regions 'relies on deep convolutional neural networks' and 'is formulated as an object detection problem.' This indicates that deep convolutional neural networks are used as the method to solve or perform the task of object detection.\"},\n",
       " {'head': 'deep convolutional neural networks',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'Faster R-CN',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"Faster R-CN is a specific type of deep convolutional neural network architecture. The text implies that state-of-the-art models 'like' Faster R-CN are used, and these models are inherently deep convolutional neural networks, meaning Faster R-CN is based on this general architecture.\"},\n",
       " {'head': 'deep convolutional neural networks',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'Mask R-CN',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"Mask R-CNN is a specific type of deep convolutional neural network. The context states that the system 'relies on deep convolutional neural networks' and then lists 'Mask R-CN' as one of the 'state-of-the-art models' used, implying it is an instance or a model derived from deep convolutional neural networks.\"},\n",
       " {'head': 'object detection problem',\n",
       "  'head_type': 'task',\n",
       "  'tail': 'accuracy',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that the object detection problem, when solved by models, 'yields prediction results of high accuracy.' This indicates that accuracy is a metric used to evaluate the performance of the solution to the object detection problem.\"},\n",
       " {'head': 'state-of-the-art models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'accuracy',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that the use of 'state-of-the-art models' 'yields prediction results of high accuracy,' indicating that accuracy is a metric by which the performance of these models is characterized or measured. This aligns with the 'evaluated_on' relation type, where a method's performance is tested or described using a metric.\"},\n",
       " {'head': 'state-of-the-art models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'layout detection',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Detection Models',\n",
       "  'evidence': 'In Layout Parser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CN[28] and Mask R-CN[12] are used. This yields prediction results of high accuracy and makes it possible to build a conc',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that 'state-of-the-art models' are 'used' for the problem, which is 'layout detection' formulated as an object detection problem. This directly aligns with the 'used_for' definition where a method solves/performs a task.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'data structures',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements.',\n",
       "  'syntax': 'direct connection',\n",
       "  'reasoning': \"The context states that 'A critical feature of Layout Parser is the implementation of a series of data structures'. This implies that Layout Parser is fundamentally built upon or incorporates these data structures as a core component of its design and functionality, similar to how a method can be 'based on' an architecture or underlying principles.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'document image analysis',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements. In document image analysis pipelines, various post-processing on the layout analysis model outputs is usually required to obtain the final outputs. Traditionally, this requires exporting DL model outputs and then loading the results into other pipelines. All model outputs from Layout Parser will be stored in carefully ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text describes Layout Parser as providing data structures and operations to process and manipulate layout elements, which are critical steps within 'document image analysis pipelines'. This indicates Layout Parser is a method used to perform or facilitate the task of document image analysis.\"},\n",
       " {'head': 'data structures',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'layout elements',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements.',\n",
       "  'syntax': \"via 'used'; using 'of'\",\n",
       "  'reasoning': \"The context states that 'data structures' are 'used to efficiently process and manipulate the layout elements'. This directly aligns with the 'applied_to' relation, where a method or component processes an object.\"},\n",
       " {'head': 'data structures',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'document image analysis',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements. In document image analysis pipelines, various post-processing on the layout analysis model outputs is usually required to obtain the final outputs. Traditionally, this requires exporting DL model outputs and then loading the results into other pipelines. All model outputs from Layout Parser will be stored in carefully ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': 'The data structures are described as being used to efficiently process and manipulate layout elements. This processing is a critical step within document image analysis pipelines, indicating the data structures are instrumental in performing aspects of the document image analysis task.'},\n",
       " {'head': 'operations',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'layout elements',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements.',\n",
       "  'syntax': \"via 'used'; using 'of'\",\n",
       "  'reasoning': \"The context states that 'operations' are 'used to efficiently process and manipulate the layout elements'. 'Operations' can be considered a method or set of actions, and 'layout elements' are explicitly identified as an object. This directly matches the definition of 'applied_to' where a method processes an object.\"},\n",
       " {'head': 'operations',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'document image analysis',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements. In document image analysis pipelines, various post-processing on the layout analysis model outputs is usually required to obtain the final outputs. Traditionally, this requires exporting DL model outputs and then loading the results into other pipelines. All model outputs from Layout Parser will be stored in carefully ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The 'operations' are described as functionalities within Layout Parser that 'can be used to efficiently process and manipulate layout elements,' which are crucial steps in 'document image analysis pipelines.' This implies the operations are instrumental in performing the task of document image analysis.\"},\n",
       " {'head': 'post-processing',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'model outputs',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'In document image analysis pipelines, various post-processing on the layout analysis model outputs is usually required to obtain the final outputs.',\n",
       "  'syntax': \"via 'required'\",\n",
       "  'reasoning': \"The context states 'post-processing on the layout analysis model outputs', indicating that 'post-processing' is an action or process performed directly on 'model outputs', which fits the 'applied_to' relation where a method processes an object.\"},\n",
       " {'head': 'post-processing',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'document digitization pipeline',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements. In document image analysis pipelines, various post-processing on the layout analysis model outputs is usually required to obtain the final outputs. Traditionally, this requires exporting DL model outputs and then loading the results into other pipelines. All model outputs from Layout Parser will be stored in carefully ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that 'various post-processing ... is usually required to obtain the final outputs' within 'document image analysis pipelines', indicating that post-processing is a method used to perform or complete the task of the pipeline.\"},\n",
       " {'head': 'layout analysis model',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'document digitization pipeline',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Layout Data Structures',\n",
       "  'evidence': 'A critical feature of Layout Parser is the implementation of a series of data structures and operations that can be used to efficiently process and manipulate the layout elements. In document image analysis pipelines, various post-processing on the layout analysis model outputs is usually required to obtain the final outputs. Traditionally, this requires exporting DL model outputs and then loading the results into other pipelines. All model outputs from Layout Parser will be stored in carefully ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that 'post-processing on the layout analysis model outputs is usually required to obtain the final outputs' within 'document image analysis pipelines'. This implies that the layout analysis model is a component or performs a function that contributes to or is used within the document digitization pipeline.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'OCR tools',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Ocr',\n",
       "  'evidence': 'Layout Parser provides a unified interface for existing OCR tools.',\n",
       "  'syntax': \"via 'provides'\",\n",
       "  'reasoning': \"Layout Parser provides an interface that operates on and manages existing OCR tools. While 'OCR tools' are not typical 'objects' or 'domain data' like in the examples, the method's functionality is directed towards and interacts with these tools, making 'applied_to' the most fitting relation.\"},\n",
       " {'head': 'Layout Parser',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'pipeline',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Ocr',\n",
       "  'evidence': 'Layout Parser provides a unified interface for existing OCR tools. Though there are many OCR tools available, they are usually configured differently with distinct APIs or protocols for using them. It can be inefficient to add new OCR tools into an existing pipeline, and difficult to make direct comparisons among the available tools to find the best option for a particular project. To this end, Layout Parser builds a series of wrappers among existing OCR engines, and provides nearly the same syn',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': 'Layout Parser provides a unified interface and wrappers for OCR tools, specifically addressing the inefficiency of adding new OCR tools into an existing pipeline. This indicates that Layout Parser is applied to the domain of managing and integrating tools within a processing pipeline.'},\n",
       " {'head': 'unified interface',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'comparisons',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Ocr',\n",
       "  'evidence': 'Layout Parser provides a unified interface for existing OCR tools. Though there are many OCR tools available, they are usually configured differently with distinct APIs or protocols for using them. It can be inefficient to add new OCR tools into an existing pipeline, and difficult to make direct comparisons among the available tools to find the best option for a particular project. To this end, Layout Parser builds a series of wrappers among existing OCR engines, and provides nearly the same syn',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context explains that without a unified interface, it is 'difficult to make direct comparisons'. The unified interface is presented as a solution that provides 'nearly the same syn' (syntax/API), implying its purpose is to facilitate or enable these comparisons. Thus, the unified interface is 'used for' making comparisons easier.\"},\n",
       " {'head': 'OCR tools',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'comparisons',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Ocr',\n",
       "  'evidence': 'Layout Parser provides a unified interface for existing OCR tools. Though there are many OCR tools available, they are usually configured differently with distinct APIs or protocols for using them. It can be inefficient to add new OCR tools into an existing pipeline, and difficult to make direct comparisons among the available tools to find the best option for a particular project. To this end, Layout Parser builds a series of wrappers among existing OCR engines, and provides nearly the same syn',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states it is 'difficult to make direct comparisons among the available tools'. This indicates that 'OCR tools' are the entities being subjected to 'comparisons'. While 'comparisons' is not another method, it represents the act of comparing, making 'compared_with' the most semantically appropriate relation.\"},\n",
       " {'head': 'APIs',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'wrappers',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Ocr',\n",
       "  'evidence': 'Layout Parser provides a unified interface for existing OCR tools. Though there are many OCR tools available, they are usually configured differently with distinct APIs or protocols for using them. It can be inefficient to add new OCR tools into an existing pipeline, and difficult to make direct comparisons among the available tools to find the best option for a particular project. To this end, Layout Parser builds a series of wrappers among existing OCR engines, and provides nearly the same syn',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that 'Layout Parser builds a series of wrappers among existing OCR engines' because these engines have 'distinct APIs'. The wrappers are designed to manage or unify these distinct APIs, implying that the 'wrappers' (method) are applied to or interact with the 'APIs' (as an object/interface they process or abstract).\"},\n",
       " {'head': 'the available tools',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'wrappers',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Ocr',\n",
       "  'evidence': 'Layout Parser provides a unified interface for existing OCR tools. Though there are many OCR tools available, they are usually configured differently with distinct APIs or protocols for using them. It can be inefficient to add new OCR tools into an existing pipeline, and difficult to make direct comparisons among the available tools to find the best option for a particular project. To this end, Layout Parser builds a series of wrappers among existing OCR engines, and provides nearly the same syn',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that 'Layout Parser builds a series of wrappers among existing OCR engines'. 'Existing OCR engines' refers to 'the available tools'. The wrappers (method) are designed to encapsulate and interact with these tools (objects/systems), fitting the 'applied_to' definition where a method processes an object or domain.\"},\n",
       " {'head': 'the available tools',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'OCR modules',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Ocr',\n",
       "  'evidence': 'Layout Parser provides a unified interface for existing OCR tools. Though there are many OCR tools available, they are usually configured differently with distinct APIs or protocols for using them. It can be inefficient to add new OCR tools into an existing pipeline, and difficult to make direct comparisons among the available tools to find the best option for a particular project. To this end, Layout Parser builds a series of wrappers among existing OCR engines, and provides nearly the same syn',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context explicitly states that it is 'difficult to make direct comparisons among the available tools' (referring to OCR tools/modules), indicating that the relationship between these entities is one of comparison.\"},\n",
       " {'head': 'DIA',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'structured database',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Storage and visualization',\n",
       "  'evidence': 'The end goal of DIA is to transform the image-based document data into a structured database.',\n",
       "  'syntax': \"via 'transform'\",\n",
       "  'reasoning': \"The context states that the 'end goal of DIA is to transform ... into a structured database.' This indicates that DIA (method) is employed to perform the task of creating a structured database, making 'used_for' the most appropriate relation. The 'structured database' is the outcome or purpose for which the method is used.\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_relations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginkgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
