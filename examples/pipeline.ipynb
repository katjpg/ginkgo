{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path modified: {str(project_root) in sys.path}\")\n",
    "\n",
    "try:\n",
    "    import client\n",
    "    import config\n",
    "    import models\n",
    "    import parsers\n",
    "    import nlp\n",
    "    print(\"All project modules are now accessible\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Module import failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from client.arxiv import ArXivClient\n",
    "from client.grobid import GROBIDClient\n",
    "from models.grobid import Form, File\n",
    "from parsers.tei import Parser\n",
    "from config.llm import LangExtractConfig, GeminiConfig\n",
    "from config.nlp import NLPConfig\n",
    "from nlp.structural import SectionProcessor\n",
    "from nlp.semantic import SemanticExtractor\n",
    "from nlp.syntactic import parse\n",
    "from utils.clean_text import preprocess_section\n",
    "\n",
    "from nlp.entity_filter import (\n",
    "    normalize_text,\n",
    "    should_merge,\n",
    "    filter_pipeline,\n",
    "    analyze_impact,\n",
    "    nlp,\n",
    "    FilterConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1706.03762\"\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_client = ArXivClient()\n",
    "metadata = arxiv_client.get_metadata(arxiv_id)\n",
    "pdf_path = output_dir / f\"{arxiv_id}.pdf\"\n",
    "arxiv_client.download_pdf(arxiv_id, str(pdf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24202d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_client = GROBIDClient()\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "form = Form(\n",
    "    file=File(payload=pdf_bytes, file_name=f\"{arxiv_id}.pdf\"),\n",
    "    consolidate_citations=1,\n",
    "    consolidate_header=1,\n",
    "    segment_sentences=True\n",
    ")\n",
    "\n",
    "response = grobid_client.process_pdf(form)\n",
    "tei_path = output_dir / f\"{arxiv_id}.tei.xml\"\n",
    "tei_path.write_bytes(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98831392",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(response.content)\n",
    "article = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_data = []\n",
    "for section in article.sections:\n",
    "    section_text = \"\"\n",
    "    for paragraph in section.paragraphs:\n",
    "        section_text += paragraph.plain_text + \" \"\n",
    "    \n",
    "    clean_text = preprocess_section(section_text.strip())\n",
    "    sections_data.append({\n",
    "        \"title\": section.title,\n",
    "        \"raw_text\": section_text.strip(),\n",
    "        \"clean_text\": clean_text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7847d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "langextract_config = LangExtractConfig()\n",
    "gemini_config = GeminiConfig()\n",
    "nlp_config = NLPConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langextract as lx\n",
    "from llm.prompts.langextract import PROMPT, EXAMPLES\n",
    "from config.nlp import normalize_section\n",
    "\n",
    "all_entities = []\n",
    "\n",
    "for section_data in sections_data:\n",
    "    normalized_title = normalize_section(section_data[\"title\"], nlp_config.patterns)\n",
    "    section_config = nlp_config.sections.get(normalized_title, nlp_config.sections[\"default\"])\n",
    "    \n",
    "    result = lx.extract(\n",
    "        text_or_documents=section_data[\"clean_text\"],\n",
    "        prompt_description=PROMPT,\n",
    "        examples=EXAMPLES,\n",
    "        model_id=langextract_config.model_id,\n",
    "        api_key=langextract_config.api_key,\n",
    "        extraction_passes=section_config.extraction_passes,\n",
    "        max_workers=langextract_config.max_workers,\n",
    "        max_char_buffer=section_config.max_char_buffer,\n",
    "    )\n",
    "    \n",
    "    section_entities = []\n",
    "    for extraction in result.extractions:\n",
    "        entity_dict = {\n",
    "            \"text\": extraction.extraction_text,\n",
    "            \"type\": extraction.extraction_class,\n",
    "            \"char_interval\": (\n",
    "                {\n",
    "                    \"start_pos\": extraction.char_interval.start_pos,\n",
    "                    \"end_pos\": extraction.char_interval.end_pos,\n",
    "                }\n",
    "                if extraction.char_interval\n",
    "                else None\n",
    "            ),\n",
    "            \"section\": section_data[\"title\"]\n",
    "        }\n",
    "        section_entities.append(entity_dict)\n",
    "    \n",
    "    title = section_data[\"title\"].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    section_json = json.dumps(section_entities, indent=2)\n",
    "    section_path = output_dir / f\"{arxiv_id}_{title}_entities.json\"\n",
    "    section_path.write_text(section_json)\n",
    "    \n",
    "    all_entities.extend(section_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_clean_text = \"\\n\\n\".join(s[\"clean_text\"] for s in sections_data)\n",
    "full_text_path = output_dir / f\"{arxiv_id}_full_text.txt\"\n",
    "full_text_path.write_text(full_clean_text)\n",
    "\n",
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "all_entities_json = json.dumps(all_entities, indent=2)\n",
    "all_entities_path.write_text(all_entities_json)\n",
    "\n",
    "print(f\"Full text saved to: {full_text_path}\")\n",
    "print(f\"All entities saved to: {all_entities_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c5489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(entities: list[dict], top_n: int = 20):\n",
    "    type_dist = Counter(e[\"type\"] for e in entities)\n",
    "    print(f\"Type distribution: {dict(type_dist)}\\n\")\n",
    "    \n",
    "    print(f\"Top {min(top_n, len(entities))} entities:\")\n",
    "    for i, e in enumerate(entities[:top_n], 1):\n",
    "        score = e.get(\"pr_score\", 0)\n",
    "        print(f\"  {i:2d}. {e['text']:<40} {e['type']:<8} {score:.4f}\")\n",
    "\n",
    "def validate_results(entities: list[dict]) -> bool:\n",
    "    entity_texts = {e[\"text\"] for e in entities}\n",
    "    \n",
    "    checks = [\n",
    "        (\"WMT 2014 English-German dataset\" in entity_texts, \"German dataset preserved\"),\n",
    "        (\"WMT 2014 English-French dataset\" in entity_texts, \"French datasets preserved\"),\n",
    "        (\"steps\" not in entity_texts, \"single-word infra filtered\"),\n",
    "        (\"sequences\" not in entity_texts, \"plural infra (lemma) filtered\"),\n",
    "        (\"hidden states\" not in entity_texts, \"infra phrases filtered\"),\n",
    "        (\"the model\" not in entity_texts, \"generic patterns filtered\"),\n",
    "    ]\n",
    "    \n",
    "    passed = sum(1 for check, _ in checks if check)\n",
    "    print(f\"\\nValidation: {passed}/{len(checks)} passed\")\n",
    "    \n",
    "    for check, description in checks:\n",
    "        if not check:\n",
    "            print(f\"  FAIL: {description}\")\n",
    "    \n",
    "    return passed == len(checks)\n",
    "\n",
    "def test_normalize():\n",
    "    print(\"Running test_normalize...\")\n",
    "    cases = [\n",
    "        (\"Scaled Dot-Product Attention\", \"scaled dot product attention\"),\n",
    "        (\"machine translations\", \"machine translation\"),\n",
    "        (\"Recurrent neural networks\", \"recurrent neural network\"),\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    for original, expected in cases:\n",
    "        result = normalize_text(original)\n",
    "        if result == expected:\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"  FAIL: '{original}' -> '{result}' (expected '{expected}')\")\n",
    "    \n",
    "    print(f\"normalize_text: {passed}/{len(cases)} passed\")\n",
    "\n",
    "def test_fuzzy_matching():\n",
    "    print(\"Running test_fuzzy_matching...\")\n",
    "    cases = [\n",
    "        (\"Recurrent neural networks\", \"recurrent network\", \"method\", True),\n",
    "        (\"machine translation\", \"neural machine translation\", \"method\", True),\n",
    "        (\"WMT 2014 English-German dataset\", \"WMT 2014 English-French dataset\", \"dataset\", False),\n",
    "        (\"input sequence\", \"output sequences\", \"object\", False),\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    for text1, text2, etype, expected in cases:\n",
    "        entity1 = {\"text\": text1, \"type\": etype}\n",
    "        entity2 = {\"text\": text2, \"type\": etype}\n",
    "        result = should_merge(entity1, entity2)\n",
    "        \n",
    "        if result == expected:\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"  FAIL: '{text1}' vs '{text2}' -> {result} (expected {expected})\")\n",
    "    \n",
    "    print(f\"should_merge: {passed}/{len(cases)} passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34e512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests...\n",
      "Running test_normalize...\n",
      "normalize_text: 3/3 passed\n",
      "Running test_fuzzy_matching...\n",
      "should_merge: 4/4 passed\n"
     ]
    }
   ],
   "source": [
    "print(\"Running tests...\")\n",
    "test_normalize()\n",
    "test_fuzzy_matching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7bf9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entities from: output/1706.03762_all_entities.json\n",
      "Loading and processing full text from: output/1706.03762_full_text.txt\n",
      "spaCy Doc object created.\n"
     ]
    }
   ],
   "source": [
    "arxiv_id = \"1706.03762\"\n",
    "output_dir = Path(\"output\")\n",
    "\n",
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "full_text_path = output_dir / f\"{arxiv_id}_full_text.txt\"\n",
    "\n",
    "print(f\"Loading entities from: {all_entities_path}\")\n",
    "with open(all_entities_path) as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "print(f\"Loading and processing full text from: {full_text_path}\")\n",
    "full_text = full_text_path.read_text()\n",
    "doc = nlp(full_text)\n",
    "print(\"spaCy Doc object created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e03ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running pipeline with config: window_size=10\n"
     ]
    }
   ],
   "source": [
    "config = FilterConfig(\n",
    "    min_freq=1,\n",
    "    exclude_other=True,\n",
    "    use_fuzzy=True,\n",
    "    top_k=50,\n",
    "    window_size=10,\n",
    "    pagerank_alpha=0.85\n",
    ")\n",
    "\n",
    "print(f\"\\nRunning pipeline with config: window_size={config.window_size}\")\n",
    "\n",
    "filtered_entities = filter_pipeline(\n",
    "    all_entities, \n",
    "    doc,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "299d0f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type distribution: {'method': 25, 'task': 15, 'metric': 3, 'dataset': 2, 'object': 2}\n",
      "\n",
      "Top 20 entities:\n",
      "   1. Transformer                              method   0.0781\n",
      "   2. self-attention                           method   0.0592\n",
      "   3. convolution                              method   0.0417\n",
      "   4. language modeling                        task     0.0413\n",
      "   5. attention mechanisms                     method   0.0364\n",
      "   6. Multi-Head Attention                     method   0.0345\n",
      "   7. transduction models                      task     0.0329\n",
      "   8. RNs                                      method   0.0324\n",
      "   9. encoder-decoder architectures            method   0.0308\n",
      "  10. machine translation                      task     0.0266\n",
      "  11. model architecture                       method   0.0257\n",
      "  12. sequence transduction                    task     0.0244\n",
      "  13. Recurrent neural networks                method   0.0233\n",
      "  14. long short-term memory                   method   0.0233\n",
      "  15. gated recurrent                          method   0.0233\n",
      "  16. recurrent language models                method   0.0233\n",
      "  17. factorization tricks                     method   0.0233\n",
      "  18. conditional computation                  method   0.0233\n",
      "  19. Extended Neural GPU                      method   0.0233\n",
      "  20. ByteNet                                  method   0.0233\n",
      "\n",
      "Validation: 6/6 passed\n",
      "\n",
      "Reduction: 85.8% (332 -> 47)\n"
     ]
    }
   ],
   "source": [
    "print_results(filtered_entities)\n",
    "validate_results(filtered_entities)\n",
    "\n",
    "impact = analyze_impact(all_entities, filtered_entities)\n",
    "print(f\"\\nReduction: {impact['reduction_pct']:.1f}% ({impact['original_count']} -> {impact['filtered_count']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginkgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
