{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71a8d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/kat/Desktop/pinned/ginkgo\n",
      "Python path modified: True\n",
      "All project modules are now accessible\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path modified: {str(project_root) in sys.path}\")\n",
    "\n",
    "try:\n",
    "    import client\n",
    "    import config\n",
    "    import models\n",
    "    import parsers\n",
    "    import nlp\n",
    "    print(\"All project modules are now accessible\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Module import failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from client.arxiv import ArXivClient\n",
    "from client.grobid import GROBIDClient\n",
    "from models.grobid import Form, File\n",
    "from parsers.tei import Parser\n",
    "from config.llm import LangExtractConfig, GeminiConfig\n",
    "from config.nlp import NLPConfig\n",
    "from nlp.structural import SectionProcessor\n",
    "from nlp.semantic import EntityExtractor\n",
    "from nlp.syntactic import parse\n",
    "from utils.clean_text import preprocess_section\n",
    "\n",
    "from nlp.entity_filter import (\n",
    "    normalize_text,\n",
    "    should_merge,\n",
    "    filter_pipeline,\n",
    "    analyze_impact,\n",
    "    nlp,\n",
    "    FilterConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873bc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1810.04805\"\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de8f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_client = ArXivClient()\n",
    "metadata = arxiv_client.get_metadata(arxiv_id)\n",
    "pdf_path = output_dir / f\"{arxiv_id}.pdf\"\n",
    "arxiv_client.download_pdf(arxiv_id, str(pdf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24202d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122604"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_client = GROBIDClient()\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "form = Form(\n",
    "    file=File(payload=pdf_bytes, file_name=f\"{arxiv_id}.pdf\"),\n",
    "    consolidate_citations=1,\n",
    "    consolidate_header=1,\n",
    "    segment_sentences=True\n",
    ")\n",
    "\n",
    "response = grobid_client.process_pdf(form)\n",
    "tei_path = output_dir / f\"{arxiv_id}.tei.xml\"\n",
    "tei_path.write_bytes(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98831392",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(response.content)\n",
    "article = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7f2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_data = []\n",
    "for section in article.sections:\n",
    "    section_text = \"\"\n",
    "    for paragraph in section.paragraphs:\n",
    "        section_text += paragraph.plain_text + \" \"\n",
    "    \n",
    "    clean_text = preprocess_section(section_text.strip())\n",
    "    sections_data.append({\n",
    "        \"title\": section.title,\n",
    "        \"raw_text\": section_text.strip(),\n",
    "        \"clean_text\": clean_text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7847d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "langextract_config = LangExtractConfig()\n",
    "gemini_config = GeminiConfig()\n",
    "nlp_config = NLPConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.nlp import normalize_section\n",
    "\n",
    "extractor = EntityExtractor(langextract_config)\n",
    "all_entities = []\n",
    "\n",
    "for section_data in sections_data:\n",
    "    normalized_title = normalize_section(section_data[\"title\"], nlp_config.patterns)\n",
    "    section_config = nlp_config.sections.get(normalized_title, nlp_config.sections[\"default\"])\n",
    "    \n",
    "    section_doc = nlp(section_data[\"clean_text\"])\n",
    "    \n",
    "    section_entities = extractor.extract(section_data[\"clean_text\"], section_config)\n",
    "    \n",
    "    entities_with_context = extractor.convert_to_spans(section_entities, section_doc, context_size=2)\n",
    "    \n",
    "    serializable_entities = []\n",
    "    for entity in entities_with_context:\n",
    "        entity_copy = dict(entity)\n",
    "        del entity_copy[\"span\"]\n",
    "        entity_copy[\"section\"] = section_data[\"title\"]\n",
    "        serializable_entities.append(entity_copy)\n",
    "    \n",
    "    title = section_data[\"title\"].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    section_path = output_dir / f\"{arxiv_id}_{title}_entities.json\"\n",
    "    section_path.write_text(json.dumps(serializable_entities, indent=2))\n",
    "    \n",
    "    all_entities.extend(serializable_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bde373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text saved to: output/1810.04805_full_text.txt\n",
      "All entities saved to: output/1810.04805_all_entities.json\n"
     ]
    }
   ],
   "source": [
    "full_clean_text = \"\\n\\n\".join(s[\"clean_text\"] for s in sections_data)\n",
    "full_text_path = output_dir / f\"{arxiv_id}_full_text.txt\"\n",
    "full_text_path.write_text(full_clean_text)\n",
    "\n",
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "all_entities_json = json.dumps(all_entities, indent=2)\n",
    "all_entities_path.write_text(all_entities_json)\n",
    "\n",
    "print(f\"Full text saved to: {full_text_path}\")\n",
    "print(f\"All entities saved to: {all_entities_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7bf9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entities from: output/1810.04805_all_entities.json\n",
      "Loading and processing full text from: output/1810.04805_full_text.txt\n",
      "spaCy Doc object created.\n"
     ]
    }
   ],
   "source": [
    "arxiv_id = \"1810.04805\"\n",
    "output_dir = Path(\"output\")\n",
    "\n",
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "full_text_path = output_dir / f\"{arxiv_id}_full_text.txt\"\n",
    "\n",
    "print(f\"Loading entities from: {all_entities_path}\")\n",
    "with open(all_entities_path) as f:\n",
    "    all_entities = json.load(f)\n",
    "\n",
    "print(f\"Loading and processing full text from: {full_text_path}\")\n",
    "full_text = full_text_path.read_text()\n",
    "doc = nlp(full_text)\n",
    "print(\"spaCy Doc object created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2cb8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 459\n",
      "Entities with valid spans: 459\n",
      "Entities with context saved to: output/1810.04805_entities_with_context.json\n"
     ]
    }
   ],
   "source": [
    "extractor = EntityExtractor(langextract_config)\n",
    "entities_with_spans = extractor.convert_to_spans(all_entities, doc, context_size=2)\n",
    "\n",
    "print(f\"Total entities: {len(all_entities)}\")\n",
    "print(f\"Entities with valid spans: {len(entities_with_spans)}\")\n",
    "\n",
    "serializable_entities = []\n",
    "for entity in entities_with_spans:\n",
    "    entity_copy = dict(entity)\n",
    "    entity_copy[\"span_text\"] = entity_copy[\"span\"].text\n",
    "    del entity_copy[\"span\"]\n",
    "    serializable_entities.append(entity_copy)\n",
    "\n",
    "entities_with_spans_path = output_dir / f\"{arxiv_id}_entities_with_context.json\"\n",
    "with open(entities_with_spans_path, \"w\") as f:\n",
    "    json.dump(serializable_entities, f, indent=2)\n",
    "\n",
    "print(f\"Entities with context saved to: {entities_with_spans_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e03ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running pipeline with config: window_size=10\n"
     ]
    }
   ],
   "source": [
    "config = FilterConfig(\n",
    "    min_freq=1,\n",
    "    exclude_other=True,\n",
    "    use_fuzzy=True,\n",
    "    top_k=50,\n",
    "    window_size=10,\n",
    "    pagerank_alpha=0.85\n",
    ")\n",
    "\n",
    "print(f\"\\nRunning pipeline with config: window_size={config.window_size}\")\n",
    "\n",
    "filtered_entities = filter_pipeline(\n",
    "    all_entities, \n",
    "    doc,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee960eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(entities: list[dict], top_n: int = 50):\n",
    "    type_dist = Counter(e[\"type\"] for e in entities)\n",
    "    print(f\"Type distribution: {dict(type_dist)}\\n\")\n",
    "    \n",
    "    print(f\"Top {min(top_n, len(entities))} entities:\")\n",
    "    for i, e in enumerate(entities[:top_n], 1):\n",
    "        score = e.get(\"pr_score\", 0)\n",
    "        print(f\"  {i:2d}. {e['text']:<40} {e['type']:<8} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "299d0f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type distribution: {'method': 10, 'task': 15, 'dataset': 25}\n",
      "\n",
      "Top 50 entities:\n",
      "   1. BERT                                     method   0.0677\n",
      "   2. fine-tuning                              task     0.0598\n",
      "   3. Open AI GPT                              method   0.0465\n",
      "   4. NER                                      method   0.0358\n",
      "   5. question answering                       task     0.0291\n",
      "   6. NLI                                      task     0.0270\n",
      "   7. classification                           task     0.0248\n",
      "   8. training data                            dataset  0.0230\n",
      "   9. ELMo                                     method   0.0226\n",
      "  10. MNLI                                     dataset  0.0200\n",
      "  11. SQuAD                                    dataset  0.0177\n",
      "  12. next sentence prediction                 task     0.0155\n",
      "  13. MRPC                                     dataset  0.0149\n",
      "  14. Masked LM                                method   0.0141\n",
      "  15. named entity recognition                 task     0.0130\n",
      "  16. Stanford Question Answering Dataset      dataset  0.0122\n",
      "  17. QNLI                                     dataset  0.0122\n",
      "  18. sentence pairs                           dataset  0.0120\n",
      "  19. Dev set                                  dataset  0.0117\n",
      "  20. Wikipedia                                dataset  0.0115\n",
      "  21. sentiment analysis                       task     0.0107\n",
      "  22. LTR                                      method   0.0102\n",
      "  23. machine translation                      task     0.0095\n",
      "  24. paraphrasing                             task     0.0091\n",
      "  25. text generation                          task     0.0081\n",
      "  26. BiLSTM                                   method   0.0080\n",
      "  27. QP Quora Question Pairs                  dataset  0.0079\n",
      "  28. positive examples                        dataset  0.0079\n",
      "  29. negative examples                        dataset  0.0079\n",
      "  30. Stanford Sentiment Treebank              dataset  0.0079\n",
      "  31. CoLA The Corpus of Linguistic Acceptability dataset  0.0079\n",
      "  32. Semantic Textual Similarity Benchmark    dataset  0.0079\n",
      "  33. GLUE tasks                               dataset  0.0078\n",
      "  34. language understanding                   task     0.0076\n",
      "  35. Books Corpus                             dataset  0.0072\n",
      "  36. LSTMs                                    method   0.0070\n",
      "  37. labeled training examples                dataset  0.0069\n",
      "  38. tuning data                              dataset  0.0068\n",
      "  39. Left-to-Right (LTR) LM                   method   0.0068\n",
      "  40. predict a single word                    task     0.0067\n",
      "  41. development set                          dataset  0.0067\n",
      "  42. ESIM+ELMo                                method   0.0066\n",
      "  43. token predictions                        task     0.0065\n",
      "  44. grounded commonsense inference           task     0.0063\n",
      "  45. sentence-pair completion                 task     0.0063\n",
      "  46. Trivia QA                                dataset  0.0063\n",
      "  47. GLUE benchmark                           dataset  0.0058\n",
      "  48. Ima-ge Net                               dataset  0.0058\n",
      "  49. RTE Recognizing Textual Entailment       dataset  0.0057\n",
      "  50. natural language inference dataset       dataset  0.0057\n"
     ]
    }
   ],
   "source": [
    "print_results(filtered_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a29ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduction: 89.1% (459 -> 50)\n"
     ]
    }
   ],
   "source": [
    "impact = analyze_impact(all_entities, filtered_entities)\n",
    "print(f\"\\nReduction: {impact['reduction_pct']:.1f}% ({impact['original_count']} -> {impact['filtered_count']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginkgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
