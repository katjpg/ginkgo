{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path modified: {str(project_root) in sys.path}\")\n",
    "\n",
    "try:\n",
    "    import client\n",
    "    import config\n",
    "    import models\n",
    "    import parsers\n",
    "    import nlp\n",
    "    print(\"All project modules are now accessible\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Module import failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.arxiv import ArXivClient\n",
    "from client.grobid import GROBIDClient\n",
    "from models.grobid import Form, File\n",
    "from parsers.tei import Parser\n",
    "from config.llm import LangExtractConfig, GeminiConfig\n",
    "from config.nlp import NLPConfig\n",
    "from nlp.structural import SectionProcessor\n",
    "from nlp.semantic import SemanticExtractor\n",
    "from nlp.syntactic import parse\n",
    "from nlp.entity_pairs import create_pairs, filter_by_type\n",
    "from nlp.relation import RelationExtractor\n",
    "from utils.clean_text import preprocess_section\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873bc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1706.03762\"\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de8f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_client = ArXivClient()\n",
    "metadata = arxiv_client.get_metadata(arxiv_id)\n",
    "pdf_path = output_dir / f\"{arxiv_id}.pdf\"\n",
    "arxiv_client.download_pdf(arxiv_id, str(pdf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24202d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87297"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_client = GROBIDClient()\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "form = Form(\n",
    "    file=File(payload=pdf_bytes, file_name=f\"{arxiv_id}.pdf\"),\n",
    "    consolidate_citations=1,\n",
    "    consolidate_header=1,\n",
    "    segment_sentences=True\n",
    ")\n",
    "\n",
    "response = grobid_client.process_pdf(form)\n",
    "tei_path = output_dir / f\"{arxiv_id}.tei.xml\"\n",
    "tei_path.write_bytes(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98831392",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(response.content)\n",
    "article = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7f2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_data = []\n",
    "for section in article.sections:\n",
    "    section_text = \"\"\n",
    "    for paragraph in section.paragraphs:\n",
    "        section_text += paragraph.plain_text + \" \"\n",
    "    \n",
    "    clean_text = preprocess_section(section_text.strip())\n",
    "    sections_data.append({\n",
    "        \"title\": section.title,\n",
    "        \"raw_text\": section_text.strip(),\n",
    "        \"clean_text\": clean_text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7847d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "langextract_config = LangExtractConfig()\n",
    "gemini_config = GeminiConfig()\n",
    "nlp_config = NLPConfig()\n",
    "semantic_extractor = SemanticExtractor(langextract_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langextract as lx\n",
    "from llm.prompts.langextract import PROMPT, EXAMPLES\n",
    "from config.nlp import normalize_section\n",
    "\n",
    "all_entities = []\n",
    "\n",
    "for section_data in sections_data:\n",
    "    normalized_title = normalize_section(section_data[\"title\"], nlp_config.patterns)\n",
    "    section_config = nlp_config.sections.get(normalized_title, nlp_config.sections[\"default\"])\n",
    "    \n",
    "    result = lx.extract(\n",
    "        text_or_documents=section_data[\"clean_text\"],\n",
    "        prompt_description=PROMPT,\n",
    "        examples=EXAMPLES,\n",
    "        model_id=langextract_config.model_id,\n",
    "        api_key=langextract_config.api_key,\n",
    "        extraction_passes=section_config.extraction_passes,\n",
    "        max_workers=langextract_config.max_workers,\n",
    "        max_char_buffer=section_config.max_char_buffer,\n",
    "    )\n",
    "    \n",
    "    section_entities = []\n",
    "    for extraction in result.extractions:\n",
    "        entity_dict = {\n",
    "            \"text\": extraction.extraction_text,\n",
    "            \"type\": extraction.extraction_class,\n",
    "            \"char_interval\": (\n",
    "                {\n",
    "                    \"start_pos\": extraction.char_interval.start_pos,\n",
    "                    \"end_pos\": extraction.char_interval.end_pos,\n",
    "                }\n",
    "                if extraction.char_interval\n",
    "                else None\n",
    "            ),\n",
    "            \"section\": section_data[\"title\"]\n",
    "        }\n",
    "        section_entities.append(entity_dict)\n",
    "    \n",
    "    title = section_data[\"title\"].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    section_json = json.dumps(section_entities, indent=2)\n",
    "    section_path = output_dir / f\"{arxiv_id}_{title}_entities.json\"\n",
    "    section_path.write_text(section_json)\n",
    "    \n",
    "    all_entities.extend(section_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bde373d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58029"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities_json = json.dumps(all_entities, indent=2)\n",
    "all_entities_path = output_dir / f\"{arxiv_id}_all_entities.json\"\n",
    "all_entities_path.write_text(all_entities_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d65e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_by_section = {}\n",
    "for entity in all_entities:\n",
    "    section = entity[\"section\"]\n",
    "    if section not in entities_by_section:\n",
    "        entities_by_section[section] = []\n",
    "    entities_by_section[section].append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c9965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_extractor = RelationExtractor(gemini_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3864138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "def find_entity_in_doc(entity_text: str, doc: Doc) -> Span | None:\n",
    "    \"\"\"Find entity by text matching instead of character positions.\"\"\"\n",
    "    entity_lower = entity_text.lower()\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for i in range(len(sent)):\n",
    "            for j in range(i + 1, min(i + 10, len(sent) + 1)):\n",
    "                span = sent[i:j]\n",
    "                if span.text.lower() == entity_lower or entity_lower in span.text.lower():\n",
    "                    return span\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d951e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Section 1/17] Introduction\n",
      "  Entities: 44\n",
      "  [1/20] Recurrent neural net -> long short-term memo ✗\n",
      "  [2/20] Recurrent neural net -> gated recurrent ✗\n",
      "  [3/20] Recurrent neural net -> sequence modeling ✓ used_for\n",
      "  [4/20] Recurrent neural net -> transduction problem ✓ used_for\n",
      "  [5/20] Recurrent neural net -> language modeling ✓ used_for\n",
      "  [6/20] long short-term memo -> gated recurrent ✓ compared_with\n",
      "  [7/20] long short-term memo -> sequence modeling ✓ used_for\n",
      "  [8/20] long short-term memo -> transduction problem ✓ used_for\n",
      "  [9/20] long short-term memo -> language modeling ✓ used_for\n",
      "  [10/20] long short-term memo -> machine translation ✓ used_for\n",
      "  [11/20] gated recurrent -> sequence modeling ✓ used_for\n",
      "  [12/20] gated recurrent -> transduction problem ✓ used_for\n",
      "  [13/20] gated recurrent -> language modeling ✓ used_for\n",
      "  [14/20] gated recurrent -> machine translation ✓ used_for\n",
      "  [15/20] gated recurrent -> recurrent language m ✗\n",
      "  [16/20] language modeling -> sequences ✓ applied_to\n",
      "  [17/20] machine translation -> sequences ✓ applied_to\n",
      "  [18/20] machine translation -> input ✗\n",
      "  [19/20] recurrent language m -> encoder-decoder arch ✗\n",
      "  [20/20] recurrent language m -> Recurrent models ✗\n",
      "  Found: 14 relations\n",
      "\n",
      "[Section 2/17] Background\n",
      "  Entities: 23\n",
      "  [1/20] Extended Neural GPU -> ByteNet ✗\n",
      "  [2/20] Extended Neural GPU -> ConvS2S ✓ based_on\n",
      "  [3/20] Extended Neural GPU -> convolutional neural ✓ based_on\n",
      "  [4/20] Extended Neural GPU -> learning dependencie ✓ used_for\n",
      "  [5/20] ByteNet -> ConvS2S ✓ based_on\n",
      "  [6/20] ByteNet -> convolutional neural ✓ based_on\n",
      "  [7/20] ByteNet -> learning dependencie ✓ used_for\n",
      "  [8/20] ByteNet -> Transformer ✗\n",
      "  [9/20] ConvS2S -> convolutional neural ✓ based_on\n",
      "  [10/20] ConvS2S -> learning dependencie ✓ used_for\n",
      "  [11/20] ConvS2S -> Transformer ✗\n",
      "  [12/20] ConvS2S -> attention ✓ based_on\n",
      "  [13/20] convolutional neural -> learning dependencie ✓ used_for\n",
      "  [14/20] convolutional neural -> Transformer ✓ based_on\n",
      "  [15/20] convolutional neural -> attention ✓ based_on\n",
      "  [16/20] convolutional neural -> Multi-Head Attention ✓ based_on\n",
      "  [17/20] these models -> learning dependencie ✓ used_for\n",
      "  [18/20] these models -> Transformer ✓ based_on\n",
      "  [19/20] these models -> attention ✗\n",
      "  [20/20] these models -> Multi-Head Attention ✗\n",
      "  Found: 15 relations\n",
      "\n",
      "[Section 3/17] Model Architecture\n",
      "  Entities: 15\n",
      "  [1/20] neural sequence tran -> encoder-decoder stru ✓ based_on\n",
      "  [2/20] neural sequence tran -> input sequence ✓ applied_to\n",
      "  [3/20] neural sequence tran -> symbol representatio ✓ applied_to\n",
      "  [4/20] neural sequence tran -> sequence of continuo ✓ applied_to\n",
      "  [5/20] encoder-decoder stru -> input sequence ✓ applied_to\n",
      "  [6/20] encoder-decoder stru -> symbol representatio ✓ applied_to\n",
      "  [7/20] encoder-decoder stru -> sequence of continuo ✓ applied_to\n",
      "  [8/20] the encoder -> input sequence ✓ applied_to\n",
      "  [9/20] the encoder -> symbol representatio ✓ applied_to\n",
      "  [10/20] the encoder -> sequence of continuo ✓ applied_to\n",
      "  [11/20] z -> output sequence ✓ applied_to\n",
      "  [12/20] z -> symbols ✓ applied_to\n",
      "  [13/20] z -> self-attention ✗\n",
      "  [14/20] the decoder -> output sequence ✓ applied_to\n",
      "  [15/20] the decoder -> symbols ✓ applied_to\n",
      "  [16/20] the decoder -> self-attention ✗\n",
      "  [17/20] the decoder -> point-wise, fully co ✗\n",
      "  [18/20] the model -> self-attention ✗\n",
      "  [19/20] the model -> point-wise, fully co ✗\n",
      "  Found: 14 relations\n",
      "\n",
      "[Section 4/17] Encoder and Decoder Stacks\n",
      "  Entities: 20\n",
      "  [1/20] multi-head self-atte -> outputs ✓ applied_to\n",
      "  [2/20] fully connected feed -> outputs ✗\n",
      "  [3/20] fully connected feed -> decoder ✗\n",
      "  [4/20] residual connection -> outputs ✓ applied_to\n",
      "  [5/20] residual connection -> decoder ✓ applied_to\n",
      "  [6/20] residual connection -> layers ✓ applied_to\n",
      "  [7/20] layer normalization -> outputs ✗\n",
      "  [8/20] layer normalization -> decoder ✓ applied_to\n",
      "  [9/20] layer normalization -> layers ✗\n",
      "  [10/20] layer normalization -> sub-layers ✓ applied_to\n",
      "  [11/20] Layer Norm -> outputs ✗\n",
      "  [12/20] Layer Norm -> decoder ✓ applied_to\n",
      "  [13/20] Layer Norm -> layers ✗\n",
      "  [14/20] Layer Norm -> sub-layers ✓ applied_to\n",
      "  [15/20] multi-head attention -> output ✗\n",
      "  [16/20] residual connections -> output embeddings ✓ applied_to\n",
      "  [17/20] residual connections -> predictions ✓ applied_to\n",
      "  [18/20] layer normalization -> output embeddings ✗\n",
      "  [19/20] layer normalization -> predictions ✗\n",
      "  [20/20] layer normalization -> outputs ✗\n",
      "  Found: 10 relations\n",
      "\n",
      "[Section 5/17] Attention\n",
      "  Entities: 15\n",
      "  [1/20] attention function -> query ✓ applied_to\n",
      "  [2/20] attention function -> key-value pairs ✓ applied_to\n",
      "  [3/20] attention function -> output ✗\n",
      "  [4/20] attention function -> query ✓ applied_to\n",
      "  [5/20] attention function -> keys ✓ applied_to\n",
      "  [6/20] Scaled Dot-Product A -> Multi-Head Attention ✗\n",
      "  Found: 4 relations\n",
      "\n",
      "[Section 6/17] Scaled Dot-Product Attention\n",
      "  Entities: 32\n",
      "  [1/20] Scaled Dot-Product A -> queries ✓ applied_to\n",
      "  [2/20] Scaled Dot-Product A -> keys ✓ applied_to\n",
      "  [3/20] Scaled Dot-Product A -> values ✓ applied_to\n",
      "  [4/20] the attention functi -> additive attention ✗\n",
      "  [5/20] a set of queries -> additive attention ✗\n",
      "  [6/20] a set of queries -> dot-product (multipl ✗\n",
      "  [7/20] The keys -> additive attention ✗\n",
      "  [8/20] The keys -> dot-product (multipl ✗\n",
      "  [9/20] The keys -> Dot-product attentio ✗\n",
      "  [10/20] The values -> additive attention ✗\n",
      "  [11/20] The values -> dot-product (multipl ✗\n",
      "  [12/20] The values -> Dot-product attentio ✗\n",
      "  [13/20] the matrix of output -> additive attention ✗\n",
      "  [14/20] the matrix of output -> dot-product (multipl ✗\n",
      "  [15/20] the matrix of output -> Dot-product attentio ✗\n",
      "  [16/20] the matrix of output -> scaling factor ✗\n",
      "  [17/20] our algorithm -> scaling factor ✗\n",
      "  [18/20] our algorithm -> Additive attention ✗\n",
      "  [19/20] our algorithm -> compatibility functi ✗\n",
      "  [20/20] our algorithm -> feed-forward network ✗\n",
      "  Found: 3 relations\n",
      "\n",
      "[Section 7/17] Multi-Head Attention\n",
      "  Entities: 19\n",
      "  [1/20] attention function -> keys ✓ applied_to\n",
      "  [2/20] attention function -> values ✓ applied_to\n",
      "  [3/20] attention function -> queries ✓ applied_to\n",
      "  [4/20] attention function -> linear projections ✗\n",
      "  [5/20] linear projections -> queries ✓ applied_to\n",
      "  [6/20] linear projections -> keys ✓ applied_to\n",
      "  [7/20] linear projections -> values ✓ applied_to\n",
      "  [8/20] linear projections -> attention function ✓ based_on\n",
      "  [9/20] these projected vers -> queries ✓ applied_to\n",
      "  [10/20] these projected vers -> keys ✓ applied_to\n",
      "  [11/20] these projected vers -> values ✓ applied_to\n",
      "  [12/20] these projected vers -> attention function ✗\n",
      "  [13/20] these projected vers -> output values ✓ applied_to\n",
      "  [14/20] attention function -> output values ✓ applied_to\n",
      "  [15/20] attention function -> Multi-head attention ✗\n",
      "  [16/20] attention function -> information ✗\n",
      "  [17/20] attention function -> representation subsp ✗\n",
      "  [18/20] the model -> information ✓ applied_to\n",
      "  [19/20] the model -> representation subsp ✓ applied_to\n",
      "  [20/20] the model -> attention head ✗\n",
      "  Found: 14 relations\n",
      "\n",
      "[Section 8/17] Applications of Attention in our Model\n",
      "  Entities: 26\n",
      "  [1/20] Transformer -> multi-head attention ✓ based_on\n",
      "  [2/20] Transformer -> encoder-decoder atte ✓ used_for\n",
      "  [3/20] Transformer -> queries ✓ applied_to\n",
      "  [4/20] Transformer -> keys ✗\n",
      "  [5/20] Transformer -> values ✗\n",
      "  [6/20] multi-head attention -> encoder-decoder atte ✓ used_for\n",
      "  [7/20] values -> sequence-to-sequence ✓ applied_to\n",
      "  [8/20] encoder -> sequence-to-sequence ✓ applied_to\n",
      "  [9/20] every position in th -> encoder-decoder atte ✗\n",
      "  [10/20] every position in th -> sequence-to-sequence ✓ applied_to\n",
      "  [11/20] every position in th -> self-attention layer ✗\n",
      "  [12/20] every position in th -> keys ✗\n",
      "  [13/20] all positions in the -> encoder-decoder atte ✓ applied_to\n",
      "  [14/20] all positions in the -> sequence-to-sequence ✓ applied_to\n",
      "  [15/20] all positions in the -> self-attention layer ✗\n",
      "  [16/20] all positions in the -> keys ✗\n",
      "  [17/20] all positions in the -> values ✗\n",
      "  [18/20] encoder-decoder atte -> sequence-to-sequence ✓ based_on\n",
      "  [19/20] sequence-to-sequence -> self-attention layer ✓ based_on\n",
      "  [20/20] sequence-to-sequence -> keys ✗\n",
      "  Found: 11 relations\n",
      "\n",
      "[Section 9/17] Position-wise Feed-Forward Networks\n",
      "  Entities: 7\n",
      "  Found: 0 relations\n",
      "\n",
      "[Section 10/17] Embeddings and Softmax\n",
      "  Entities: 11\n",
      "  [1/20] sequence transductio -> input tokens ✓ applied_to\n",
      "  [2/20] sequence transductio -> output tokens ✓ applied_to\n",
      "  [3/20] sequence transductio -> vectors ✓ applied_to\n",
      "  [4/20] sequence transductio -> linear transformatio ✗\n",
      "  [5/20] sequence transductio -> softmax function ✗\n",
      "  [6/20] our model -> weight matrix ✗\n",
      "  [7/20] our model -> embedding layers ✗\n",
      "  [8/20] our model -> pre-softmax linear t ✓ based_on\n",
      "  [9/20] our model -> weights ✗\n",
      "  Found: 4 relations\n",
      "\n",
      "[Section 11/17] Positional Encoding\n",
      "  Entities: 27\n",
      "  [1/20] our model -> recurrence ✗\n",
      "  [2/20] our model -> convolution ✗\n",
      "  [3/20] our model -> the sequence ✓ applied_to\n",
      "  [4/20] our model -> positional encodings ✓ based_on\n",
      "  [5/20] recurrence -> the sequence ✗\n",
      "  [6/20] convolution -> the sequence ✓ applied_to\n",
      "  [7/20] the model -> the sequence ✓ applied_to\n",
      "  [8/20] the model -> positional encodings ✗\n",
      "  [9/20] the model -> input embeddings ✓ applied_to\n",
      "  [10/20] the model -> encoder ✗\n",
      "  [11/20] the model -> decoder stacks ✗\n",
      "  [12/20] positional encodings -> embeddings ✓ applied_to\n",
      "  [13/20] input embeddings -> embeddings ✗\n",
      "  [14/20] encoder -> embeddings ✓ applied_to\n",
      "  [15/20] decoder stacks -> embeddings ✓ applied_to\n",
      "  [16/20] decoder stacks -> position ✓ applied_to\n",
      "  [17/20] positional encodings -> embeddings ✓ applied_to\n",
      "  [18/20] positional encodings -> position ✓ applied_to\n",
      "  [19/20] positional encodings -> dimension ✗\n",
      "  [20/20] positional encodings -> position ✓ applied_to\n",
      "  Found: 12 relations\n",
      "\n",
      "[Section 12/17] Why Self-Attention\n",
      "  Entities: 48\n",
      "  [1/20] self-attention layer -> sequence of symbol r ✓ compared_with\n",
      "  [2/20] self-attention layer -> sequence ✓ compared_with\n",
      "  [3/20] self-attention layer -> hidden layer ✓ applied_to\n",
      "  [4/20] self-attention layer -> sequence transductio ✓ used_for\n",
      "  [5/20] recurrent and convol -> sequence of symbol r ✓ applied_to\n",
      "  [6/20] recurrent and convol -> sequence ✓ compared_with\n",
      "  [7/20] recurrent and convol -> hidden layer ✓ applied_to\n",
      "  [8/20] recurrent and convol -> sequence transductio ✓ used_for\n",
      "  [9/20] computational comple -> Learning long-range  ✗\n",
      "  [10/20] computation -> Learning long-range  ✗\n",
      "  [11/20] sequential operation -> Learning long-range  ✗\n",
      "  [12/20] path length -> Learning long-range  ✗\n",
      "  [13/20] long-range dependenc -> Learning long-range  ✗\n",
      "  [14/20] self-attention layer -> sentence representat ✓ applied_to\n",
      "  [15/20] recurrent layer -> sentence representat ✓ applied_to\n",
      "  [16/20] recurrent layer -> machine translations ✓ used_for\n",
      "  [17/20] computational comple -> sentence representat ✗\n",
      "  [18/20] computational comple -> machine translations ✗\n",
      "  [19/20] self-attention layer -> sentence representat ✓ applied_to\n",
      "  [20/20] self-attention layer -> machine translations ✓ used_for\n",
      "  Found: 13 relations\n",
      "\n",
      "[Section 13/17] Training Data and Batching\n",
      "  Entities: 10\n",
      "  [1/20] WMT 2014 English-Ger -> sentence pairs ✓ applied_to\n",
      "  [2/20] WMT 2014 English-Ger -> vocabulary ✗\n",
      "  [3/20] WMT 2014 English-Ger -> word-piece vocabular ✗\n",
      "  [4/20] byte-pair encoding -> vocabulary ✓ applied_to\n",
      "  [5/20] byte-pair encoding -> word-piece vocabular ✗\n",
      "  [6/20] byte-pair encoding -> Sentence pairs ✓ applied_to\n",
      "  [7/20] byte-pair encoding -> sentence pairs ✓ applied_to\n",
      "  [8/20] WMT 2014 English-Fre -> word-piece vocabular ✗\n",
      "  [9/20] WMT 2014 English-Fre -> Sentence pairs ✗\n",
      "  [10/20] WMT 2014 English-Fre -> sentence pairs ✗\n",
      "  [11/20] WMT 2014 English-Fre -> tokens ✗\n",
      "  [12/20] WMT 2014 English-Fre -> tokens ✗\n",
      "  Found: 4 relations\n",
      "\n",
      "[Section 14/17] Hardware and Schedule\n",
      "  Entities: 13\n",
      "  [1/20] our models -> seconds ✗\n",
      "  [2/20] our models -> steps ✗\n",
      "  [3/20] our models -> hours ✗\n",
      "  [4/20] our base models -> seconds ✓ evaluated_on\n",
      "  [5/20] our base models -> steps ✗\n",
      "  [6/20] our base models -> hours ✗\n",
      "  [7/20] the base models -> steps ✓ evaluated_on\n",
      "  [8/20] the base models -> hours ✓ evaluated_on\n",
      "  [9/20] the base models -> seconds ✗\n",
      "  [10/20] our big models -> seconds ✗\n",
      "  [11/20] our big models -> steps ✗\n",
      "  [12/20] our big models -> days ✗\n",
      "  [13/20] The big models -> steps ✗\n",
      "  [14/20] The big models -> days ✗\n",
      "  Found: 3 relations\n",
      "\n",
      "[Section 15/17] Optimizer\n",
      "  Entities: 6\n",
      "  [1/20] Adam optimizer -> β 1 ✓ evaluated_on\n",
      "  [2/20] Adam optimizer -> β 2 ✓ evaluated_on\n",
      "  [3/20] Adam optimizer -> ϵ ✓ evaluated_on\n",
      "  [4/20] Adam optimizer -> learning rate ✗\n",
      "  [5/20] Adam optimizer -> warmup_steps ✗\n",
      "  Found: 3 relations\n",
      "\n",
      "[Section 16/17] Regularization\n",
      "  Entities: 11\n",
      "  [1/20] regularization -> Residual Dropout ✓ applied_to\n",
      "  [2/20] regularization -> sub-layer ✓ applied_to\n",
      "  [3/20] regularization -> sub-layer input ✓ applied_to\n",
      "  [4/20] Residual Dropout -> dropout ✓ applied_to\n",
      "  [5/20] Residual Dropout -> sub-layer ✓ applied_to\n",
      "  [6/20] Residual Dropout -> sub-layer input ✓ applied_to\n",
      "  [7/20] Residual Dropout -> dropout ✓ applied_to\n",
      "  [8/20] Residual Dropout -> sums of the embeddin ✓ applied_to\n",
      "  [9/20] dropout -> sub-layer ✓ applied_to\n",
      "  [10/20] dropout -> sub-layer input ✓ applied_to\n",
      "  [11/20] dropout -> sums of the embeddin ✓ applied_to\n",
      "  [12/20] dropout -> positional encodings ✓ applied_to\n",
      "  [13/20] dropout -> sums of the embeddin ✓ applied_to\n",
      "  [14/20] dropout -> positional encodings ✓ applied_to\n",
      "  [15/20] dropout -> encoder ✓ applied_to\n",
      "  [16/20] dropout -> decoder stacks ✓ applied_to\n",
      "  Found: 16 relations\n",
      "\n",
      "[Section 17/17] Label Smoothing\n",
      "  Entities: 5\n",
      "  [1/20] the model -> accuracy ✓ evaluated_on\n",
      "  [2/20] the model -> BLEU score ✓ evaluated_on\n",
      "  Found: 2 relations\n"
     ]
    }
   ],
   "source": [
    "from nlp.entity_pairs import VALID_TYPE_PAIRS\n",
    "from nlp.syntactic import find_sdp, verbalize_path, parse\n",
    "\n",
    "all_relations = []\n",
    "MAX_PAIRS_PER_SECTION = 20\n",
    "\n",
    "for section_idx, (section_title, section_entities) in enumerate(entities_by_section.items()):\n",
    "    if len(section_entities) < 2:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[Section {section_idx+1}/{len(entities_by_section)}] {section_title}\")\n",
    "    print(f\"  Entities: {len(section_entities)}\")\n",
    "    \n",
    "    section_data = next(s for s in sections_data if s[\"title\"] == section_title)\n",
    "    doc = parse(section_data[\"clean_text\"])\n",
    "    \n",
    "    pairs_processed = 0\n",
    "    relations_found = 0\n",
    "    \n",
    "    for i, e1 in enumerate(section_entities):\n",
    "        if pairs_processed >= MAX_PAIRS_PER_SECTION:\n",
    "            break\n",
    "            \n",
    "        for e2 in section_entities[i+1:min(i+6, len(section_entities))]:\n",
    "            if pairs_processed >= MAX_PAIRS_PER_SECTION:\n",
    "                break\n",
    "                \n",
    "            type_tuple = (e1[\"type\"].upper(), e2[\"type\"].upper())\n",
    "            if type_tuple not in VALID_TYPE_PAIRS:\n",
    "                continue\n",
    "            \n",
    "            span1 = find_entity_in_doc(e1[\"text\"], doc)\n",
    "            span2 = find_entity_in_doc(e2[\"text\"], doc)\n",
    "            \n",
    "            syntax = \"no pattern\"\n",
    "            sentence = section_data[\"clean_text\"][:500]\n",
    "            \n",
    "            if span1 and span2 and span1.sent == span2.sent:\n",
    "                syntax = verbalize_path(span1.root, span2.root)\n",
    "                sentence = span1.sent.text\n",
    "            \n",
    "            pair = {\n",
    "                \"head\": {\"text\": e1[\"text\"], \"type\": e1[\"type\"].upper()},\n",
    "                \"tail\": {\"text\": e2[\"text\"], \"type\": e2[\"type\"].upper()},\n",
    "                \"sentence\": sentence,\n",
    "                \"syntax\": syntax\n",
    "            }\n",
    "            \n",
    "            result = relation_extractor._classify(pair)\n",
    "            pairs_processed += 1\n",
    "            \n",
    "            print(f\"  [{pairs_processed}/{MAX_PAIRS_PER_SECTION}] {e1['text'][:20]} -> {e2['text'][:20]}\", end=\"\")\n",
    "            \n",
    "            if result[\"relation\"] != \"NONE\":\n",
    "                relations_found += 1\n",
    "                print(f\" ✓ {result['relation']}\")\n",
    "                \n",
    "                all_relations.append({\n",
    "                    \"head\": e1[\"text\"],\n",
    "                    \"head_type\": e1[\"type\"],\n",
    "                    \"tail\": e2[\"text\"],\n",
    "                    \"tail_type\": e2[\"type\"],\n",
    "                    \"relation\": result[\"relation\"],\n",
    "                    \"confidence\": result[\"confidence\"],\n",
    "                    \"section\": section_title,\n",
    "                    \"evidence\": sentence,\n",
    "                    \"syntax\": syntax,\n",
    "                    \"reasoning\": result.get(\"reasoning\", \"\")\n",
    "                })\n",
    "            else:\n",
    "                print(\" ✗\")\n",
    "    \n",
    "    print(f\"  Found: {relations_found} relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c03d9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126532"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_json = json.dumps(all_relations, indent=2)\n",
    "relations_path = output_dir / f\"{arxiv_id}_relations.json\"\n",
    "relations_path.write_text(relations_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1fa4b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Recurrent neural networks',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 0, 'end_pos': 25},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'long short-term memory',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 27, 'end_pos': 49},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'gated recurrent',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 58, 'end_pos': 73},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'sequence modeling',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 171, 'end_pos': 188},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'transduction problems',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 193, 'end_pos': 214},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'language modeling',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 223, 'end_pos': 240},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'machine translation',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 245, 'end_pos': 264},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'recurrent language models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 338, 'end_pos': 363},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'encoder-decoder architectures',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 368, 'end_pos': 397},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Recurrent models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 409, 'end_pos': 425},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'sequences',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 171, 'end_pos': 179},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'input',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 489, 'end_pos': 494},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'output sequences',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 499, 'end_pos': 515},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'sequence',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 586, 'end_pos': 594},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'hidden states',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 598, 'end_pos': 611},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'input',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 489, 'end_pos': 494},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'position',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 472, 'end_pos': 481},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'hidden state',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 647, 'end_pos': 659},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'input',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 674, 'end_pos': 679},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'position',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 684, 'end_pos': 692},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'sequence',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 171, 'end_pos': 179},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'sequence lengths',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 815, 'end_pos': 831},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'examples',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 877, 'end_pos': 885},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'factorization tricks',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 973, 'end_pos': 993},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'conditional computation',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1002, 'end_pos': 1025},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'model performance',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1052, 'end_pos': 1069},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Attention mechanisms',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1165, 'end_pos': 1185},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'sequence modeling',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1229, 'end_pos': 1246},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'transduction models',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1251, 'end_pos': 1270},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'dependencies',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1310, 'end_pos': 1322},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'sequences',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 171, 'end_pos': 179},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'input sequences',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1363, 'end_pos': 1368},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'output sequences',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1372, 'end_pos': 1388},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'attention mechanisms',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1438, 'end_pos': 1458},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'recurrent network',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1490, 'end_pos': 1507},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Transformer',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1537, 'end_pos': 1548},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'model architecture',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1552, 'end_pos': 1570},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'recurrence',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1581, 'end_pos': 1591},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'attention mechanism',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1165, 'end_pos': 1185},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'dependencies',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1310, 'end_pos': 1322},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'input',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 489, 'end_pos': 494},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'output',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 499, 'end_pos': 505},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'The Transformer',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1533, 'end_pos': 1548},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'translation quality',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 1803, 'end_pos': 1822},\n",
       "  'section': 'Introduction'},\n",
       " {'text': 'Extended Neural GPU',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 77, 'end_pos': 96},\n",
       "  'section': 'Background'},\n",
       " {'text': 'ByteNet',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 102, 'end_pos': 109},\n",
       "  'section': 'Background'},\n",
       " {'text': 'ConvS2S',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 118, 'end_pos': 125},\n",
       "  'section': 'Background'},\n",
       " {'text': 'convolutional neural networks',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 147, 'end_pos': 176},\n",
       "  'section': 'Background'},\n",
       " {'text': 'these models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 286, 'end_pos': 298},\n",
       "  'section': 'Background'},\n",
       " {'text': 'learning dependencies',\n",
       "  'type': 'task',\n",
       "  'char_interval': None,\n",
       "  'section': 'Background'},\n",
       " {'text': 'Transformer',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 581, 'end_pos': 592},\n",
       "  'section': 'Background'},\n",
       " {'text': 'attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 713, 'end_pos': 722},\n",
       "  'section': 'Background'},\n",
       " {'text': 'Multi-Head Attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 772, 'end_pos': 792},\n",
       "  'section': 'Background'},\n",
       " {'text': 'Self-attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 822, 'end_pos': 836},\n",
       "  'section': 'Background'},\n",
       " {'text': 'intra-attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 855, 'end_pos': 870},\n",
       "  'section': 'Background'},\n",
       " {'text': 'reading comprehension',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1075, 'end_pos': 1096},\n",
       "  'section': 'Background'},\n",
       " {'text': 'abstractive summarization',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1098, 'end_pos': 1123},\n",
       "  'section': 'Background'},\n",
       " {'text': 'textual entailment',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1125, 'end_pos': 1143},\n",
       "  'section': 'Background'},\n",
       " {'text': 'learning task-independent sentence representations',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1148, 'end_pos': 1198},\n",
       "  'section': 'Background'},\n",
       " {'text': 'End-to-end memory networks',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1212, 'end_pos': 1238},\n",
       "  'section': 'Background'},\n",
       " {'text': 'recurrent attention mechanism',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1254, 'end_pos': 1283},\n",
       "  'section': 'Background'},\n",
       " {'text': 'language question answering',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1368, 'end_pos': 1395},\n",
       "  'section': 'Background'},\n",
       " {'text': 'language modeling',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1400, 'end_pos': 1417},\n",
       "  'section': 'Background'},\n",
       " {'text': 'Transformer',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1472, 'end_pos': 1483},\n",
       "  'section': 'Background'},\n",
       " {'text': 'self-attention',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1536, 'end_pos': 1550},\n",
       "  'section': 'Background'},\n",
       " {'text': 'RNs',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1632, 'end_pos': 1635},\n",
       "  'section': 'Background'},\n",
       " {'text': 'convolution',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1639, 'end_pos': 1650},\n",
       "  'section': 'Background'},\n",
       " {'text': 'neural sequence transduction models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 17, 'end_pos': 52},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'encoder-decoder structure',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 61, 'end_pos': 86},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'the encoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 102, 'end_pos': 113},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'input sequence',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 122, 'end_pos': 136},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'symbol representations',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 140, 'end_pos': 162},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'sequence of continuous representations',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 184, 'end_pos': 222},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'z',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 223, 'end_pos': 224},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'the decoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 253, 'end_pos': 264},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'output sequence',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 283, 'end_pos': 298},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'symbols',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 318, 'end_pos': 325},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 362, 'end_pos': 371},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'self-attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 549, 'end_pos': 563},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'point-wise, fully connected layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 568, 'end_pos': 602},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'the encoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 612, 'end_pos': 623},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'the decoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 253, 'end_pos': 264},\n",
       "  'section': 'Model Architecture'},\n",
       " {'text': 'encoder',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 0, 'end_pos': 7},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'layers',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 63, 'end_pos': 69},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'multi-head self-attention mechanism',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 117, 'end_pos': 152},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'fully connected feed-forward network',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 195, 'end_pos': 231},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'residual connection',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 245, 'end_pos': 264},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'layer normalization',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 316, 'end_pos': 335},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'Layer Norm',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 381, 'end_pos': 391},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'outputs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 353, 'end_pos': 359},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'decoder',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 629, 'end_pos': 636},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'layers',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 63, 'end_pos': 69},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'sub-layers',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 292, 'end_pos': 302},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'multi-head attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 816, 'end_pos': 836},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'output',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 846, 'end_pos': 852},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'residual connections',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 909, 'end_pos': 929},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'layer normalization',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 973, 'end_pos': 992},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'self-attention sub-layer',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1013, 'end_pos': 1037},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'masking',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1125, 'end_pos': 1132},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'output embeddings',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1162, 'end_pos': 1179},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'predictions',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1225, 'end_pos': 1236},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'outputs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1281, 'end_pos': 1288},\n",
       "  'section': 'Encoder and Decoder Stacks'},\n",
       " {'text': 'attention function',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 3, 'end_pos': 21},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'query',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 52, 'end_pos': 57},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'key-value pairs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 71, 'end_pos': 86},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'output',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 93, 'end_pos': 99},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'query',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 111, 'end_pos': 116},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'keys',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 118, 'end_pos': 122},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'values',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 124, 'end_pos': 130},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'output',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 136, 'end_pos': 142},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'vectors',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 151, 'end_pos': 158},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'output',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 164, 'end_pos': 170},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'values',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 75, 'end_pos': 80},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'query',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 52, 'end_pos': 57},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'key',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 71, 'end_pos': 74},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'Scaled Dot-Product Attention',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 201, 'end_pos': 229},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'Multi-Head Attention',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 230, 'end_pos': 250},\n",
       "  'section': 'Attention'},\n",
       " {'text': 'Scaled Dot-Product Attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 34, 'end_pos': 62},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'queries',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 97, 'end_pos': 104},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'keys',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 109, 'end_pos': 113},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'values',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 136, 'end_pos': 142},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the dot products',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 172, 'end_pos': 188},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the query',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 192, 'end_pos': 201},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'all keys',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 207, 'end_pos': 215},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the softmax function',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1309, 'end_pos': 1329},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the weights',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 278, 'end_pos': 289},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the values',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 293, 'end_pos': 303},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the attention function',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 329, 'end_pos': 351},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'a set of queries',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 355, 'end_pos': 371},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'The keys',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 421, 'end_pos': 429},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'The values',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 293, 'end_pos': 303},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the matrix of outputs',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 500, 'end_pos': 521},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'additive attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 577, 'end_pos': 595},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'dot-product (multiplicative) attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 604, 'end_pos': 642},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'Dot-product attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 644, 'end_pos': 665},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'our algorithm',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 682, 'end_pos': 695},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'scaling factor',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 712, 'end_pos': 726},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'Additive attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 733, 'end_pos': 751},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'compatibility function',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 765, 'end_pos': 787},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'feed-forward network',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 796, 'end_pos': 816},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'single hidden layer',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 824, 'end_pos': 843},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'dot-product attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 898, 'end_pos': 919},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'additive attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 577, 'end_pos': 595},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'dot product attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1151, 'end_pos': 1172},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the two mechanisms',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1082, 'end_pos': 1100},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'additive attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1120, 'end_pos': 1138},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'dot product attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1151, 'end_pos': 1172},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the dot products',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1259, 'end_pos': 1275},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'the softmax function',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1309, 'end_pos': 1329},\n",
       "  'section': 'Scaled Dot-Product Attention'},\n",
       " {'text': 'attention function',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 31, 'end_pos': 49},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'keys',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 76, 'end_pos': 80},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'values',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 82, 'end_pos': 88},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'queries',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 93, 'end_pos': 100},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'linear projections',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 206, 'end_pos': 224},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'these projected versions',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 282, 'end_pos': 306},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'queries',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 310, 'end_pos': 317},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'keys',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 319, 'end_pos': 323},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'values',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 328, 'end_pos': 334},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'attention function',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 355, 'end_pos': 373},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'output values',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 413, 'end_pos': 426},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'Multi-head attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 532, 'end_pos': 552},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 560, 'end_pos': 569},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'information',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 591, 'end_pos': 602},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'representation subspaces',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 618, 'end_pos': 642},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'attention head',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 681, 'end_pos': 695},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'parameter matrices',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 754, 'end_pos': 772},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'attention layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 811, 'end_pos': 827},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'heads',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 832, 'end_pos': 837},\n",
       "  'section': 'Multi-Head Attention'},\n",
       " {'text': 'Transformer',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 4, 'end_pos': 15},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'multi-head attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 21, 'end_pos': 41},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'encoder-decoder attention',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 73, 'end_pos': 98},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'queries',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 112, 'end_pos': 119},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'keys',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 173, 'end_pos': 177},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'values',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 182, 'end_pos': 188},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'encoder',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 217, 'end_pos': 224},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'every position in the decoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 238, 'end_pos': 267},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'all positions in the input sequence',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 283, 'end_pos': 318},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'encoder-decoder attention mechanisms',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 344, 'end_pos': 380},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'sequence-to-sequence models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 384, 'end_pos': 411},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'self-attention layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 452, 'end_pos': 473},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'keys',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 512, 'end_pos': 516},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'values',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 518, 'end_pos': 524},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'queries',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 529, 'end_pos': 536},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'the encoder',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 613, 'end_pos': 624},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'Each position in the encoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 626, 'end_pos': 654},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'all positions in the previous layer of the encoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 669, 'end_pos': 719},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'self-attention layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 734, 'end_pos': 755},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'each position in the decoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 777, 'end_pos': 805},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'all positions in the decoder',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 819, 'end_pos': 847},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'leftward information flow',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 902, 'end_pos': 927},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'auto-regressive property',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 959, 'end_pos': 983},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'scaled dot-product attention',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 1013, 'end_pos': 1041},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'softmax',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1104, 'end_pos': 1111},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'values',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1077, 'end_pos': 1083},\n",
       "  'section': 'Applications of Attention in our Model'},\n",
       " {'text': 'attention sub-layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 15, 'end_pos': 35},\n",
       "  'section': 'Position-wise Feed-Forward Networks'},\n",
       " {'text': 'fully connected feed-forward network',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 94, 'end_pos': 130},\n",
       "  'section': 'Position-wise Feed-Forward Networks'},\n",
       " {'text': 'ReLU activation',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 245, 'end_pos': 260},\n",
       "  'section': 'Position-wise Feed-Forward Networks'},\n",
       " {'text': 'linear transformations',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 215, 'end_pos': 237},\n",
       "  'section': 'Position-wise Feed-Forward Networks'},\n",
       " {'text': 'convolutions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 439, 'end_pos': 451},\n",
       "  'section': 'Position-wise Feed-Forward Networks'},\n",
       " {'text': 'd model',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 514, 'end_pos': 521},\n",
       "  'section': 'Position-wise Feed-Forward Networks'},\n",
       " {'text': 'd f f',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 568, 'end_pos': 573},\n",
       "  'section': 'Position-wise Feed-Forward Networks'},\n",
       " {'text': 'sequence transduction models',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 19, 'end_pos': 47},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'input tokens',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 90, 'end_pos': 102},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'output tokens',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 107, 'end_pos': 120},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'vectors',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 124, 'end_pos': 131},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'linear transformation',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 184, 'end_pos': 205},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'softmax function',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 210, 'end_pos': 226},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'our model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 299, 'end_pos': 308},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'weight matrix',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 328, 'end_pos': 341},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'embedding layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 358, 'end_pos': 374},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'pre-softmax linear transformation',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 383, 'end_pos': 416},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'weights',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 477, 'end_pos': 484},\n",
       "  'section': 'Embeddings and Softmax'},\n",
       " {'text': 'our model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 6, 'end_pos': 15},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'recurrence',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 28, 'end_pos': 38},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'convolution',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 46, 'end_pos': 57},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 72, 'end_pos': 81},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'the sequence',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 110, 'end_pos': 122},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'positional encodings',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 248, 'end_pos': 268},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'input embeddings',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 277, 'end_pos': 293},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'encoder',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 316, 'end_pos': 323},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'decoder stacks',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 328, 'end_pos': 342},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'positional encodings',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 348, 'end_pos': 368},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'embeddings',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 408, 'end_pos': 418},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'positional encodings',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 477, 'end_pos': 497},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'sine and cosine functions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 542, 'end_pos': 567},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'position',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 611, 'end_pos': 619},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'dimension',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 633, 'end_pos': 642},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'positional encoding',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 675, 'end_pos': 694},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'sinusoid',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 712, 'end_pos': 720},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'positional encodings',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 248, 'end_pos': 268},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 852, 'end_pos': 861},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'relative positions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 891, 'end_pos': 909},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'PE pos+k',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 943, 'end_pos': 950},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'PE pos',\n",
       "  'type': 'other',\n",
       "  'char_interval': None,\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'learned positional embeddings',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1035, 'end_pos': 1064},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'the two versions',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1092, 'end_pos': 1108},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'sinusoidal version',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1178, 'end_pos': 1196},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 1218, 'end_pos': 1227},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'sequence lengths',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1246, 'end_pos': 1262},\n",
       "  'section': 'Positional Encoding'},\n",
       " {'text': 'self-attention layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 46, 'end_pos': 67},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'recurrent and convolutional layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 75, 'end_pos': 109},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'sequence of symbol representations',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 156, 'end_pos': 190},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'sequence',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 218, 'end_pos': 226},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'hidden layer',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 291, 'end_pos': 303},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'sequence transduction',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 317, 'end_pos': 338},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'self-attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 381, 'end_pos': 395},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'computational complexity',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 443, 'end_pos': 467},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'computation',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 504, 'end_pos': 515},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'sequential operations',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 579, 'end_pos': 600},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'path length',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 628, 'end_pos': 639},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'long-range dependencies',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 648, 'end_pos': 671},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'Learning long-range dependencies',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 688, 'end_pos': 720},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'paths',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 862, 'end_pos': 867},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'signals',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 889, 'end_pos': 896},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'network',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 921, 'end_pos': 928},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'path length',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1115, 'end_pos': 1126},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'self-attention layer',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1243, 'end_pos': 1263},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'recurrent layer',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1357, 'end_pos': 1372},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'computational complexity',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1422, 'end_pos': 1446},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'self-attention layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1448, 'end_pos': 1469},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'recurrent layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1486, 'end_pos': 1502},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'sentence representations',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1617, 'end_pos': 1641},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'machine translations',\n",
       "  'type': 'task',\n",
       "  'char_interval': {'start_pos': 1677, 'end_pos': 1697},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'word-piece',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1707, 'end_pos': 1717},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'byte-pair',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1726, 'end_pos': 1735},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'computational performance',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1768, 'end_pos': 1793},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'long sequences',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1819, 'end_pos': 1833},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'self-attention',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1835, 'end_pos': 1849},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'neighborhood',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 1892, 'end_pos': 1904},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'input sequence',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1922, 'end_pos': 1936},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'output position',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 1968, 'end_pos': 1983},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'the maximum path length',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2005, 'end_pos': 2028},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'this approach',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2063, 'end_pos': 2076},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'convolutional layer',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2110, 'end_pos': 2129},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'contiguous kernels',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2284, 'end_pos': 2302},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'dilated convolutions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2335, 'end_pos': 2355},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'Convolutional layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2248, 'end_pos': 2268},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'recurrent layers',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2501, 'end_pos': 2517},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'Separable convolutions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2537, 'end_pos': 2559},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'separable convolution',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2664, 'end_pos': 2685},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'self-attention',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2719, 'end_pos': 2733},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'point-wise feed-forward layer',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2746, 'end_pos': 2775},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'our model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2801, 'end_pos': 2810},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'self-attention',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 2829, 'end_pos': 2843},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'our models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 2801, 'end_pos': 2810},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'attention heads',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 3007, 'end_pos': 3022},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'attention distributions',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 2894, 'end_pos': 2917},\n",
       "  'section': 'Why Self-Attention'},\n",
       " {'text': 'WMT 2014 English-German dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 27, 'end_pos': 58},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'sentence pairs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 91, 'end_pos': 105},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'byte-pair encoding',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 136, 'end_pos': 154},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'vocabulary',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 191, 'end_pos': 201},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'WMT 2014 English-French dataset',\n",
       "  'type': 'dataset',\n",
       "  'char_interval': {'start_pos': 278, 'end_pos': 309},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'word-piece vocabulary',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 368, 'end_pos': 389},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'Sentence pairs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 395, 'end_pos': 409},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'sentence pairs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 503, 'end_pos': 517},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'tokens',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 556, 'end_pos': 562},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'tokens',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 580, 'end_pos': 586},\n",
       "  'section': 'Training Data and Batching'},\n",
       " {'text': 'machine',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 29, 'end_pos': 36},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'NVIDIA P100 GPUs',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 44, 'end_pos': 60},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'our models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 11, 'end_pos': 21},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'our base models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 66, 'end_pos': 81},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'seconds',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 174, 'end_pos': 181},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'the base models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 194, 'end_pos': 209},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'steps',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 233, 'end_pos': 238},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'hours',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 245, 'end_pos': 250},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'our big models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 256, 'end_pos': 270},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'seconds',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 331, 'end_pos': 338},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'The big models',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 340, 'end_pos': 354},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'steps',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 380, 'end_pos': 385},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'days',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 391, 'end_pos': 395},\n",
       "  'section': 'Hardware and Schedule'},\n",
       " {'text': 'Adam optimizer',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 12, 'end_pos': 26},\n",
       "  'section': 'Optimizer'},\n",
       " {'text': 'β 1',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 36, 'end_pos': 39},\n",
       "  'section': 'Optimizer'},\n",
       " {'text': 'β 2',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 47, 'end_pos': 50},\n",
       "  'section': 'Optimizer'},\n",
       " {'text': 'ϵ',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 62, 'end_pos': 63},\n",
       "  'section': 'Optimizer'},\n",
       " {'text': 'learning rate',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 87, 'end_pos': 100},\n",
       "  'section': 'Optimizer'},\n",
       " {'text': 'warmup_steps',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 228, 'end_pos': 240},\n",
       "  'section': 'Optimizer'},\n",
       " {'text': 'regularization',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 25, 'end_pos': 39},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'Residual Dropout',\n",
       "  'type': 'method',\n",
       "  'char_interval': {'start_pos': 57, 'end_pos': 73},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'dropout',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 83, 'end_pos': 90},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'sub-layer',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 117, 'end_pos': 126},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'sub-layer input',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 154, 'end_pos': 169},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'dropout',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 208, 'end_pos': 215},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'sums of the embeddings',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 223, 'end_pos': 245},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'positional encodings',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 254, 'end_pos': 274},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'encoder',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 287, 'end_pos': 294},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'decoder stacks',\n",
       "  'type': 'object',\n",
       "  'char_interval': {'start_pos': 299, 'end_pos': 313},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'P drop',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 352, 'end_pos': 358},\n",
       "  'section': 'Regularization'},\n",
       " {'text': 'label smoothing',\n",
       "  'type': 'other',\n",
       "  'char_interval': {'start_pos': 29, 'end_pos': 44},\n",
       "  'section': 'Label Smoothing'},\n",
       " {'text': 'perplexity',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 81, 'end_pos': 91},\n",
       "  'section': 'Label Smoothing'},\n",
       " {'text': 'the model',\n",
       "  'type': 'generic',\n",
       "  'char_interval': {'start_pos': 96, 'end_pos': 105},\n",
       "  'section': 'Label Smoothing'},\n",
       " {'text': 'accuracy',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 145, 'end_pos': 153},\n",
       "  'section': 'Label Smoothing'},\n",
       " {'text': 'BLEU score',\n",
       "  'type': 'metric',\n",
       "  'char_interval': {'start_pos': 158, 'end_pos': 168},\n",
       "  'section': 'Label Smoothing'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d0c6514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': 'Recurrent neural networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'sequence modeling',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'; using 'as'\",\n",
       "  'reasoning': 'The text states that Recurrent neural networks are established as approaches in sequence modeling, indicating that RNNs are used for the task of sequence modeling.'},\n",
       " {'head': 'Recurrent neural networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'transduction problems',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': \"The text states that Recurrent neural networks have been established as approaches for 'sequence modeling and transduction problems', indicating they are used for these tasks.\"},\n",
       " {'head': 'Recurrent neural networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'language modeling',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': 'The text states that Recurrent neural networks have been established as approaches in sequence modeling and transduction problems such as language modeling, indicating that Recurrent neural networks are used for language modeling.'},\n",
       " {'head': 'long short-term memory',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'gated recurrent',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': \"The text mentions both 'long short-term memory' and 'gated recurrent' neural networks in the context of sequence modeling, implying they are comparable approaches. However, there is no direct comparison of their performance or a statement of one improving upon the other. The relation is inferred from their co-occurrence as established state-of-the-art approaches.\"},\n",
       " {'head': 'long short-term memory',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'sequence modeling',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'; using 'as'\",\n",
       "  'reasoning': 'The text states that long short-term memory networks have been established as state-of-the-art approaches in sequence modeling, indicating that LSTM is used for this task.'},\n",
       " {'head': 'long short-term memory',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'transduction problems',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': 'The text states that long short-term memory networks have been established as state-of-the-art approaches for transduction problems, indicating that LSTM is used for solving these problems.'},\n",
       " {'head': 'long short-term memory',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'language modeling',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': 'The text states that long short-term memory networks have been established as state-of-the-art approaches in sequence modeling problems such as language modeling, indicating that LSTM is used for language modeling.'},\n",
       " {'head': 'long short-term memory',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'machine translation',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': 'The text states that long short-term memory networks have been established as state-of-the-art approaches in sequence modeling and transduction problems such as machine translation. This indicates that LSTM is used for the task of machine translation.'},\n",
       " {'head': 'gated recurrent',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'sequence modeling',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'; using 'as'\",\n",
       "  'reasoning': 'The text states that gated recurrent neural networks have been established as state-of-the-art approaches in sequence modeling, indicating that gated recurrent networks are used for sequence modeling.'},\n",
       " {'head': 'gated recurrent',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'transduction problems',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': \"The text states that gated recurrent neural networks 'have been firmly established as state of the art approaches in ... transduction problems', indicating that the method is used for the task.\"},\n",
       " {'head': 'gated recurrent',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'language modeling',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': 'The text states that gated recurrent neural networks have been established as state-of-the-art approaches in sequence modeling and transduction problems such as language modeling. This indicates that gated recurrent networks are used for the task of language modeling.'},\n",
       " {'head': 'gated recurrent',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'machine translation',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5].',\n",
       "  'syntax': \"via 'established'\",\n",
       "  'reasoning': 'The text states that gated recurrent neural networks have been established as state-of-the-art approaches in sequence modeling and transduction problems such as machine translation. This indicates that gated recurrent networks are used for machine translation.'},\n",
       " {'head': 'language modeling',\n",
       "  'head_type': 'task',\n",
       "  'tail': 'sequences',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures[38,24,15]. Recurrent models typically factor computation along the symbol positions of the input and o',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'sequence modeling' and 'language modeling' in relation to 'recurrent neural networks'. While language modeling is a task, 'sequences' are mentioned as the object being modeled. The relation 'applied_to' seems most fitting as the task of language modeling is applied to sequences, but the connection is not explicit and the syntax is not clear.\"},\n",
       " {'head': 'machine translation',\n",
       "  'head_type': 'task',\n",
       "  'tail': 'sequences',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Introduction',\n",
       "  'evidence': 'Recurrent neural networks, long short-term memory[13] and gated recurrent[7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation[35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures[38,24,15]. Recurrent models typically factor computation along the symbol positions of the input and o',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions machine translation as a problem where recurrent neural networks are used. While machine translation deals with sequences, the direct relationship between 'machine translation' (TASK) and 'sequences' (OBJECT) is not explicitly stated as 'applied_to'. The context focuses more on the methods used for machine translation rather than the direct application of the task to sequences.\"},\n",
       " {'head': 'Extended Neural GPU',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'ConvS2S',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.',\n",
       "  'syntax': \"via 'forms'\",\n",
       "  'reasoning': \"The text states that the Extended Neural GPU 'forms the foundation of' ConvS2S, indicating that ConvS2S is based on the Extended Neural GPU.\"},\n",
       " {'head': 'Extended Neural GPU',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'convolutional neural networks',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.',\n",
       "  'syntax': \"via 'forms'\",\n",
       "  'reasoning': \"The text states that the Extended Neural GPU, ByteNet, and ConvS2S 'use convolutional neural networks as basic building block', which indicates that convolutional neural networks are the foundation or basis for these methods.\"},\n",
       " {'head': 'Extended Neural GPU',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'learning dependencies',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions that the Extended Neural GPU is a foundation for learning dependencies, but it doesn't explicitly state that the Extended Neural GPU is used for learning dependencies. The relationship is implied rather than directly stated.\"},\n",
       " {'head': 'ByteNet',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'ConvS2S',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.',\n",
       "  'syntax': \"via 'forms'\",\n",
       "  'reasoning': \"The text states that ByteNet and ConvS2S 'forms the foundation' of a goal, and that they 'use convolutional neural networks as basic building block'. This implies that ByteNet and ConvS2S are built upon or derived from the concept of using convolutional neural networks, fitting the 'based_on' relation.\"},\n",
       " {'head': 'ByteNet',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'convolutional neural networks',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.',\n",
       "  'syntax': \"via 'forms'\",\n",
       "  'reasoning': 'The text states that ByteNet uses convolutional neural networks as a basic building block, indicating that ByteNet is based on convolutional neural networks.'},\n",
       " {'head': 'ByteNet',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'learning dependencies',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions ByteNet in the context of models that aim to reduce sequential computation and compute hidden representations in parallel. While 'learning dependencies' is a task that ByteNet likely performs, the text doesn't explicitly state 'ByteNet for learning dependencies' or a similar direct phrasing. The connection is implied through the description of the models' goals and mechanisms.\"},\n",
       " {'head': 'ConvS2S',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'convolutional neural networks',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.',\n",
       "  'syntax': \"via 'forms'\",\n",
       "  'reasoning': 'The text states that ConvS2S uses convolutional neural networks as a basic building block, indicating that ConvS2S is based on convolutional neural networks.'},\n",
       " {'head': 'ConvS2S',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'learning dependencies',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions that ConvS2S uses convolutional neural networks as a basic building block for computing hidden representations in parallel. While 'learning dependencies' is a task, the text doesn't explicitly state that ConvS2S is used for this task. The relationship is implied rather than directly stated, making the confidence low.\"},\n",
       " {'head': 'ConvS2S',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'attention',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that ConvS2S uses convolutional neural networks as a basic building block, implying it is based on them. However, the tail entity 'attention' is not directly related to ConvS2S in this context. The sentence structure does not clearly indicate a direct relationship between ConvS2S and 'attention' as a foundational element or component in the same way it does with convolutional neural networks.\"},\n",
       " {'head': 'convolutional neural networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'learning dependencies',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that convolutional neural networks are used as a 'basic building block' for models that aim to compute hidden representations, which is a form of task execution or problem-solving.\"},\n",
       " {'head': 'convolutional neural networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'Transformer',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that the Extended Neural GPU, ByteNet, and ConvS2S 'use convolutional neural networks as basic building block'. This implies that these models are built upon or derived from convolutional neural networks, fitting the 'based_on' relation. However, the phrasing 'basic building block' is not as direct as 'based on', leading to a lower confidence.\"},\n",
       " {'head': 'convolutional neural networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'attention',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that the models mentioned 'use convolutional neural networks as basic building block', which implies that these models are based on CNNs. However, the syntax is not explicit and the head entity is a task/domain rather than a specific method.\"},\n",
       " {'head': 'convolutional neural networks',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'Multi-Head Attention',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that 'convolutional neural networks' are used as a 'basic building block' for other models, indicating that these other models are based on CNNs.\"},\n",
       " {'head': 'these models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'learning dependencies',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions that the models are used for learning dependencies, but the connection is indirect and not explicitly stated with a clear syntactic marker. The phrase 'learning dependencies' describes a general goal or capability rather than a specific task the models are directly applied to in the provided snippet.\"},\n",
       " {'head': 'these models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'Transformer',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Background',\n",
       "  'evidence': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet[18] and ConvS2S[9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This mak',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that the models use convolutional neural networks as a basic building block, implying they are based on this architecture. However, the head entity 'these models' is very generic and the direct syntactic link is not as strong as in typical 'based_on' examples.\"},\n",
       " {'head': 'neural sequence transduction models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'encoder-decoder structure',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35].',\n",
       "  'syntax': \"via 'have'\",\n",
       "  'reasoning': \"The context states that 'neural sequence transduction models have an encoder-decoder structure', indicating that the encoder-decoder structure is a foundational component or basis for these models.\"},\n",
       " {'head': 'neural sequence transduction models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'input sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes how neural sequence transduction models process an 'input sequence' by mapping it to a sequence of continuous representations. This fits the 'applied_to' relation where a method (neural sequence transduction models) processes an object (input sequence).\"},\n",
       " {'head': 'neural sequence transduction models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'symbol representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': 'The context states that neural sequence transduction models map an input sequence of symbol representations to a sequence of continuous representations. This indicates that the models are processing or operating on symbol representations.'},\n",
       " {'head': 'neural sequence transduction models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'sequence of continuous representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes how neural sequence transduction models process an input sequence to produce a sequence of continuous representations, which aligns with the 'applied_to' relation where a method processes an object or domain data.\"},\n",
       " {'head': 'encoder-decoder structure',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'input sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that the encoder maps an 'input sequence' to representations, indicating that the encoder-decoder structure is applied to process input sequences.\"},\n",
       " {'head': 'encoder-decoder structure',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'symbol representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': 'The context states that the encoder maps an input sequence of symbol representations to a sequence of continuous representations, indicating that the encoder-decoder structure is applied to process symbol representations.'},\n",
       " {'head': 'encoder-decoder structure',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence of continuous representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': 'The context describes the encoder-decoder structure as mapping an input sequence to a sequence of continuous representations, which suggests it is applied to process this type of data.'},\n",
       " {'head': 'the encoder',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'input sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ).',\n",
       "  'syntax': \"via 'maps'\",\n",
       "  'reasoning': \"The context states 'the encoder maps an input sequence', indicating that the encoder (METHOD) processes the input sequence (OBJECT).\"},\n",
       " {'head': 'the encoder',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'symbol representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ).',\n",
       "  'syntax': \"via 'maps'\",\n",
       "  'reasoning': \"The context states 'the encoder maps an input sequence of symbol representations', indicating that the encoder (METHOD) processes or transforms symbol representations (OBJECT).\"},\n",
       " {'head': 'the encoder',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'sequence of continuous representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ).',\n",
       "  'syntax': \"via 'maps'; using 'to'\",\n",
       "  'reasoning': \"The context states that 'the encoder maps an input sequence... to a sequence of continuous representations', indicating that the encoder (METHOD) processes or transforms the input sequence (OBJECT) into the output sequence.\"},\n",
       " {'head': 'z',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'output sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes how the encoder maps an input sequence to a sequence of representations 'z', and then the decoder generates an 'output sequence' from 'z'. This suggests 'z' is used in the process of generating the output sequence, fitting the 'applied_to' relation where 'z' is processed to produce the output.\"},\n",
       " {'head': 'z',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'symbols',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1,..., x n ) to a sequence of continuous representations z = (z 1,..., z n ). Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next. The Transformer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that 'z' maps an input sequence of symbol representations to a sequence of continuous representations, and the decoder generates an output sequence of symbols. This indicates that 'z' is applied to process or represent symbols.\"},\n",
       " {'head': 'the decoder',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'output sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time.',\n",
       "  'syntax': \"via 'Given'\",\n",
       "  'reasoning': \"The context states that 'the decoder then generates an output sequence', indicating that the decoder (a method or component) processes or produces the output sequence (an object).\"},\n",
       " {'head': 'the decoder',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'symbols',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Model Architecture',\n",
       "  'evidence': 'Given z, the decoder then generates an output sequence (y 1,..., y m ) of symbols one element at a time.',\n",
       "  'syntax': \"via 'Given'\",\n",
       "  'reasoning': \"The context suggests that the decoder (a method or component) generates an output sequence of symbols. While 'symbols' are not explicitly a dataset or task, they represent the objects or data that the decoder processes or acts upon, fitting the 'applied_to' relation.\"},\n",
       " {'head': 'multi-head self-attention mechanism',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'outputs',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions that the multi-head self-attention mechanism is part of the encoder, which processes inputs to produce outputs. However, the direct relationship between the mechanism and 'outputs' is not explicitly stated as 'applied to' or any other defined relation type. The connection is implied through the encoder's function.\"},\n",
       " {'head': 'residual connection',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'outputs',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'residual connection around each of the two sub-layers', and the sub-layers process inputs to produce outputs. This suggests the residual connection is applied to the outputs of these sub-layers, which are derived from the inputs. However, the phrasing is indirect and the 'outputs' are not explicitly stated as the tail entity.\"},\n",
       " {'head': 'residual connection',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'decoder',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'residual connection around each of the two sub-layers', where 'sub-layers' are part of the encoder. While 'decoder' is not explicitly mentioned in the immediate context, residual connections are a common architectural component in both encoders and decoders of sequence-to-sequence models. The relation 'applied_to' fits as the residual connection is a component that is part of or applied to the decoder's structure.\"},\n",
       " {'head': 'residual connection',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'layers',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'residual connection around each of the two sub-layers', where 'layers' are part of the encoder architecture. This suggests the residual connection is applied to the layers within the encoder.\"},\n",
       " {'head': 'layer normalization',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'decoder',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes the structure of an encoder, mentioning that layer normalization is applied after residual connections around sub-layers. While 'applied_to' is a potential fit, the context doesn't explicitly state that layer normalization is applied *to* the decoder. It's more about its placement within the encoder's layers. The relation is not strongly stated, hence MEDIUM confidence.\"},\n",
       " {'head': 'layer normalization',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sub-layers',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states 'followed by layer normalization' and 'output of each sub-layer is Layer Norm(x + Sublayer(x))', indicating that layer normalization is applied to the output of the sub-layers.\"},\n",
       " {'head': 'Layer Norm',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'decoder',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes the output of a sub-layer being processed by Layer Norm, indicating that Layer Norm is applied to the output of the decoder's sub-layers (which can be considered an object or component within the decoder).\"},\n",
       " {'head': 'Layer Norm',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sub-layers',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states 'output of each sub-layer is Layer Norm(x + Sublayer(x))', indicating that Layer Norm is applied to the output of the sub-layers.\"},\n",
       " {'head': 'residual connections',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'output embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions residual connections are used around sub-layers within an encoder, which processes input data. While not explicitly stated as 'applied to', the residual connection is a component integrated into the processing of the encoder's internal representations, which can be considered a form of application to the data processing pipeline.\"},\n",
       " {'head': 'residual connections',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'predictions',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Encoder and Decoder Stacks',\n",
       "  'evidence': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection[11] around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer is Layer Norm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions residual connections are used around sub-layers within an encoder, which processes input data. While 'predictions' is mentioned later in the text, the direct connection to 'residual connections' is weak and indirect. The residual connection is a component within a larger architecture, and its application is to the intermediate outputs of sub-layers, not directly to 'predictions' as an object in the way 'applied_to' is typically used for processing domain data.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'query',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Attention',\n",
       "  'evidence': 'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.',\n",
       "  'syntax': \"via 'described'\",\n",
       "  'reasoning': \"The context describes what an attention function is and how it operates, mentioning 'query' as one of its inputs. However, it doesn't explicitly state that the attention function is applied to queries in a way that fits the 'applied_to' definition of processing a domain or object. The relation is weak and more definitional.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'key-value pairs',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Attention',\n",
       "  'evidence': 'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.',\n",
       "  'syntax': \"via 'described'\",\n",
       "  'reasoning': \"The context describes the attention function's role in processing key-value pairs, which aligns with the 'applied_to' relation where a method or concept processes an object or domain data.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'query',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Attention',\n",
       "  'evidence': 'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.',\n",
       "  'syntax': \"via 'described'\",\n",
       "  'reasoning': \"The context describes what an attention function is and how it operates, mentioning 'query' as one of its inputs. However, it doesn't explicitly state that the attention function is applied to queries in a way that fits the 'applied_to' definition of processing a domain or object. The relation is weak and more definitional.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'keys',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Attention',\n",
       "  'evidence': 'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.',\n",
       "  'syntax': \"via 'described'; using 'of', 'to'\",\n",
       "  'reasoning': \"The context describes what an attention function is and its inputs/outputs (query, keys, values, output). While 'keys' are mentioned as part of the input, the relation is descriptive rather than indicating the attention function is actively processing or being applied to 'keys' in a task-oriented or evaluative manner. The relation is weak and could be interpreted as 'NONE'.\"},\n",
       " {'head': 'Scaled Dot-Product Attention',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'queries',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Scaled Dot-Product Attention',\n",
       "  'evidence': 'We call our particular attention \"Scaled Dot-Product Attention\" (Figure2). The input consists of queries and keys of dimension d k, and values of dimension d v. We compute the dot products of the query with all keys, divide each by √ d k, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes how 'Scaled Dot-Product Attention' operates on 'queries' (and keys/values) as its input, which fits the 'applied_to' relation where a method or mechanism processes an object or domain data.\"},\n",
       " {'head': 'Scaled Dot-Product Attention',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'keys',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Scaled Dot-Product Attention',\n",
       "  'evidence': 'We call our particular attention \"Scaled Dot-Product Attention\" (Figure2). The input consists of queries and keys of dimension d k, and values of dimension d v. We compute the dot products of the query with all keys, divide each by √ d k, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states 'We compute the dot products of the query with all keys', indicating that the 'keys' are an input object to the 'Scaled Dot-Product Attention' mechanism. This aligns with the 'applied_to' relation where a method processes an object.\"},\n",
       " {'head': 'Scaled Dot-Product Attention',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'values',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Scaled Dot-Product Attention',\n",
       "  'evidence': 'We call our particular attention \"Scaled Dot-Product Attention\" (Figure2). The input consists of queries and keys of dimension d k, and values of dimension d v. We compute the dot products of the query with all keys, divide each by √ d k, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes how 'Scaled Dot-Product Attention' operates on 'values' by computing weights and applying a softmax function. This suggests that the attention mechanism is being applied to the values as part of its processing.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'keys',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k,',\n",
       "  'syntax': \"via 'found'; using 'of'\",\n",
       "  'reasoning': \"The context mentions 'performing a single attention function with d model-dimensional keys', suggesting the attention function is applied to or operates on the keys. However, the phrasing is indirect and the primary focus of the sentence is on projecting keys, values, and queries, not solely on the application of the attention function to keys.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'values',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k,',\n",
       "  'syntax': \"via 'found'; using 'of'\",\n",
       "  'reasoning': \"The context mentions 'attention function' and 'values' but the relationship is not explicit. The phrase 'attention function with d model-dimensional keys, values and queries' suggests that the attention function operates on values, but it's not a direct application or evaluation.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'queries',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k,',\n",
       "  'syntax': \"via 'found'; using 'of', 'with'\",\n",
       "  'reasoning': \"The context states 'performing a single attention function with ... queries', indicating that the attention function is applied to queries.\"},\n",
       " {'head': 'linear projections',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'queries',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k,',\n",
       "  'syntax': \"via 'learned'; using 'of', 'with'\",\n",
       "  'reasoning': \"The context states 'linearly project the queries, keys and values', indicating that linear projections are applied to the queries (and keys/values).\"},\n",
       " {'head': 'linear projections',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'keys',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k,',\n",
       "  'syntax': \"via 'learned'; using 'of'\",\n",
       "  'reasoning': \"The context states 'linearly project the queries, keys and values', indicating that the 'linear projections' are being applied to the 'keys' (as well as queries and values).\"},\n",
       " {'head': 'linear projections',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'values',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k,',\n",
       "  'syntax': \"via 'learned'; using 'of'\",\n",
       "  'reasoning': \"The context suggests that 'linear projections' are being applied to 'values' (and keys, queries) as part of an attention mechanism, transforming them. While 'values' is an OBJECT, the relation is not as direct as in Example 8 where a method is applied to a domain. The phrasing 'linearly project the queries, keys and values' indicates an operation being performed on these components.\"},\n",
       " {'head': 'linear projections',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'attention function',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k,',\n",
       "  'syntax': \"via 'learned'; using 'of'\",\n",
       "  'reasoning': \"The text suggests that linear projections are used as a component or a modification of the attention function, implying that the attention function might be based on or incorporate linear projections. However, the phrasing 'linearly project the queries, keys and values' could also imply that linear projections are applied to the components of the attention function, making 'applied_to' a possible alternative. Given the context, 'based_on' seems slightly more fitting as it suggests a foundational element or a derived form.\"},\n",
       " {'head': 'these projected versions',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'queries',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k, d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text describes performing an attention function on 'projected versions of queries', which suggests an application of a method (attention function) to an object (projected queries). However, the relationship is not explicitly stated with clear keywords, making the confidence low.\"},\n",
       " {'head': 'these projected versions',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'keys',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k, d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'projected versions of queries, keys and values' and then states 'we then perform the attention function in parallel'. This suggests that the projected versions are the input to the attention function, which is a form of application. However, the relationship is not explicitly stated and the entities are somewhat abstract.\"},\n",
       " {'head': 'these projected versions',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'values',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k, d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'projected versions of queries, keys and values' and then states 'we then perform the attention function in parallel, yielding d v -dimensional output values'. This suggests that the projected versions are inputs to the attention function, which can be interpreted as being 'applied to' the attention function. However, the relationship is not explicitly stated and the entities are abstract.\"},\n",
       " {'head': 'these projected versions',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'output values',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values.',\n",
       "  'syntax': \"via 'perform'; using 'On', 'in'\",\n",
       "  'reasoning': \"The context states 'perform the attention function in parallel, yielding d v -dimensional output values' on 'these projected versions of queries, keys and values'. This indicates that the attention function (a method) is being applied to the projected versions of queries, keys, and values (objects/data) to produce output values.\"},\n",
       " {'head': 'attention function',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'output values',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k, d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes performing an 'attention function' and then processing 'output values' derived from it. While 'output values' isn't a typical object or domain, the phrasing suggests the attention function is being applied to generate or process these values, fitting the 'applied_to' relation more closely than others.\"},\n",
       " {'head': 'the model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'information',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.',\n",
       "  'syntax': \"via 'allows'\",\n",
       "  'reasoning': \"The context suggests that the model processes or utilizes 'information' from different subspaces, which aligns with the 'applied_to' relation where a method (the model) processes an object or domain data (information).\"},\n",
       " {'head': 'the model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'representation subspaces',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Multi-Head Attention',\n",
       "  'evidence': 'Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.',\n",
       "  'syntax': \"via 'allows'\",\n",
       "  'reasoning': \"The context suggests that the model (a method) is processing or utilizing 'representation subspaces' (an object/domain data) to attend to information. While 'applied_to' fits, the generic nature of 'the model' and 'representation subspaces' makes the relation less definitive than typical examples.\"},\n",
       " {'head': 'Transformer',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'multi-head attention',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.',\n",
       "  'syntax': \"via 'uses'\",\n",
       "  'reasoning': \"The text explicitly states that the Transformer 'uses' multi-head attention, indicating that multi-head attention is a fundamental component or building block upon which the Transformer is based.\"},\n",
       " {'head': 'Transformer',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'encoder-decoder attention',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.',\n",
       "  'syntax': \"via 'uses'; using 'In'\",\n",
       "  'reasoning': \"The context states that the Transformer 'uses' encoder-decoder attention, indicating that this attention mechanism is a component or function that the Transformer method performs or is designed for.\"},\n",
       " {'head': 'Transformer',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'queries',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.',\n",
       "  'syntax': \"via 'uses'\",\n",
       "  'reasoning': \"The context describes how the Transformer model utilizes multi-head attention, where 'queries' are a component processed within the attention mechanism. While not a direct application to a domain like protein sequences, it describes a functional application of a component within the method.\"},\n",
       " {'head': 'multi-head attention',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'encoder-decoder attention',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.',\n",
       "  'syntax': \"via 'uses'; using 'In'\",\n",
       "  'reasoning': \"The context states that the Transformer 'uses multi-head attention in... encoder-decoder attention layers', indicating that multi-head attention is employed for the task of encoder-decoder attention.\"},\n",
       " {'head': 'values',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence-to-sequence models',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]. • The encoder contains self-attention layers. In a self-attention layer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions that 'memory keys and values come from the output of the encoder' and this 'mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models'. This suggests that the 'values' (from the encoder output) are being used or processed within the context of sequence-to-sequence models, which aligns with an 'applied_to' relationship, although the phrasing is indirect.\"},\n",
       " {'head': 'encoder',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence-to-sequence models',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]. • The encoder contains self-attention layers. In a self-attention layer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': 'The context describes how the encoder-decoder attention mechanism in the Transformer is used in conjunction with sequence-to-sequence models, suggesting an application or integration rather than a direct improvement or evaluation.'},\n",
       " {'head': 'every position in the decoder',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'sequence-to-sequence models',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]. • The encoder contains self-attention layers. In a self-attention layer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes how 'every position in the decoder' interacts with or processes information from 'sequence-to-sequence models' (specifically, the encoder-decoder attention mechanism within them). While not a direct 'method applied to object' in the strictest sense, it describes a functional application or integration within the broader model architecture.\"},\n",
       " {'head': 'all positions in the input sequence',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'encoder-decoder attention mechanisms',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]. • The encoder contains self-attention layers. In a self-attention layer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that 'every position in the decoder to attend over all positions in the input sequence', which indicates that the encoder-decoder attention mechanisms are applied to process information from all positions in the input sequence.\"},\n",
       " {'head': 'all positions in the input sequence',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'sequence-to-sequence models',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]. • The encoder contains self-attention layers. In a self-attention layer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes how 'all positions in the input sequence' are attended to by the decoder, which is a component of 'sequence-to-sequence models'. This suggests that the attention mechanism (implied by the context of Transformer and encoder-decoder attention) is being applied to process information from all positions in the input sequence within the framework of sequence-to-sequence models.\"},\n",
       " {'head': 'encoder-decoder attention mechanisms',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence-to-sequence models',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9].',\n",
       "  'syntax': \"via 'mimics'\",\n",
       "  'reasoning': \"The context states that the 'encoder-decoder attention mechanisms' mimic 'sequence-to-sequence models', implying that the attention mechanisms are a component or a characteristic derived from or inspired by these models.\"},\n",
       " {'head': 'sequence-to-sequence models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'self-attention layers',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Applications of Attention in our Model',\n",
       "  'evidence': 'The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38,2,9]. • The encoder contains self-attention layers. In a self-attention layer',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that the Transformer uses multi-head attention in three ways, and one of these ways mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models. This implies that sequence-to-sequence models are a foundational concept or architecture upon which the Transformer's attention mechanism is based or derived from.\"},\n",
       " {'head': 'sequence transduction models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'input tokens',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Embeddings and Softmax',\n",
       "  'evidence': 'Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model.',\n",
       "  'syntax': \"via 'use'; using 'to'\",\n",
       "  'reasoning': \"The context states that 'sequence transduction models' 'use learned embeddings to convert the input tokens...to vectors'. This indicates that the models are processing the input tokens.\"},\n",
       " {'head': 'sequence transduction models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'output tokens',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Embeddings and Softmax',\n",
       "  'evidence': 'Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model.',\n",
       "  'syntax': \"via 'use'; using 'to'\",\n",
       "  'reasoning': \"The context mentions 'sequence transduction models' and 'output tokens'. The phrase 'convert the input tokens and output tokens to vectors' suggests that the models process or operate on these tokens, which aligns with the 'applied_to' relation. However, the relation is not as direct as in typical 'applied_to' examples, and the syntax 'use...to convert' is slightly ambiguous.\"},\n",
       " {'head': 'sequence transduction models',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'vectors',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Embeddings and Softmax',\n",
       "  'evidence': 'Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model.',\n",
       "  'syntax': \"via 'use'; using 'to'\",\n",
       "  'reasoning': \"The context states that sequence transduction models 'use learned embeddings to convert the input tokens and output tokens to vectors'. This indicates that the models process or transform input tokens into vectors, which aligns with the 'applied_to' relation where a method processes an object or domain data.\"},\n",
       " {'head': 'our model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'pre-softmax linear transformation',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Embeddings and Softmax',\n",
       "  'evidence': 'In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to[30].',\n",
       "  'syntax': \"via 'share'; using 'In', 'between'\",\n",
       "  'reasoning': \"The context states that 'our model' shares a weight matrix with the 'pre-softmax linear transformation', implying that the model's architecture or a component of it is derived from or similar to the pre-softmax linear transformation, which aligns with the 'based_on' relation. The confidence is MEDIUM because the phrasing 'share the same weight matrix...similar to' suggests a similarity or inspiration rather than a direct derivation, but it's the closest fit among the provided relation types.\"},\n",
       " {'head': 'our model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'the sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.',\n",
       "  'syntax': \"via 'contains'; using 'in'\",\n",
       "  'reasoning': \"The context mentions 'our model' and 'the sequence', and the phrase 'in order for the model to make use of the order of the sequence' suggests that the model is processing or operating on the sequence. While 'applied_to' is a good fit, the phrasing is not as direct as typical examples, hence the medium confidence.\"},\n",
       " {'head': 'our model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'positional encodings',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'based_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, l',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that positional encodings are added to the input embeddings, implying that the model's input is based on these encodings. However, the phrasing is not as direct as typical 'based_on' examples.\"},\n",
       " {'head': 'convolution',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'the sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.',\n",
       "  'syntax': \"via 'contains'; using 'in'\",\n",
       "  'reasoning': \"The context mentions 'no convolution' and 'order of the sequence', implying a potential application of convolution to sequences, but it's stated negatively. The relation is weak because convolution is explicitly excluded.\"},\n",
       " {'head': 'the model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'the sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.',\n",
       "  'syntax': \"via 'inject'; using 'in'\",\n",
       "  'reasoning': \"The context states 'in order for the model to make use of the order of the sequence', indicating that the model is processing or utilizing the sequence data. The phrase 'inject some information about the relative or absolute position of the tokens in the sequence' further supports that the model is being applied to the sequence.\"},\n",
       " {'head': 'the model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'input embeddings',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, l',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context describes adding 'positional encodings' to 'input embeddings' to inject positional information into the model. This suggests the positional encodings are being applied to the input embeddings as a form of processing or augmentation before they are used by the model.\"},\n",
       " {'head': 'positional encodings',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.',\n",
       "  'syntax': \"via 'add'\",\n",
       "  'reasoning': \"The context states that 'positional encodings' are added 'to the input embeddings', indicating that the positional encodings are being applied to the embeddings.\"},\n",
       " {'head': 'encoder',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.',\n",
       "  'syntax': \"via 'add'; using 'to'\",\n",
       "  'reasoning': \"The context suggests that positional encodings are being added to input embeddings, which are then processed by the encoder. While 'add' and 'to' are present, the direct application of the encoder to embeddings isn't explicitly stated as a primary function or evaluation. The relationship is more about how components interact within a larger system rather than a direct 'applied_to' or 'used_for' scenario.\"},\n",
       " {'head': 'decoder stacks',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.',\n",
       "  'syntax': \"via 'add'; using 'at', 'to'\",\n",
       "  'reasoning': \"The context mentions adding positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. While 'decoder stacks' are mentioned in relation to 'embeddings', the primary action is 'adding positional encodings', not directly applying the decoder stacks to the embeddings. The relationship is indirect and not a primary application.\"},\n",
       " {'head': 'decoder stacks',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'position',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, l',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'decoder stacks' and 'position' in relation to injecting positional information. While 'position' can be considered an aspect of an 'object' (like a sequence), the relationship isn't explicitly stated as a direct application or processing. The positional encodings are added to the decoder stacks, which is a form of application, but the direct relationship between 'decoder stacks' and 'position' itself is more about how position information is handled within the stacks rather than the stacks being applied to position.\"},\n",
       " {'head': 'positional encodings',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.',\n",
       "  'syntax': \"via 'add'\",\n",
       "  'reasoning': \"The context states that 'positional encodings' are added 'to the input embeddings', indicating that the positional encodings are being applied to the embeddings.\"},\n",
       " {'head': 'positional encodings',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'position',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, l',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text describes adding positional encodings to input embeddings to incorporate positional information into a model. While 'positional encodings' are a component and 'position' is a concept, the direct relationship isn't a standard 'applied_to' where a method processes a domain. It's more about augmenting embeddings with positional information. The relation is weak because 'position' is an abstract concept rather than a concrete object or domain data.\"},\n",
       " {'head': 'positional encodings',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'position',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Positional Encoding',\n",
       "  'evidence': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, l',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text describes adding positional encodings to input embeddings to incorporate positional information into a model. While 'positional encodings' are a component and 'position' is a concept, the direct relationship isn't a standard 'applied_to' where a method processes a domain. It's more about augmenting embeddings with positional information. The relation is weak because 'position' is an abstract concept rather than a concrete object or domain data.\"},\n",
       " {'head': 'self-attention layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence of symbol representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x',\n",
       "  'syntax': \"via 'compare'; using 'to', 'for'\",\n",
       "  'reasoning': \"The text explicitly states 'we compare various aspects of self-attention layers to the recurrent and convolutional layers', indicating a comparison between the head and tail entities.\"},\n",
       " {'head': 'self-attention layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x',\n",
       "  'syntax': \"via 'compare'; using 'to'\",\n",
       "  'reasoning': \"The text explicitly states 'compare various aspects of self-attention layers to the recurrent and convolutional layers', indicating a comparison between the head and tail entities.\"},\n",
       " {'head': 'self-attention layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'hidden layer',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions 'self-attention layers' and 'a hidden layer', and the context discusses mapping sequences. While 'applied_to' is a possibility, the relationship isn't explicitly stated or strongly implied. The syntax doesn't provide a clear indicator.\"},\n",
       " {'head': 'self-attention layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence transduction',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that self-attention layers are commonly used for sequence transduction tasks, aligning with the 'used_for' relation where a method or component is employed to perform a specific task.\"},\n",
       " {'head': 'recurrent and convolutional layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence of symbol representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x',\n",
       "  'syntax': \"via 'compare'; using 'to', 'for'\",\n",
       "  'reasoning': \"The context mentions 'recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations', indicating that these layers are applied to process sequences of symbol representations. While 'compare' is present, the core action described for the head entity is its application to the tail entity.\"},\n",
       " {'head': 'recurrent and convolutional layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'compared_with',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x',\n",
       "  'syntax': \"via 'compare'; using 'to'\",\n",
       "  'reasoning': \"The text explicitly states 'compare various aspects of self-attention layers to the recurrent and convolutional layers', indicating a comparison between the two entities.\"},\n",
       " {'head': 'recurrent and convolutional layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'hidden layer',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'recurrent and convolutional layers commonly used for mapping one variable-length sequence... to another sequence... such as a hidden layer'. This suggests that these layers are applied to process data that forms a hidden layer, but the connection is not explicit or strong.\"},\n",
       " {'head': 'recurrent and convolutional layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sequence transduction',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context states that recurrent and convolutional layers are 'commonly used for mapping one variable-length sequence... to another sequence', which directly indicates a 'used_for' relationship where the layers are used for the task of sequence transduction.\"},\n",
       " {'head': 'self-attention layer',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sentence representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions 'self-attention layers' and 'sentence representations' in a comparative context, suggesting that self-attention layers are used to process or generate sentence representations. However, the explicit verb linking them is 'compare', and the direct application is not clearly stated, making the relation 'applied_to' a weak inference.\"},\n",
       " {'head': 'recurrent layer',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sentence representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations... to another sequence', which suggests that recurrent layers are applied to sequence representations. However, the syntax is not explicit, and the primary focus of the text is on self-attention layers.\"},\n",
       " {'head': 'recurrent layer',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'machine translations',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The context mentions 'recurrent and convolutional layers commonly used for mapping one variable-length sequence... to another sequence', which suggests a 'used_for' relationship between recurrent layers and sequence transduction (which includes machine translations). However, the direct mention of 'machine translations' as the tail entity is absent, and the syntax is not explicit.\"},\n",
       " {'head': 'self-attention layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sentence representations',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions 'self-attention layers' and 'sentence representations' in a comparative context, suggesting that self-attention layers might be applied to process sentence representations. However, the direct syntactic link is weak, and the primary focus of the sentence is on comparing different layer types rather than explicitly stating an application.\"},\n",
       " {'head': 'self-attention layers',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'machine translations',\n",
       "  'tail_type': 'task',\n",
       "  'relation': 'used_for',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Why Self-Attention',\n",
       "  'evidence': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n ) to another sequence of equal length (z 1,..., z n ), with x i, z i ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text mentions 'self-attention layers' and 'sequence transduction encoder or decoder' which is related to machine translation, but the direct link is not explicit. The phrase 'commonly used for mapping one variable-length sequence of symbol representations' suggests a task, but it's not explicitly stated as 'machine translation'.\"},\n",
       " {'head': 'WMT 2014 English-German dataset',\n",
       "  'head_type': 'dataset',\n",
       "  'tail': 'sentence pairs',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Training Data and Batching',\n",
       "  'evidence': 'We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.',\n",
       "  'syntax': \"via 'trained'; using 'on'\",\n",
       "  'reasoning': \"The context states that the WMT 2014 English-German dataset was used for training, and this dataset consists of sentence pairs. While 'trained on' often implies 'used_for' when the tail is a task, here the tail is 'sentence pairs' which are the objects within the dataset being processed. Therefore, 'applied_to' is a plausible relation, though 'used_for' could also be argued if 'sentence pairs' is interpreted as the data being processed for a task.\"},\n",
       " {'head': 'byte-pair encoding',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'vocabulary',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Training Data and Batching',\n",
       "  'evidence': 'Sentences were encoded using byte-pair encoding[3], which has a shared sourcetarget vocabulary of about 37000 tokens.',\n",
       "  'syntax': \"via 'encoded'\",\n",
       "  'reasoning': \"The text states that sentences were 'encoded using byte-pair encoding', and byte-pair encoding is a method that processes or creates a vocabulary. Therefore, byte-pair encoding is applied to the creation or processing of a vocabulary.\"},\n",
       " {'head': 'byte-pair encoding',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'Sentence pairs',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Training Data and Batching',\n",
       "  'evidence': 'We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding[3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary[38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states that 'Sentences were encoded using byte-pair encoding', indicating that byte-pair encoding is a method applied to sentence pairs for the purpose of encoding.\"},\n",
       " {'head': 'byte-pair encoding',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sentence pairs',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Training Data and Batching',\n",
       "  'evidence': 'We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding[3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary[38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set ',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text states 'Sentences were encoded using byte-pair encoding', indicating that byte-pair encoding (the head) is applied to sentence pairs (the tail) for processing.\"},\n",
       " {'head': 'our base models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'seconds',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Hardware and Schedule',\n",
       "  'evidence': 'For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.',\n",
       "  'syntax': \"via 'described'; using 'For'\",\n",
       "  'reasoning': \"The context states that the training step took a certain amount of time ('seconds'), which is a metric used to evaluate the performance of the 'base models'.\"},\n",
       " {'head': 'the base models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'steps',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Hardware and Schedule',\n",
       "  'evidence': 'We trained the base models for a total of 100,000 steps or 12 hours.',\n",
       "  'syntax': \"via 'trained'\",\n",
       "  'reasoning': \"The context mentions training for a number of 'steps', which can be interpreted as a metric for evaluation, but it's not a direct performance evaluation like accuracy or F1 score. The primary relation is about training duration rather than performance testing.\"},\n",
       " {'head': 'the base models',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'hours',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Hardware and Schedule',\n",
       "  'evidence': 'We trained the base models for a total of 100,000 steps or 12 hours.',\n",
       "  'syntax': \"via 'trained'; using 'for'\",\n",
       "  'reasoning': \"The context mentions 'hours' in relation to training time, which could be interpreted as a metric for evaluation, but it's not a standard performance metric like accuracy or F1 score. The primary focus is on training duration rather than performance evaluation.\"},\n",
       " {'head': 'Adam optimizer',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'β 1',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Optimizer',\n",
       "  'evidence': 'We used the Adam optimizer[20] with β 1 = 0.9, β 2 = 0.98 and ϵ = 10 -9.',\n",
       "  'syntax': \"via 'used'\",\n",
       "  'reasoning': \"The context mentions the Adam optimizer and its parameters beta1, beta2, and epsilon. While beta1 is a metric, the relationship is not one of performance evaluation. The optimizer is simply configured with these metric values, not evaluated on them. Therefore, 'evaluated_on' is the closest fit, but with low confidence.\"},\n",
       " {'head': 'Adam optimizer',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'β 2',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Optimizer',\n",
       "  'evidence': 'We used the Adam optimizer[20] with β 1 = 0.9, β 2 = 0.98 and ϵ = 10 -9.',\n",
       "  'syntax': \"via 'used'; using 'with'\",\n",
       "  'reasoning': \"The context mentions 'Adam optimizer' and 'β 2' which is a hyperparameter of the optimizer. While hyperparameters are often tuned and evaluated, the sentence does not explicitly state that β 2 is being used as a metric to evaluate the Adam optimizer's performance. It is more likely describing the configuration of the optimizer.\"},\n",
       " {'head': 'Adam optimizer',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'ϵ',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'LOW',\n",
       "  'section': 'Optimizer',\n",
       "  'evidence': 'We used the Adam optimizer[20] with β 1 = 0.9, β 2 = 0.98 and ϵ = 10 -9.',\n",
       "  'syntax': \"via 'used'; using 'with'\",\n",
       "  'reasoning': \"The context mentions 'Adam optimizer' and 'epsilon (ϵ)' together, but epsilon is a hyperparameter of the Adam optimizer, not a metric it is evaluated on. The relation is weak and not a direct evaluation.\"},\n",
       " {'head': 'regularization',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'Residual Dropout',\n",
       "  'tail_type': 'method',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'MEDIUM',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'employ'\",\n",
       "  'reasoning': \"The context states 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer'. This indicates that Residual Dropout (a method) is applied to the output of sub-layers (an object/part of a larger system).\"},\n",
       " {'head': 'regularization',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sub-layer',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'employ'; using 'to'\",\n",
       "  'reasoning': \"The context states 'We apply dropout[33] to the output of each sub-layer', indicating that the regularization technique (dropout) is being applied to the sub-layer object.\"},\n",
       " {'head': 'regularization',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sub-layer input',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'employ'\",\n",
       "  'reasoning': \"The context states 'We apply dropout... to the output of each sub-layer, before it is added to the sub-layer input'. This indicates that regularization (dropout) is being applied to the sub-layer input.\"},\n",
       " {'head': 'Residual Dropout',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'dropout',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'apply'\",\n",
       "  'reasoning': \"The context states 'We apply dropout[33] to the output of each sub-layer', indicating that Residual Dropout (the head) is being applied to the concept of dropout (the tail) in the described process.\"},\n",
       " {'head': 'Residual Dropout',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'sub-layer',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'apply'; using 'to'\",\n",
       "  'reasoning': \"The context explicitly states 'We apply dropout ... to the output of each sub-layer', indicating that the method 'Residual Dropout' is applied to the object 'sub-layer'.\"},\n",
       " {'head': 'Residual Dropout',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'sub-layer input',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'apply'\",\n",
       "  'reasoning': \"The text explicitly states 'We apply dropout... to the output of each sub-layer, before it is added to the sub-layer input'. This indicates that the method (Residual Dropout, implied by 'dropout') is being applied to a specific object (sub-layer input).\"},\n",
       " {'head': 'Residual Dropout',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'dropout',\n",
       "  'tail_type': 'other',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'apply'\",\n",
       "  'reasoning': \"The context states 'We apply dropout[33] to the output of each sub-layer', indicating that Residual Dropout (the head) is being applied to the concept of dropout (the tail) in the described process.\"},\n",
       " {'head': 'Residual Dropout',\n",
       "  'head_type': 'method',\n",
       "  'tail': 'sums of the embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'we apply dropout to the sums of the embeddings', indicating that the method (Residual Dropout) is applied to the object (sums of the embeddings).\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sub-layer',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'apply'; using 'to'\",\n",
       "  'reasoning': \"The context states 'We apply dropout[33] to the output of each sub-layer', indicating that the method 'dropout' is applied to the object 'sub-layer'.\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sub-layer input',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.',\n",
       "  'syntax': \"via 'apply'\",\n",
       "  'reasoning': \"The text explicitly states 'We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.' This indicates that dropout is being applied to the sub-layer input.\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sums of the embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'we apply dropout to the sums of the embeddings', indicating that the method 'dropout' is applied to the object 'sums of the embeddings'.\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'positional encodings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'we apply dropout to the sums of the embeddings and the positional encodings', indicating that dropout (OTHER) is being applied to positional encodings (OBJECT).\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'sums of the embeddings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'we apply dropout to the sums of the embeddings', indicating that the method 'dropout' is applied to the object 'sums of the embeddings'.\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'positional encodings',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'we apply dropout to the sums of the embeddings and the positional encodings', indicating that dropout (OTHER) is being applied to positional encodings (OBJECT).\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'encoder',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'we apply dropout [...] to the output of each sub-layer' and 'we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks'. This indicates that dropout (OTHER) is being applied to components within the encoder (OBJECT).\"},\n",
       " {'head': 'dropout',\n",
       "  'head_type': 'other',\n",
       "  'tail': 'decoder stacks',\n",
       "  'tail_type': 'object',\n",
       "  'relation': 'applied_to',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Regularization',\n",
       "  'evidence': 'We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.',\n",
       "  'syntax': 'no pattern',\n",
       "  'reasoning': \"The text explicitly states 'we apply dropout to the output of each sub-layer' and 'we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks'. This indicates that dropout, an 'OTHER' entity in this context (a technique or regularization method), is being applied to specific components ('decoder stacks' being one of them) of a model.\"},\n",
       " {'head': 'the model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'accuracy',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Label Smoothing',\n",
       "  'evidence': 'This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.',\n",
       "  'syntax': \"via 'hurts'\",\n",
       "  'reasoning': \"The context states that 'the model' improves 'accuracy', which indicates that accuracy is a metric used to evaluate the model's performance.\"},\n",
       " {'head': 'the model',\n",
       "  'head_type': 'generic',\n",
       "  'tail': 'BLEU score',\n",
       "  'tail_type': 'metric',\n",
       "  'relation': 'evaluated_on',\n",
       "  'confidence': 'HIGH',\n",
       "  'section': 'Label Smoothing',\n",
       "  'evidence': 'This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.',\n",
       "  'syntax': \"via 'hurts'\",\n",
       "  'reasoning': \"The context states that the model improves the BLEU score, which is a metric used to evaluate the performance of models, particularly in natural language generation tasks. The improvement in the BLEU score indicates that the model's performance is being measured and assessed using this metric.\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_relations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginkgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
